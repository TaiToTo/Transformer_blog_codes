{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    print(\"Inside 'MultiHeadAttention' class...\")\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    print()\n",
    "    print(\"The shape of 'q' is \" + str(q.shape))\n",
    "    print(\"The shape of 'k' is \" + str(k.shape))\n",
    "    print(\"The shape of 'v' is \" + str(v.shape))\n",
    "\n",
    " \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    print()\n",
    "    print(\"After splitting the heads....\")\n",
    "    print(\"The shape of 'q' is \" + str(q.shape))\n",
    "    print(\"The shape of 'k' is \" + str(k.shape))\n",
    "    print(\"The shape of 'v' is \" + str(v.shape))\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    \n",
    "    print()\n",
    "    print(\"The shape of 'attention_weights' is \" + str(attention_weights.shape))\n",
    "    print(\"The shape of 'scaled_attention' is \" + str(scaled_attention.shape))\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "    \n",
    "    print()\n",
    "    print(\"After transposing....\")\n",
    "    print(\"The shape of 'scaled_attention' is \" + str(scaled_attention.shape))\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    \n",
    "    print()\n",
    "    print(\"The shape of 'concat_attention' is \" + str(concat_attention.shape))\n",
    "    \n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "    print()\n",
    "    print(\"The shape of 'output' is \" + str(output.shape))\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside 'MultiHeadAttention' class...\n",
      "\n",
      "The shape of 'q' is (1, 9, 512)\n",
      "The shape of 'k' is (1, 9, 512)\n",
      "The shape of 'v' is (1, 9, 512)\n",
      "\n",
      "After splitting the heads....\n",
      "The shape of 'q' is (1, 8, 9, 64)\n",
      "The shape of 'k' is (1, 8, 9, 64)\n",
      "The shape of 'v' is (1, 8, 9, 64)\n",
      "\n",
      "The shape of 'attention_weights' is (1, 8, 9, 9)\n",
      "The shape of 'scaled_attention' is (1, 8, 9, 64)\n",
      "\n",
      "After transposing....\n",
      "The shape of 'scaled_attention' is (1, 9, 8, 64)\n",
      "\n",
      "The shape of 'concat_attention' is (1, 9, 512)\n",
      "\n",
      "The shape of 'output' is (1, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 9, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 9, 512]), TensorShape([1, 8, 9, 9]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = np.arange(1*9*512).reshape((1, 9, 512)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 512)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 9, 64), dtype=int64, numpy=\n",
       "array([[[[   1,    2,    3, ...,   62,   63,   64],\n",
       "         [  65,   66,   67, ...,  126,  127,  128],\n",
       "         [ 129,  130,  131, ...,  190,  191,  192],\n",
       "         ...,\n",
       "         [ 385,  386,  387, ...,  446,  447,  448],\n",
       "         [ 449,  450,  451, ...,  510,  511,  512],\n",
       "         [ 513,  514,  515, ...,  574,  575,  576]],\n",
       "\n",
       "        [[ 577,  578,  579, ...,  638,  639,  640],\n",
       "         [ 641,  642,  643, ...,  702,  703,  704],\n",
       "         [ 705,  706,  707, ...,  766,  767,  768],\n",
       "         ...,\n",
       "         [ 961,  962,  963, ..., 1022, 1023, 1024],\n",
       "         [1025, 1026, 1027, ..., 1086, 1087, 1088],\n",
       "         [1089, 1090, 1091, ..., 1150, 1151, 1152]],\n",
       "\n",
       "        [[1153, 1154, 1155, ..., 1214, 1215, 1216],\n",
       "         [1217, 1218, 1219, ..., 1278, 1279, 1280],\n",
       "         [1281, 1282, 1283, ..., 1342, 1343, 1344],\n",
       "         ...,\n",
       "         [1537, 1538, 1539, ..., 1598, 1599, 1600],\n",
       "         [1601, 1602, 1603, ..., 1662, 1663, 1664],\n",
       "         [1665, 1666, 1667, ..., 1726, 1727, 1728]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[2881, 2882, 2883, ..., 2942, 2943, 2944],\n",
       "         [2945, 2946, 2947, ..., 3006, 3007, 3008],\n",
       "         [3009, 3010, 3011, ..., 3070, 3071, 3072],\n",
       "         ...,\n",
       "         [3265, 3266, 3267, ..., 3326, 3327, 3328],\n",
       "         [3329, 3330, 3331, ..., 3390, 3391, 3392],\n",
       "         [3393, 3394, 3395, ..., 3454, 3455, 3456]],\n",
       "\n",
       "        [[3457, 3458, 3459, ..., 3518, 3519, 3520],\n",
       "         [3521, 3522, 3523, ..., 3582, 3583, 3584],\n",
       "         [3585, 3586, 3587, ..., 3646, 3647, 3648],\n",
       "         ...,\n",
       "         [3841, 3842, 3843, ..., 3902, 3903, 3904],\n",
       "         [3905, 3906, 3907, ..., 3966, 3967, 3968],\n",
       "         [3969, 3970, 3971, ..., 4030, 4031, 4032]],\n",
       "\n",
       "        [[4033, 4034, 4035, ..., 4094, 4095, 4096],\n",
       "         [4097, 4098, 4099, ..., 4158, 4159, 4160],\n",
       "         [4161, 4162, 4163, ..., 4222, 4223, 4224],\n",
       "         ...,\n",
       "         [4417, 4418, 4419, ..., 4478, 4479, 4480],\n",
       "         [4481, 4482, 4483, ..., 4542, 4543, 4544],\n",
       "         [4545, 4546, 4547, ..., 4606, 4607, 4608]]]])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = tf.convert_to_tensor(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = tf.reshape(sample_query, (1, 8, 9, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 8, 9, 64])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 64), dtype=int64, numpy=\n",
       "array([[  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "         27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "         40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "         53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64],\n",
       "       [ 65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128],\n",
       "       [129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141,\n",
       "        142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
       "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180,\n",
       "        181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192],\n",
       "       [193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205,\n",
       "        206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218,\n",
       "        219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231,\n",
       "        232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244,\n",
       "        245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256],\n",
       "       [257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "        270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282,\n",
       "        283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295,\n",
       "        296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
       "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320],\n",
       "       [321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333,\n",
       "        334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346,\n",
       "        347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359,\n",
       "        360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372,\n",
       "        373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384],\n",
       "       [385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397,\n",
       "        398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410,\n",
       "        411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "        424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436,\n",
       "        437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448],\n",
       "       [449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
       "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474,\n",
       "        475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487,\n",
       "        488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500,\n",
       "        501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512],\n",
       "       [513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
       "        526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
       "        539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
       "        552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
       "        565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576]])>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 64), dtype=int64, numpy=\n",
       "array([[ 577,  578,  579,  580,  581,  582,  583,  584,  585,  586,  587,\n",
       "         588,  589,  590,  591,  592,  593,  594,  595,  596,  597,  598,\n",
       "         599,  600,  601,  602,  603,  604,  605,  606,  607,  608,  609,\n",
       "         610,  611,  612,  613,  614,  615,  616,  617,  618,  619,  620,\n",
       "         621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
       "         632,  633,  634,  635,  636,  637,  638,  639,  640],\n",
       "       [ 641,  642,  643,  644,  645,  646,  647,  648,  649,  650,  651,\n",
       "         652,  653,  654,  655,  656,  657,  658,  659,  660,  661,  662,\n",
       "         663,  664,  665,  666,  667,  668,  669,  670,  671,  672,  673,\n",
       "         674,  675,  676,  677,  678,  679,  680,  681,  682,  683,  684,\n",
       "         685,  686,  687,  688,  689,  690,  691,  692,  693,  694,  695,\n",
       "         696,  697,  698,  699,  700,  701,  702,  703,  704],\n",
       "       [ 705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
       "         716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,\n",
       "         727,  728,  729,  730,  731,  732,  733,  734,  735,  736,  737,\n",
       "         738,  739,  740,  741,  742,  743,  744,  745,  746,  747,  748,\n",
       "         749,  750,  751,  752,  753,  754,  755,  756,  757,  758,  759,\n",
       "         760,  761,  762,  763,  764,  765,  766,  767,  768],\n",
       "       [ 769,  770,  771,  772,  773,  774,  775,  776,  777,  778,  779,\n",
       "         780,  781,  782,  783,  784,  785,  786,  787,  788,  789,  790,\n",
       "         791,  792,  793,  794,  795,  796,  797,  798,  799,  800,  801,\n",
       "         802,  803,  804,  805,  806,  807,  808,  809,  810,  811,  812,\n",
       "         813,  814,  815,  816,  817,  818,  819,  820,  821,  822,  823,\n",
       "         824,  825,  826,  827,  828,  829,  830,  831,  832],\n",
       "       [ 833,  834,  835,  836,  837,  838,  839,  840,  841,  842,  843,\n",
       "         844,  845,  846,  847,  848,  849,  850,  851,  852,  853,  854,\n",
       "         855,  856,  857,  858,  859,  860,  861,  862,  863,  864,  865,\n",
       "         866,  867,  868,  869,  870,  871,  872,  873,  874,  875,  876,\n",
       "         877,  878,  879,  880,  881,  882,  883,  884,  885,  886,  887,\n",
       "         888,  889,  890,  891,  892,  893,  894,  895,  896],\n",
       "       [ 897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
       "         908,  909,  910,  911,  912,  913,  914,  915,  916,  917,  918,\n",
       "         919,  920,  921,  922,  923,  924,  925,  926,  927,  928,  929,\n",
       "         930,  931,  932,  933,  934,  935,  936,  937,  938,  939,  940,\n",
       "         941,  942,  943,  944,  945,  946,  947,  948,  949,  950,  951,\n",
       "         952,  953,  954,  955,  956,  957,  958,  959,  960],\n",
       "       [ 961,  962,  963,  964,  965,  966,  967,  968,  969,  970,  971,\n",
       "         972,  973,  974,  975,  976,  977,  978,  979,  980,  981,  982,\n",
       "         983,  984,  985,  986,  987,  988,  989,  990,  991,  992,  993,\n",
       "         994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004,\n",
       "        1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
       "        1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024],\n",
       "       [1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035,\n",
       "        1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046,\n",
       "        1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057,\n",
       "        1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068,\n",
       "        1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079,\n",
       "        1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088],\n",
       "       [1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
       "        1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110,\n",
       "        1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121,\n",
       "        1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132,\n",
       "        1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143,\n",
       "        1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152]])>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_query[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
