{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    print(\"Inside 'MultiHeadAttention' class...\")\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    print()\n",
    "    print(\"The shape of 'q' is \" + str(q.shape))\n",
    "    print(\"The shape of 'k' is \" + str(k.shape))\n",
    "    print(\"The shape of 'v' is \" + str(v.shape))\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    print()\n",
    "    print(\"After passing 'q', 'k', 'v' through densely connected layers....\")\n",
    "    print(\"The shape of 'q' is \" + str(q.shape))\n",
    "    print(\"The shape of 'k' is \" + str(k.shape))\n",
    "    print(\"The shape of 'v' is \" + str(v.shape))\n",
    "\n",
    " \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    print()\n",
    "    print(\"After splitting the heads....\")\n",
    "    print(\"The shape of 'q' is \" + str(q.shape))\n",
    "    print(\"The shape of 'k' is \" + str(k.shape))\n",
    "    print(\"The shape of 'v' is \" + str(v.shape))\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    \n",
    "    print()\n",
    "    print(\"The shape of 'attention_weights' is \" + str(attention_weights.shape))\n",
    "\n",
    "\n",
    "    print(\"The shape of 'scaled_attention' is \" + str(scaled_attention.shape))\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "    \n",
    "    print()\n",
    "    print(\"After transposing....\")\n",
    "    print(\"The shape of 'scaled_attention' is \" + str(scaled_attention.shape))\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "    \n",
    "    print()\n",
    "    print(\"The shape of 'concat_attention' is \" + str(concat_attention.shape))\n",
    "    \n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "    print()\n",
    "    print(\"The shape of 'output' is \" + str(output.shape))\n",
    "\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside 'MultiHeadAttention' class...\n",
      "\n",
      "The shape of 'q' is (1, 9, 512)\n",
      "The shape of 'k' is (1, 9, 512)\n",
      "The shape of 'v' is (1, 9, 512)\n",
      "\n",
      "After passing 'q', 'k', 'v' through densely connected layers....\n",
      "The shape of 'q' is (1, 9, 512)\n",
      "The shape of 'k' is (1, 9, 512)\n",
      "The shape of 'v' is (1, 9, 512)\n",
      "\n",
      "After splitting the heads....\n",
      "The shape of 'q' is (1, 8, 9, 64)\n",
      "The shape of 'k' is (1, 8, 9, 64)\n",
      "The shape of 'v' is (1, 8, 9, 64)\n",
      "\n",
      "The shape of 'attention_weights' is (1, 8, 9, 9)\n",
      "The shape of 'scaled_attention' is (1, 8, 9, 64)\n",
      "\n",
      "After transposing....\n",
      "The shape of 'scaled_attention' is (1, 9, 8, 64)\n",
      "\n",
      "The shape of 'concat_attention' is (1, 9, 512)\n",
      "\n",
      "The shape of 'output' is (1, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "sample_sentence = tf.random.uniform((1, 9, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(v=sample_sentence, k=sample_sentence, q=sample_sentence, mask=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 12, 512]), TensorShape([1, 8, 12, 9]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[   1    2    3 ...  510  511  512]\n",
      "  [ 513  514  515 ... 1022 1023 1024]\n",
      "  [1025 1026 1027 ... 1534 1535 1536]\n",
      "  ...\n",
      "  [3073 3074 3075 ... 3582 3583 3584]\n",
      "  [3585 3586 3587 ... 4094 4095 4096]\n",
      "  [4097 4098 4099 ... 4606 4607 4608]]], shape=(1, 9, 512), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_query = np.arange(1*9*512).reshape((1, 9, 512)) + 1\n",
    "sample_query = tf.convert_to_tensor(sample_query)\n",
    "print(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query = tf.reshape(sample_query, (1, 9, 8, 64))\n",
    "sample_query = tf.transpose(sample_query, perm=[0, 2, 1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[   1    2    3 ...   62   63   64]\n",
      "   [ 513  514  515 ...  574  575  576]\n",
      "   [1025 1026 1027 ... 1086 1087 1088]\n",
      "   ...\n",
      "   [3073 3074 3075 ... 3134 3135 3136]\n",
      "   [3585 3586 3587 ... 3646 3647 3648]\n",
      "   [4097 4098 4099 ... 4158 4159 4160]]\n",
      "\n",
      "  [[  65   66   67 ...  126  127  128]\n",
      "   [ 577  578  579 ...  638  639  640]\n",
      "   [1089 1090 1091 ... 1150 1151 1152]\n",
      "   ...\n",
      "   [3137 3138 3139 ... 3198 3199 3200]\n",
      "   [3649 3650 3651 ... 3710 3711 3712]\n",
      "   [4161 4162 4163 ... 4222 4223 4224]]\n",
      "\n",
      "  [[ 129  130  131 ...  190  191  192]\n",
      "   [ 641  642  643 ...  702  703  704]\n",
      "   [1153 1154 1155 ... 1214 1215 1216]\n",
      "   ...\n",
      "   [3201 3202 3203 ... 3262 3263 3264]\n",
      "   [3713 3714 3715 ... 3774 3775 3776]\n",
      "   [4225 4226 4227 ... 4286 4287 4288]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 321  322  323 ...  382  383  384]\n",
      "   [ 833  834  835 ...  894  895  896]\n",
      "   [1345 1346 1347 ... 1406 1407 1408]\n",
      "   ...\n",
      "   [3393 3394 3395 ... 3454 3455 3456]\n",
      "   [3905 3906 3907 ... 3966 3967 3968]\n",
      "   [4417 4418 4419 ... 4478 4479 4480]]\n",
      "\n",
      "  [[ 385  386  387 ...  446  447  448]\n",
      "   [ 897  898  899 ...  958  959  960]\n",
      "   [1409 1410 1411 ... 1470 1471 1472]\n",
      "   ...\n",
      "   [3457 3458 3459 ... 3518 3519 3520]\n",
      "   [3969 3970 3971 ... 4030 4031 4032]\n",
      "   [4481 4482 4483 ... 4542 4543 4544]]\n",
      "\n",
      "  [[ 449  450  451 ...  510  511  512]\n",
      "   [ 961  962  963 ... 1022 1023 1024]\n",
      "   [1473 1474 1475 ... 1534 1535 1536]\n",
      "   ...\n",
      "   [3521 3522 3523 ... 3582 3583 3584]\n",
      "   [4033 4034 4035 ... 4094 4095 4096]\n",
      "   [4545 4546 4547 ... 4606 4607 4608]]]], shape=(1, 8, 9, 64), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   1    2    3    4    5    6    7    8    9   10   11   12   13   14\n",
      "    15   16   17   18   19   20   21   22   23   24   25   26   27   28\n",
      "    29   30   31   32   33   34   35   36   37   38   39   40   41   42\n",
      "    43   44   45   46   47   48   49   50   51   52   53   54   55   56\n",
      "    57   58   59   60   61   62   63   64]\n",
      " [ 513  514  515  516  517  518  519  520  521  522  523  524  525  526\n",
      "   527  528  529  530  531  532  533  534  535  536  537  538  539  540\n",
      "   541  542  543  544  545  546  547  548  549  550  551  552  553  554\n",
      "   555  556  557  558  559  560  561  562  563  564  565  566  567  568\n",
      "   569  570  571  572  573  574  575  576]\n",
      " [1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038\n",
      "  1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052\n",
      "  1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066\n",
      "  1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080\n",
      "  1081 1082 1083 1084 1085 1086 1087 1088]\n",
      " [1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550\n",
      "  1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564\n",
      "  1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578\n",
      "  1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592\n",
      "  1593 1594 1595 1596 1597 1598 1599 1600]\n",
      " [2049 2050 2051 2052 2053 2054 2055 2056 2057 2058 2059 2060 2061 2062\n",
      "  2063 2064 2065 2066 2067 2068 2069 2070 2071 2072 2073 2074 2075 2076\n",
      "  2077 2078 2079 2080 2081 2082 2083 2084 2085 2086 2087 2088 2089 2090\n",
      "  2091 2092 2093 2094 2095 2096 2097 2098 2099 2100 2101 2102 2103 2104\n",
      "  2105 2106 2107 2108 2109 2110 2111 2112]\n",
      " [2561 2562 2563 2564 2565 2566 2567 2568 2569 2570 2571 2572 2573 2574\n",
      "  2575 2576 2577 2578 2579 2580 2581 2582 2583 2584 2585 2586 2587 2588\n",
      "  2589 2590 2591 2592 2593 2594 2595 2596 2597 2598 2599 2600 2601 2602\n",
      "  2603 2604 2605 2606 2607 2608 2609 2610 2611 2612 2613 2614 2615 2616\n",
      "  2617 2618 2619 2620 2621 2622 2623 2624]\n",
      " [3073 3074 3075 3076 3077 3078 3079 3080 3081 3082 3083 3084 3085 3086\n",
      "  3087 3088 3089 3090 3091 3092 3093 3094 3095 3096 3097 3098 3099 3100\n",
      "  3101 3102 3103 3104 3105 3106 3107 3108 3109 3110 3111 3112 3113 3114\n",
      "  3115 3116 3117 3118 3119 3120 3121 3122 3123 3124 3125 3126 3127 3128\n",
      "  3129 3130 3131 3132 3133 3134 3135 3136]\n",
      " [3585 3586 3587 3588 3589 3590 3591 3592 3593 3594 3595 3596 3597 3598\n",
      "  3599 3600 3601 3602 3603 3604 3605 3606 3607 3608 3609 3610 3611 3612\n",
      "  3613 3614 3615 3616 3617 3618 3619 3620 3621 3622 3623 3624 3625 3626\n",
      "  3627 3628 3629 3630 3631 3632 3633 3634 3635 3636 3637 3638 3639 3640\n",
      "  3641 3642 3643 3644 3645 3646 3647 3648]\n",
      " [4097 4098 4099 4100 4101 4102 4103 4104 4105 4106 4107 4108 4109 4110\n",
      "  4111 4112 4113 4114 4115 4116 4117 4118 4119 4120 4121 4122 4123 4124\n",
      "  4125 4126 4127 4128 4129 4130 4131 4132 4133 4134 4135 4136 4137 4138\n",
      "  4139 4140 4141 4142 4143 4144 4145 4146 4147 4148 4149 4150 4151 4152\n",
      "  4153 4154 4155 4156 4157 4158 4159 4160]], shape=(9, 64), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(sample_query[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8, 9, 9), dtype=int64, numpy=\n",
       "array([[[[     89440,    1154400,    2219360,    3284320,    4349280,\n",
       "             5414240,    6479200,    7544160,    8609120],\n",
       "         [   1154400,   18996576,   36838752,   54680928,   72523104,\n",
       "            90365280,  108207456,  126049632,  143891808],\n",
       "         [   2219360,   36838752,   71458144,  106077536,  140696928,\n",
       "           175316320,  209935712,  244555104,  279174496],\n",
       "         [   3284320,   54680928,  106077536,  157474144,  208870752,\n",
       "           260267360,  311663968,  363060576,  414457184],\n",
       "         [   4349280,   72523104,  140696928,  208870752,  277044576,\n",
       "           345218400,  413392224,  481566048,  549739872],\n",
       "         [   5414240,   90365280,  175316320,  260267360,  345218400,\n",
       "           430169440,  515120480,  600071520,  685022560],\n",
       "         [   6479200,  108207456,  209935712,  311663968,  413392224,\n",
       "           515120480,  616848736,  718576992,  820305248],\n",
       "         [   7544160,  126049632,  244555104,  363060576,  481566048,\n",
       "           600071520,  718576992,  837082464,  955587936],\n",
       "         [   8609120,  143891808,  279174496,  414457184,  549739872,\n",
       "           685022560,  820305248,  955587936, 1090870624]],\n",
       "\n",
       "        [[    617824,    3779936,    6942048,   10104160,   13266272,\n",
       "            16428384,   19590496,   22752608,   25914720],\n",
       "         [   3779936,   23719264,   43658592,   63597920,   83537248,\n",
       "           103476576,  123415904,  143355232,  163294560],\n",
       "         [   6942048,   43658592,   80375136,  117091680,  153808224,\n",
       "           190524768,  227241312,  263957856,  300674400],\n",
       "         [  10104160,   63597920,  117091680,  170585440,  224079200,\n",
       "           277572960,  331066720,  384560480,  438054240],\n",
       "         [  13266272,   83537248,  153808224,  224079200,  294350176,\n",
       "           364621152,  434892128,  505163104,  575434080],\n",
       "         [  16428384,  103476576,  190524768,  277572960,  364621152,\n",
       "           451669344,  538717536,  625765728,  712813920],\n",
       "         [  19590496,  123415904,  227241312,  331066720,  434892128,\n",
       "           538717536,  642542944,  746368352,  850193760],\n",
       "         [  22752608,  143355232,  263957856,  384560480,  505163104,\n",
       "           625765728,  746368352,  866970976,  987573600],\n",
       "         [  25914720,  163294560,  300674400,  438054240,  575434080,\n",
       "           712813920,  850193760,  987573600, 1124953440]],\n",
       "\n",
       "        [[   1670496,    6929760,   12189024,   17448288,   22707552,\n",
       "            27966816,   33226080,   38485344,   43744608],\n",
       "         [   6929760,   28966240,   51002720,   73039200,   95075680,\n",
       "           117112160,  139148640,  161185120,  183221600],\n",
       "         [  12189024,   51002720,   89816416,  128630112,  167443808,\n",
       "           206257504,  245071200,  283884896,  322698592],\n",
       "         [  17448288,   73039200,  128630112,  184221024,  239811936,\n",
       "           295402848,  350993760,  406584672,  462175584],\n",
       "         [  22707552,   95075680,  167443808,  239811936,  312180064,\n",
       "           384548192,  456916320,  529284448,  601652576],\n",
       "         [  27966816,  117112160,  206257504,  295402848,  384548192,\n",
       "           473693536,  562838880,  651984224,  741129568],\n",
       "         [  33226080,  139148640,  245071200,  350993760,  456916320,\n",
       "           562838880,  668761440,  774684000,  880606560],\n",
       "         [  38485344,  161185120,  283884896,  406584672,  529284448,\n",
       "           651984224,  774684000,  897383776, 1020083552],\n",
       "         [  43744608,  183221600,  322698592,  462175584,  601652576,\n",
       "           741129568,  880606560, 1020083552, 1159560544]],\n",
       "\n",
       "        [[   3247456,   10603872,   17960288,   25316704,   32673120,\n",
       "            40029536,   47385952,   54742368,   62098784],\n",
       "         [  10603872,   34737504,   58871136,   83004768,  107138400,\n",
       "           131272032,  155405664,  179539296,  203672928],\n",
       "         [  17960288,   58871136,   99781984,  140692832,  181603680,\n",
       "           222514528,  263425376,  304336224,  345247072],\n",
       "         [  25316704,   83004768,  140692832,  198380896,  256068960,\n",
       "           313757024,  371445088,  429133152,  486821216],\n",
       "         [  32673120,  107138400,  181603680,  256068960,  330534240,\n",
       "           404999520,  479464800,  553930080,  628395360],\n",
       "         [  40029536,  131272032,  222514528,  313757024,  404999520,\n",
       "           496242016,  587484512,  678727008,  769969504],\n",
       "         [  47385952,  155405664,  263425376,  371445088,  479464800,\n",
       "           587484512,  695504224,  803523936,  911543648],\n",
       "         [  54742368,  179539296,  304336224,  429133152,  553930080,\n",
       "           678727008,  803523936,  928320864, 1053117792],\n",
       "         [  62098784,  203672928,  345247072,  486821216,  628395360,\n",
       "           769969504,  911543648, 1053117792, 1194691936]],\n",
       "\n",
       "        [[   5348704,   14802272,   24255840,   33709408,   43162976,\n",
       "            52616544,   62070112,   71523680,   80977248],\n",
       "         [  14802272,   41033056,   67263840,   93494624,  119725408,\n",
       "           145956192,  172186976,  198417760,  224648544],\n",
       "         [  24255840,   67263840,  110271840,  153279840,  196287840,\n",
       "           239295840,  282303840,  325311840,  368319840],\n",
       "         [  33709408,   93494624,  153279840,  213065056,  272850272,\n",
       "           332635488,  392420704,  452205920,  511991136],\n",
       "         [  43162976,  119725408,  196287840,  272850272,  349412704,\n",
       "           425975136,  502537568,  579100000,  655662432],\n",
       "         [  52616544,  145956192,  239295840,  332635488,  425975136,\n",
       "           519314784,  612654432,  705994080,  799333728],\n",
       "         [  62070112,  172186976,  282303840,  392420704,  502537568,\n",
       "           612654432,  722771296,  832888160,  943005024],\n",
       "         [  71523680,  198417760,  325311840,  452205920,  579100000,\n",
       "           705994080,  832888160,  959782240, 1086676320],\n",
       "         [  80977248,  224648544,  368319840,  511991136,  655662432,\n",
       "           799333728,  943005024, 1086676320, 1230347616]],\n",
       "\n",
       "        [[   7974240,   19524960,   31075680,   42626400,   54177120,\n",
       "            65727840,   77278560,   88829280,  100380000],\n",
       "         [  19524960,   47852896,   76180832,  104508768,  132836704,\n",
       "           161164640,  189492576,  217820512,  246148448],\n",
       "         [  31075680,   76180832,  121285984,  166391136,  211496288,\n",
       "           256601440,  301706592,  346811744,  391916896],\n",
       "         [  42626400,  104508768,  166391136,  228273504,  290155872,\n",
       "           352038240,  413920608,  475802976,  537685344],\n",
       "         [  54177120,  132836704,  211496288,  290155872,  368815456,\n",
       "           447475040,  526134624,  604794208,  683453792],\n",
       "         [  65727840,  161164640,  256601440,  352038240,  447475040,\n",
       "           542911840,  638348640,  733785440,  829222240],\n",
       "         [  77278560,  189492576,  301706592,  413920608,  526134624,\n",
       "           638348640,  750562656,  862776672,  974990688],\n",
       "         [  88829280,  217820512,  346811744,  475802976,  604794208,\n",
       "           733785440,  862776672,  991767904, 1120759136],\n",
       "         [ 100380000,  246148448,  391916896,  537685344,  683453792,\n",
       "           829222240,  974990688, 1120759136, 1266527584]],\n",
       "\n",
       "        [[  11124064,   24771936,   38419808,   52067680,   65715552,\n",
       "            79363424,   93011296,  106659168,  120307040],\n",
       "         [  24771936,   55197024,   85622112,  116047200,  146472288,\n",
       "           176897376,  207322464,  237747552,  268172640],\n",
       "         [  38419808,   85622112,  132824416,  180026720,  227229024,\n",
       "           274431328,  321633632,  368835936,  416038240],\n",
       "         [  52067680,  116047200,  180026720,  244006240,  307985760,\n",
       "           371965280,  435944800,  499924320,  563903840],\n",
       "         [  65715552,  146472288,  227229024,  307985760,  388742496,\n",
       "           469499232,  550255968,  631012704,  711769440],\n",
       "         [  79363424,  176897376,  274431328,  371965280,  469499232,\n",
       "           567033184,  664567136,  762101088,  859635040],\n",
       "         [  93011296,  207322464,  321633632,  435944800,  550255968,\n",
       "           664567136,  778878304,  893189472, 1007500640],\n",
       "         [ 106659168,  237747552,  368835936,  499924320,  631012704,\n",
       "           762101088,  893189472, 1024277856, 1155366240],\n",
       "         [ 120307040,  268172640,  416038240,  563903840,  711769440,\n",
       "           859635040, 1007500640, 1155366240, 1303231840]],\n",
       "\n",
       "        [[  14798176,   30543200,   46288224,   62033248,   77778272,\n",
       "            93523296,  109268320,  125013344,  140758368],\n",
       "         [  30543200,   63065440,   95587680,  128109920,  160632160,\n",
       "           193154400,  225676640,  258198880,  290721120],\n",
       "         [  46288224,   95587680,  144887136,  194186592,  243486048,\n",
       "           292785504,  342084960,  391384416,  440683872],\n",
       "         [  62033248,  128109920,  194186592,  260263264,  326339936,\n",
       "           392416608,  458493280,  524569952,  590646624],\n",
       "         [  77778272,  160632160,  243486048,  326339936,  409193824,\n",
       "           492047712,  574901600,  657755488,  740609376],\n",
       "         [  93523296,  193154400,  292785504,  392416608,  492047712,\n",
       "           591678816,  691309920,  790941024,  890572128],\n",
       "         [ 109268320,  225676640,  342084960,  458493280,  574901600,\n",
       "           691309920,  807718240,  924126560, 1040534880],\n",
       "         [ 125013344,  258198880,  391384416,  524569952,  657755488,\n",
       "           790941024,  924126560, 1057312096, 1190497632],\n",
       "         [ 140758368,  290721120,  440683872,  590646624,  740609376,\n",
       "           890572128, 1040534880, 1190497632, 1340460384]]]])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(sample_query, sample_query, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 9), dtype=int64, numpy=\n",
       "array([[     89440,    1154400,    2219360,    3284320,    4349280,\n",
       "           5414240,    6479200,    7544160,    8609120],\n",
       "       [   1154400,   18996576,   36838752,   54680928,   72523104,\n",
       "          90365280,  108207456,  126049632,  143891808],\n",
       "       [   2219360,   36838752,   71458144,  106077536,  140696928,\n",
       "         175316320,  209935712,  244555104,  279174496],\n",
       "       [   3284320,   54680928,  106077536,  157474144,  208870752,\n",
       "         260267360,  311663968,  363060576,  414457184],\n",
       "       [   4349280,   72523104,  140696928,  208870752,  277044576,\n",
       "         345218400,  413392224,  481566048,  549739872],\n",
       "       [   5414240,   90365280,  175316320,  260267360,  345218400,\n",
       "         430169440,  515120480,  600071520,  685022560],\n",
       "       [   6479200,  108207456,  209935712,  311663968,  413392224,\n",
       "         515120480,  616848736,  718576992,  820305248],\n",
       "       [   7544160,  126049632,  244555104,  363060576,  481566048,\n",
       "         600071520,  718576992,  837082464,  955587936],\n",
       "       [   8609120,  143891808,  279174496,  414457184,  549739872,\n",
       "         685022560,  820305248,  955587936, 1090870624]])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(sample_query[0][0], tf.transpose(sample_query[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside 'MultiHeadAttention' class...\n",
      "\n",
      "The shape of 'q' is (1, 12, 512)\n",
      "The shape of 'k' is (1, 9, 512)\n",
      "The shape of 'v' is (1, 9, 512)\n",
      "\n",
      "After passing 'q', 'k', 'v' through densely connected layers....\n",
      "The shape of 'q' is (1, 12, 512)\n",
      "The shape of 'k' is (1, 9, 512)\n",
      "The shape of 'v' is (1, 9, 512)\n",
      "\n",
      "After splitting the heads....\n",
      "The shape of 'q' is (1, 8, 12, 64)\n",
      "The shape of 'k' is (1, 8, 9, 64)\n",
      "The shape of 'v' is (1, 8, 9, 64)\n",
      "\n",
      "The shape of 'attention_weights' is (1, 8, 12, 9)\n",
      "The shape of 'scaled_attention' is (1, 8, 12, 64)\n",
      "\n",
      "After transposing....\n",
      "The shape of 'scaled_attention' is (1, 12, 8, 64)\n",
      "\n",
      "The shape of 'concat_attention' is (1, 12, 512)\n",
      "\n",
      "The shape of 'output' is (1, 12, 512)\n"
     ]
    }
   ],
   "source": [
    "# As I mentioned, \"queries\" can be in different language from \"keys\" or \"values.\"\n",
    "# * They are supposed to be different in translation tasks. \n",
    "\n",
    "# In this case you compare \"quries\" in the target language, with the \"keys\" in the original language. \n",
    "# And after that you reweight \"values\" in the original language. \n",
    "\n",
    "# Usually, the numbef or \"queries\" is different from that of \"keys\" or \"values.\" because \n",
    "# translated sentences usually have different number of tokens. \n",
    "\n",
    "# Let's see an example where the number of input sentence is 9 and that of the translated sentence is 12. \n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "sample_sentence_source_lang = tf.random.uniform((1, 9, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "sample_sentence_target_lang = tf.random.uniform((1, 12, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(v=sample_sentence_source_lang, k=sample_sentence_source_lang, q=sample_sentence_target_lang, mask=None)\n",
    "\n",
    "# In the results below, you can see that you reweight the \"values\" in the original sentence with a (12, 9) sized matrix\n",
    "# in each head, and the the size of the resulting 'scaled_attention' is (12, 64) in each head. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
