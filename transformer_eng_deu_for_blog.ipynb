{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:25:16.887127Z",
     "iopub.status.busy": "2020-09-27T01:25:16.886350Z",
     "iopub.status.idle": "2020-09-27T01:25:23.514378Z",
     "shell.execute_reply": "2020-09-27T01:25:23.513828Z"
    },
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from bpemb import BPEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_de = BPEmb(lang='de', vs=10000, dim=100)\n",
    "bpemb_en = BPEmb(lang='en', vs=10000, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"./datasets/deu.txt\"\n",
    "\n",
    "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "temp_list = []\n",
    "corpus = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    temp_list =  lines[i].split('\\t')[:-1]\n",
    "    corpus.append(temp_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, de = np.array(corpus).T\n",
    "\n",
    "en_encoded = []\n",
    "de_encoded = []\n",
    "\n",
    "cnt_en = 0\n",
    "cnt_de = 0\n",
    "\n",
    "for i in range(len(en)):\n",
    "    en_encoded_temp = bpemb_en.encode_ids(en[i])\n",
    "    de_encoded_temp = bpemb_de.encode_ids(de[i])\n",
    "\n",
    "    if (len(en_encoded_temp)<=40) and (len(de_encoded_temp)<=40):\n",
    "        en_encoded.append([10000] + en_encoded_temp + [10001])\n",
    "        de_encoded.append([10000] + de_encoded_temp + [10001])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_padded = tf.keras.preprocessing.sequence.pad_sequences(en_encoded, padding='post')\n",
    "de_padded = tf.keras.preprocessing.sequence.pad_sequences(de_encoded, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(en_padded, de_padded, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:45.616318Z",
     "iopub.status.busy": "2020-09-27T01:27:45.615771Z",
     "iopub.status.idle": "2020-09-27T01:27:45.617969Z",
     "shell.execute_reply": "2020-09-27T01:27:45.617431Z"
    },
    "id": "bcRp7VcQ5m6g"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89e3671f847d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:45.847530Z",
     "iopub.status.busy": "2020-09-27T01:27:45.846912Z",
     "iopub.status.idle": "2020-09-27T01:27:45.848626Z",
     "shell.execute_reply": "2020-09-27T01:27:45.849030Z"
    },
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:46.374095Z",
     "iopub.status.busy": "2020-09-27T01:27:46.373573Z",
     "iopub.status.idle": "2020-09-27T01:27:46.375314Z",
     "shell.execute_reply": "2020-09-27T01:27:46.375664Z"
    },
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:46.617545Z",
     "iopub.status.busy": "2020-09-27T01:27:46.616460Z",
     "iopub.status.idle": "2020-09-27T01:27:46.619254Z",
     "shell.execute_reply": "2020-09-27T01:27:46.619670Z"
    },
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:46.659573Z",
     "iopub.status.busy": "2020-09-27T01:27:46.658915Z",
     "iopub.status.idle": "2020-09-27T01:27:46.661172Z",
     "shell.execute_reply": "2020-09-27T01:27:46.660671Z"
    },
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:46.699328Z",
     "iopub.status.busy": "2020-09-27T01:27:46.698640Z",
     "iopub.status.idle": "2020-09-27T01:27:46.700613Z",
     "shell.execute_reply": "2020-09-27T01:27:46.700958Z"
    },
    "id": "ncyS-Ms3i2x_"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:46.765993Z",
     "iopub.status.busy": "2020-09-27T01:27:46.765450Z",
     "iopub.status.idle": "2020-09-27T01:27:46.767350Z",
     "shell.execute_reply": "2020-09-27T01:27:46.766844Z"
    },
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "    \n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:46.839967Z",
     "iopub.status.busy": "2020-09-27T01:27:46.839333Z",
     "iopub.status.idle": "2020-09-27T01:27:46.841099Z",
     "shell.execute_reply": "2020-09-27T01:27:46.841443Z"
    },
    "id": "jpEox7gJ8FCI"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    #self.pre_embedding = tf.keras.layers.Dense(input_vocab_size, 100)\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "    \n",
    "    \n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "  \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    \n",
    "    #print(\"The shape of 'x' is \" + str(tf.shape(x)))\n",
    "    #x = self.pre_embedding(x)\n",
    "    #print(\"After self embedding.....\")\n",
    "    #print(\"The shape of 'x' is \" + str(tf.shape(x)))\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "    \n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "    return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:47.163368Z",
     "iopub.status.busy": "2020-09-27T01:27:47.162774Z",
     "iopub.status.idle": "2020-09-27T01:27:47.164447Z",
     "shell.execute_reply": "2020-09-27T01:27:47.164782Z"
    },
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    #self.pre_embedding = tf.keras.layers.Dense(target_vocab_size, 100)\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    \n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    #print(\"The shape of 'x' is \" + str(tf.shape(x)))\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    #print(\"'seq_len' is \" + str(seq_len))\n",
    "    \n",
    "    attention_weights = {}\n",
    "    \n",
    "    #x = self.pre_embedding(x)\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "    \n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "      \n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    #print(\"The shape of 'x' is \" + str(tf.shape(x)))\n",
    "    \n",
    "    return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:47.394726Z",
     "iopub.status.busy": "2020-09-27T01:27:47.394164Z",
     "iopub.status.idle": "2020-09-27T01:27:47.395964Z",
     "shell.execute_reply": "2020-09-27T01:27:47.396326Z"
    },
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "    \n",
    "    '''\n",
    "    The output of the last layer of the encoder is passed to all the layers of the decoder. \n",
    "    '''\n",
    "    #dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "    \n",
    "    '''\n",
    "    Tee final part of Transformer model. In case of machine translation, you predict a \n",
    "    'target_vocab_size' dimensional vector at every potition of the target sentence. \n",
    "    '''\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:47.931823Z",
     "iopub.status.busy": "2020-09-27T01:27:47.931263Z",
     "iopub.status.idle": "2020-09-27T01:27:47.933193Z",
     "shell.execute_reply": "2020-09-27T01:27:47.933543Z"
    },
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 10000 + 2\n",
    "target_vocab_size = 10000 + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:47.937203Z",
     "iopub.status.busy": "2020-09-27T01:27:47.936656Z",
     "iopub.status.idle": "2020-09-27T01:27:47.939422Z",
     "shell.execute_reply": "2020-09-27T01:27:47.938920Z"
    },
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.082201Z",
     "iopub.status.busy": "2020-09-27T01:27:48.081646Z",
     "iopub.status.idle": "2020-09-27T01:27:48.083932Z",
     "shell.execute_reply": "2020-09-27T01:27:48.083447Z"
    },
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.088318Z",
     "iopub.status.busy": "2020-09-27T01:27:48.087763Z",
     "iopub.status.idle": "2020-09-27T01:27:48.090088Z",
     "shell.execute_reply": "2020-09-27T01:27:48.089616Z"
    },
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  \n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.099267Z",
     "iopub.status.busy": "2020-09-27T01:27:48.098546Z",
     "iopub.status.idle": "2020-09-27T01:27:48.108840Z",
     "shell.execute_reply": "2020-09-27T01:27:48.109262Z"
    },
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.114357Z",
     "iopub.status.busy": "2020-09-27T01:27:48.113730Z",
     "iopub.status.idle": "2020-09-27T01:27:48.258900Z",
     "shell.execute_reply": "2020-09-27T01:27:48.259413Z"
    },
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.264274Z",
     "iopub.status.busy": "2020-09-27T01:27:48.263688Z",
     "iopub.status.idle": "2020-09-27T01:27:48.265626Z",
     "shell.execute_reply": "2020-09-27T01:27:48.266073Z"
    },
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.271010Z",
     "iopub.status.busy": "2020-09-27T01:27:48.270443Z",
     "iopub.status.idle": "2020-09-27T01:27:48.272778Z",
     "shell.execute_reply": "2020-09-27T01:27:48.272356Z"
    },
    "id": "hNhuYfllndLZ"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints_deu/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.276032Z",
     "iopub.status.busy": "2020-09-27T01:27:48.275504Z",
     "iopub.status.idle": "2020-09-27T01:27:48.277426Z",
     "shell.execute_reply": "2020-09-27T01:27:48.277775Z"
    },
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.284178Z",
     "iopub.status.busy": "2020-09-27T01:27:48.283590Z",
     "iopub.status.idle": "2020-09-27T01:27:48.285953Z",
     "shell.execute_reply": "2020-09-27T01:27:48.285551Z"
    },
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-27T01:27:48.292269Z",
     "iopub.status.busy": "2020-09-27T01:27:48.291673Z",
     "iopub.status.idle": "2020-09-27T01:38:11.257344Z",
     "shell.execute_reply": "2020-09-27T01:38:11.256866Z"
    },
    "id": "bbvmaKNiznHZ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.7406 Accuracy 0.2195\n",
      "Epoch 1 Batch 50 Loss 0.7292 Accuracy 0.2199\n",
      "Epoch 1 Batch 100 Loss 0.7118 Accuracy 0.2190\n",
      "Epoch 1 Batch 150 Loss 0.7136 Accuracy 0.2198\n",
      "Epoch 1 Batch 200 Loss 0.7044 Accuracy 0.2200\n",
      "Epoch 1 Batch 250 Loss 0.7053 Accuracy 0.2200\n",
      "Epoch 1 Batch 300 Loss 0.7083 Accuracy 0.2197\n",
      "Epoch 1 Batch 350 Loss 0.7109 Accuracy 0.2195\n",
      "Epoch 1 Batch 400 Loss 0.7130 Accuracy 0.2195\n",
      "Epoch 1 Batch 450 Loss 0.7103 Accuracy 0.2197\n",
      "Epoch 1 Batch 500 Loss 0.7128 Accuracy 0.2196\n",
      "Epoch 1 Batch 550 Loss 0.7142 Accuracy 0.2198\n",
      "Epoch 1 Batch 600 Loss 0.7158 Accuracy 0.2199\n",
      "Epoch 1 Batch 650 Loss 0.7182 Accuracy 0.2199\n",
      "Epoch 1 Batch 700 Loss 0.7188 Accuracy 0.2199\n",
      "Epoch 1 Batch 750 Loss 0.7191 Accuracy 0.2199\n",
      "Epoch 1 Batch 800 Loss 0.7208 Accuracy 0.2199\n",
      "Epoch 1 Batch 850 Loss 0.7210 Accuracy 0.2199\n",
      "Epoch 1 Batch 900 Loss 0.7232 Accuracy 0.2198\n",
      "Epoch 1 Batch 950 Loss 0.7233 Accuracy 0.2196\n",
      "Epoch 1 Batch 1000 Loss 0.7253 Accuracy 0.2196\n",
      "Epoch 1 Batch 1050 Loss 0.7267 Accuracy 0.2195\n",
      "Epoch 1 Batch 1100 Loss 0.7268 Accuracy 0.2195\n",
      "Epoch 1 Batch 1150 Loss 0.7272 Accuracy 0.2195\n",
      "Epoch 1 Batch 1200 Loss 0.7294 Accuracy 0.2195\n",
      "Epoch 1 Batch 1250 Loss 0.7299 Accuracy 0.2195\n",
      "Epoch 1 Batch 1300 Loss 0.7312 Accuracy 0.2195\n",
      "Epoch 1 Batch 1350 Loss 0.7313 Accuracy 0.2194\n",
      "Epoch 1 Batch 1400 Loss 0.7324 Accuracy 0.2194\n",
      "Epoch 1 Batch 1450 Loss 0.7324 Accuracy 0.2195\n",
      "Epoch 1 Batch 1500 Loss 0.7335 Accuracy 0.2194\n",
      "Epoch 1 Batch 1550 Loss 0.7342 Accuracy 0.2194\n",
      "Epoch 1 Batch 1600 Loss 0.7347 Accuracy 0.2194\n",
      "Epoch 1 Batch 1650 Loss 0.7353 Accuracy 0.2194\n",
      "Epoch 1 Batch 1700 Loss 0.7353 Accuracy 0.2195\n",
      "Epoch 1 Batch 1750 Loss 0.7348 Accuracy 0.2195\n",
      "Epoch 1 Batch 1800 Loss 0.7350 Accuracy 0.2194\n",
      "Epoch 1 Batch 1850 Loss 0.7352 Accuracy 0.2194\n",
      "Epoch 1 Batch 1900 Loss 0.7362 Accuracy 0.2194\n",
      "Epoch 1 Batch 1950 Loss 0.7366 Accuracy 0.2194\n",
      "Epoch 1 Batch 2000 Loss 0.7365 Accuracy 0.2194\n",
      "Epoch 1 Batch 2050 Loss 0.7366 Accuracy 0.2194\n",
      "Epoch 1 Batch 2100 Loss 0.7370 Accuracy 0.2193\n",
      "Epoch 1 Batch 2150 Loss 0.7371 Accuracy 0.2193\n",
      "Epoch 1 Batch 2200 Loss 0.7380 Accuracy 0.2192\n",
      "Epoch 1 Batch 2250 Loss 0.7391 Accuracy 0.2192\n",
      "Epoch 1 Batch 2300 Loss 0.7392 Accuracy 0.2192\n",
      "Epoch 1 Batch 2350 Loss 0.7398 Accuracy 0.2191\n",
      "Epoch 1 Batch 2400 Loss 0.7401 Accuracy 0.2192\n",
      "Epoch 1 Batch 2450 Loss 0.7404 Accuracy 0.2192\n",
      "Epoch 1 Batch 2500 Loss 0.7407 Accuracy 0.2192\n",
      "Epoch 1 Batch 2550 Loss 0.7412 Accuracy 0.2192\n",
      "Epoch 1 Batch 2600 Loss 0.7416 Accuracy 0.2192\n",
      "Epoch 1 Batch 2650 Loss 0.7420 Accuracy 0.2191\n",
      "Epoch 1 Batch 2700 Loss 0.7423 Accuracy 0.2192\n",
      "Epoch 1 Batch 2750 Loss 0.7424 Accuracy 0.2192\n",
      "Epoch 1 Batch 2800 Loss 0.7422 Accuracy 0.2191\n",
      "Epoch 1 Loss 0.7422 Accuracy 0.2191\n",
      "Time taken for 1 epoch: 1135.0273399353027 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7515 Accuracy 0.2092\n",
      "Epoch 2 Batch 50 Loss 0.6609 Accuracy 0.2220\n",
      "Epoch 2 Batch 100 Loss 0.6683 Accuracy 0.2229\n",
      "Epoch 2 Batch 150 Loss 0.6741 Accuracy 0.2222\n",
      "Epoch 2 Batch 200 Loss 0.6767 Accuracy 0.2222\n",
      "Epoch 2 Batch 250 Loss 0.6809 Accuracy 0.2218\n",
      "Epoch 2 Batch 300 Loss 0.6851 Accuracy 0.2216\n",
      "Epoch 2 Batch 350 Loss 0.6842 Accuracy 0.2215\n",
      "Epoch 2 Batch 400 Loss 0.6845 Accuracy 0.2214\n",
      "Epoch 2 Batch 450 Loss 0.6860 Accuracy 0.2215\n",
      "Epoch 2 Batch 500 Loss 0.6890 Accuracy 0.2214\n",
      "Epoch 2 Batch 550 Loss 0.6895 Accuracy 0.2212\n",
      "Epoch 2 Batch 600 Loss 0.6924 Accuracy 0.2211\n",
      "Epoch 2 Batch 650 Loss 0.6936 Accuracy 0.2211\n",
      "Epoch 2 Batch 700 Loss 0.6955 Accuracy 0.2210\n",
      "Epoch 2 Batch 750 Loss 0.6975 Accuracy 0.2210\n",
      "Epoch 2 Batch 800 Loss 0.6999 Accuracy 0.2209\n",
      "Epoch 2 Batch 850 Loss 0.7008 Accuracy 0.2208\n",
      "Epoch 2 Batch 900 Loss 0.7027 Accuracy 0.2207\n",
      "Epoch 2 Batch 950 Loss 0.7046 Accuracy 0.2206\n",
      "Epoch 2 Batch 1000 Loss 0.7070 Accuracy 0.2206\n",
      "Epoch 2 Batch 1050 Loss 0.7063 Accuracy 0.2206\n",
      "Epoch 2 Batch 1100 Loss 0.7068 Accuracy 0.2207\n",
      "Epoch 2 Batch 1150 Loss 0.7079 Accuracy 0.2206\n",
      "Epoch 2 Batch 1200 Loss 0.7088 Accuracy 0.2207\n",
      "Epoch 2 Batch 1250 Loss 0.7098 Accuracy 0.2206\n",
      "Epoch 2 Batch 1300 Loss 0.7094 Accuracy 0.2206\n",
      "Epoch 2 Batch 1350 Loss 0.7099 Accuracy 0.2206\n",
      "Epoch 2 Batch 1400 Loss 0.7099 Accuracy 0.2206\n",
      "Epoch 2 Batch 1450 Loss 0.7108 Accuracy 0.2205\n",
      "Epoch 2 Batch 1500 Loss 0.7112 Accuracy 0.2204\n",
      "Epoch 2 Batch 1550 Loss 0.7116 Accuracy 0.2204\n",
      "Epoch 2 Batch 1600 Loss 0.7117 Accuracy 0.2203\n",
      "Epoch 2 Batch 1650 Loss 0.7118 Accuracy 0.2203\n",
      "Epoch 2 Batch 1700 Loss 0.7133 Accuracy 0.2202\n",
      "Epoch 2 Batch 1750 Loss 0.7141 Accuracy 0.2201\n",
      "Epoch 2 Batch 1800 Loss 0.7154 Accuracy 0.2200\n",
      "Epoch 2 Batch 1850 Loss 0.7158 Accuracy 0.2200\n",
      "Epoch 2 Batch 1900 Loss 0.7166 Accuracy 0.2200\n",
      "Epoch 2 Batch 1950 Loss 0.7171 Accuracy 0.2200\n",
      "Epoch 2 Batch 2000 Loss 0.7179 Accuracy 0.2199\n",
      "Epoch 2 Batch 2050 Loss 0.7183 Accuracy 0.2200\n",
      "Epoch 2 Batch 2100 Loss 0.7192 Accuracy 0.2199\n",
      "Epoch 2 Batch 2150 Loss 0.7195 Accuracy 0.2199\n",
      "Epoch 2 Batch 2200 Loss 0.7204 Accuracy 0.2199\n",
      "Epoch 2 Batch 2250 Loss 0.7211 Accuracy 0.2198\n",
      "Epoch 2 Batch 2300 Loss 0.7216 Accuracy 0.2198\n",
      "Epoch 2 Batch 2350 Loss 0.7223 Accuracy 0.2198\n",
      "Epoch 2 Batch 2400 Loss 0.7226 Accuracy 0.2197\n",
      "Epoch 2 Batch 2450 Loss 0.7232 Accuracy 0.2197\n",
      "Epoch 2 Batch 2500 Loss 0.7237 Accuracy 0.2197\n",
      "Epoch 2 Batch 2550 Loss 0.7238 Accuracy 0.2197\n",
      "Epoch 2 Batch 2600 Loss 0.7244 Accuracy 0.2197\n",
      "Epoch 2 Batch 2650 Loss 0.7241 Accuracy 0.2197\n",
      "Epoch 2 Batch 2700 Loss 0.7247 Accuracy 0.2197\n",
      "Epoch 2 Batch 2750 Loss 0.7257 Accuracy 0.2196\n",
      "Epoch 2 Batch 2800 Loss 0.7260 Accuracy 0.2196\n",
      "Epoch 2 Loss 0.7259 Accuracy 0.2196\n",
      "Time taken for 1 epoch: 1048.8350141048431 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.5168 Accuracy 0.2153\n",
      "Epoch 3 Batch 50 Loss 0.6707 Accuracy 0.2239\n",
      "Epoch 3 Batch 100 Loss 0.6746 Accuracy 0.2221\n",
      "Epoch 3 Batch 150 Loss 0.6775 Accuracy 0.2219\n",
      "Epoch 3 Batch 200 Loss 0.6749 Accuracy 0.2216\n",
      "Epoch 3 Batch 250 Loss 0.6731 Accuracy 0.2223\n",
      "Epoch 3 Batch 300 Loss 0.6771 Accuracy 0.2221\n",
      "Epoch 3 Batch 350 Loss 0.6775 Accuracy 0.2220\n",
      "Epoch 3 Batch 400 Loss 0.6790 Accuracy 0.2217\n",
      "Epoch 3 Batch 450 Loss 0.6803 Accuracy 0.2217\n",
      "Epoch 3 Batch 500 Loss 0.6837 Accuracy 0.2216\n",
      "Epoch 3 Batch 550 Loss 0.6850 Accuracy 0.2214\n",
      "Epoch 3 Batch 600 Loss 0.6891 Accuracy 0.2213\n",
      "Epoch 3 Batch 650 Loss 0.6899 Accuracy 0.2212\n",
      "Epoch 3 Batch 700 Loss 0.6896 Accuracy 0.2210\n",
      "Epoch 3 Batch 750 Loss 0.6892 Accuracy 0.2209\n",
      "Epoch 3 Batch 800 Loss 0.6889 Accuracy 0.2211\n",
      "Epoch 3 Batch 850 Loss 0.6916 Accuracy 0.2210\n",
      "Epoch 3 Batch 900 Loss 0.6938 Accuracy 0.2208\n",
      "Epoch 3 Batch 950 Loss 0.6951 Accuracy 0.2208\n",
      "Epoch 3 Batch 1000 Loss 0.6954 Accuracy 0.2207\n",
      "Epoch 3 Batch 1050 Loss 0.6982 Accuracy 0.2206\n",
      "Epoch 3 Batch 1100 Loss 0.6990 Accuracy 0.2207\n",
      "Epoch 3 Batch 1150 Loss 0.6994 Accuracy 0.2206\n",
      "Epoch 3 Batch 1200 Loss 0.7007 Accuracy 0.2206\n",
      "Epoch 3 Batch 1250 Loss 0.7005 Accuracy 0.2206\n",
      "Epoch 3 Batch 1300 Loss 0.7011 Accuracy 0.2206\n",
      "Epoch 3 Batch 1350 Loss 0.7011 Accuracy 0.2206\n",
      "Epoch 3 Batch 1400 Loss 0.7030 Accuracy 0.2204\n",
      "Epoch 3 Batch 1450 Loss 0.7038 Accuracy 0.2205\n",
      "Epoch 3 Batch 1500 Loss 0.7038 Accuracy 0.2204\n",
      "Epoch 3 Batch 1550 Loss 0.7044 Accuracy 0.2203\n",
      "Epoch 3 Batch 1600 Loss 0.7052 Accuracy 0.2202\n",
      "Epoch 3 Batch 1650 Loss 0.7055 Accuracy 0.2202\n",
      "Epoch 3 Batch 1700 Loss 0.7051 Accuracy 0.2202\n",
      "Epoch 3 Batch 1750 Loss 0.7052 Accuracy 0.2201\n",
      "Epoch 3 Batch 1800 Loss 0.7052 Accuracy 0.2201\n",
      "Epoch 3 Batch 1850 Loss 0.7060 Accuracy 0.2202\n",
      "Epoch 3 Batch 1900 Loss 0.7068 Accuracy 0.2201\n",
      "Epoch 3 Batch 1950 Loss 0.7080 Accuracy 0.2201\n",
      "Epoch 3 Batch 2000 Loss 0.7085 Accuracy 0.2201\n",
      "Epoch 3 Batch 2050 Loss 0.7090 Accuracy 0.2200\n",
      "Epoch 3 Batch 2100 Loss 0.7100 Accuracy 0.2200\n",
      "Epoch 3 Batch 2150 Loss 0.7105 Accuracy 0.2200\n",
      "Epoch 3 Batch 2200 Loss 0.7111 Accuracy 0.2199\n",
      "Epoch 3 Batch 2250 Loss 0.7116 Accuracy 0.2199\n",
      "Epoch 3 Batch 2300 Loss 0.7127 Accuracy 0.2198\n",
      "Epoch 3 Batch 2350 Loss 0.7131 Accuracy 0.2199\n",
      "Epoch 3 Batch 2400 Loss 0.7145 Accuracy 0.2199\n",
      "Epoch 3 Batch 2450 Loss 0.7151 Accuracy 0.2199\n",
      "Epoch 3 Batch 2500 Loss 0.7155 Accuracy 0.2198\n",
      "Epoch 3 Batch 2550 Loss 0.7153 Accuracy 0.2199\n",
      "Epoch 3 Batch 2600 Loss 0.7155 Accuracy 0.2198\n",
      "Epoch 3 Batch 2650 Loss 0.7155 Accuracy 0.2198\n",
      "Epoch 3 Batch 2700 Loss 0.7155 Accuracy 0.2199\n",
      "Epoch 3 Batch 2750 Loss 0.7158 Accuracy 0.2198\n",
      "Epoch 3 Batch 2800 Loss 0.7162 Accuracy 0.2198\n",
      "Epoch 3 Loss 0.7163 Accuracy 0.2198\n",
      "Time taken for 1 epoch: 1028.9510340690613 secs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0 Loss 0.5321 Accuracy 0.2306\n",
      "Epoch 4 Batch 50 Loss 0.6658 Accuracy 0.2194\n",
      "Epoch 4 Batch 100 Loss 0.6744 Accuracy 0.2193\n",
      "Epoch 4 Batch 150 Loss 0.6730 Accuracy 0.2197\n",
      "Epoch 4 Batch 200 Loss 0.6725 Accuracy 0.2203\n",
      "Epoch 4 Batch 250 Loss 0.6716 Accuracy 0.2206\n",
      "Epoch 4 Batch 300 Loss 0.6690 Accuracy 0.2206\n",
      "Epoch 4 Batch 350 Loss 0.6690 Accuracy 0.2209\n",
      "Epoch 4 Batch 400 Loss 0.6712 Accuracy 0.2211\n",
      "Epoch 4 Batch 450 Loss 0.6713 Accuracy 0.2209\n",
      "Epoch 4 Batch 500 Loss 0.6745 Accuracy 0.2213\n",
      "Epoch 4 Batch 550 Loss 0.6747 Accuracy 0.2213\n",
      "Epoch 4 Batch 600 Loss 0.6764 Accuracy 0.2213\n",
      "Epoch 4 Batch 650 Loss 0.6796 Accuracy 0.2214\n",
      "Epoch 4 Batch 700 Loss 0.6783 Accuracy 0.2214\n",
      "Epoch 4 Batch 750 Loss 0.6792 Accuracy 0.2214\n",
      "Epoch 4 Batch 800 Loss 0.6799 Accuracy 0.2213\n",
      "Epoch 4 Batch 850 Loss 0.6803 Accuracy 0.2213\n",
      "Epoch 4 Batch 900 Loss 0.6805 Accuracy 0.2213\n",
      "Epoch 4 Batch 950 Loss 0.6812 Accuracy 0.2213\n",
      "Epoch 4 Batch 1000 Loss 0.6832 Accuracy 0.2212\n",
      "Epoch 4 Batch 1050 Loss 0.6846 Accuracy 0.2212\n",
      "Epoch 4 Batch 1100 Loss 0.6847 Accuracy 0.2212\n",
      "Epoch 4 Batch 1150 Loss 0.6853 Accuracy 0.2212\n",
      "Epoch 4 Batch 1200 Loss 0.6865 Accuracy 0.2210\n",
      "Epoch 4 Batch 1250 Loss 0.6865 Accuracy 0.2210\n",
      "Epoch 4 Batch 1300 Loss 0.6868 Accuracy 0.2210\n",
      "Epoch 4 Batch 1350 Loss 0.6877 Accuracy 0.2210\n",
      "Epoch 4 Batch 1400 Loss 0.6881 Accuracy 0.2209\n",
      "Epoch 4 Batch 1450 Loss 0.6896 Accuracy 0.2209\n",
      "Epoch 4 Batch 1500 Loss 0.6907 Accuracy 0.2209\n",
      "Epoch 4 Batch 1550 Loss 0.6920 Accuracy 0.2209\n",
      "Epoch 4 Batch 1600 Loss 0.6936 Accuracy 0.2209\n",
      "Epoch 4 Batch 1650 Loss 0.6942 Accuracy 0.2210\n",
      "Epoch 4 Batch 1700 Loss 0.6953 Accuracy 0.2210\n",
      "Epoch 4 Batch 1750 Loss 0.6962 Accuracy 0.2210\n",
      "Epoch 4 Batch 1800 Loss 0.6962 Accuracy 0.2209\n",
      "Epoch 4 Batch 1850 Loss 0.6966 Accuracy 0.2209\n",
      "Epoch 4 Batch 1900 Loss 0.6971 Accuracy 0.2208\n",
      "Epoch 4 Batch 1950 Loss 0.6984 Accuracy 0.2207\n",
      "Epoch 4 Batch 2000 Loss 0.6990 Accuracy 0.2207\n",
      "Epoch 4 Batch 2050 Loss 0.7001 Accuracy 0.2206\n",
      "Epoch 4 Batch 2100 Loss 0.7011 Accuracy 0.2205\n",
      "Epoch 4 Batch 2150 Loss 0.7023 Accuracy 0.2205\n",
      "Epoch 4 Batch 2200 Loss 0.7030 Accuracy 0.2205\n",
      "Epoch 4 Batch 2250 Loss 0.7039 Accuracy 0.2204\n",
      "Epoch 4 Batch 2300 Loss 0.7044 Accuracy 0.2204\n",
      "Epoch 4 Batch 2350 Loss 0.7054 Accuracy 0.2203\n",
      "Epoch 4 Batch 2400 Loss 0.7063 Accuracy 0.2203\n",
      "Epoch 4 Batch 2450 Loss 0.7065 Accuracy 0.2203\n",
      "Epoch 4 Batch 2500 Loss 0.7067 Accuracy 0.2203\n",
      "Epoch 4 Batch 2550 Loss 0.7073 Accuracy 0.2202\n",
      "Epoch 4 Batch 2600 Loss 0.7075 Accuracy 0.2203\n",
      "Epoch 4 Batch 2650 Loss 0.7084 Accuracy 0.2203\n",
      "Epoch 4 Batch 2700 Loss 0.7086 Accuracy 0.2202\n",
      "Epoch 4 Batch 2750 Loss 0.7089 Accuracy 0.2202\n",
      "Epoch 4 Batch 2800 Loss 0.7091 Accuracy 0.2202\n",
      "Epoch 4 Loss 0.7091 Accuracy 0.2202\n",
      "Time taken for 1 epoch: 1028.6685481071472 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5887 Accuracy 0.2401\n",
      "Epoch 5 Batch 50 Loss 0.6345 Accuracy 0.2240\n",
      "Epoch 5 Batch 100 Loss 0.6578 Accuracy 0.2224\n",
      "Epoch 5 Batch 150 Loss 0.6645 Accuracy 0.2217\n",
      "Epoch 5 Batch 200 Loss 0.6639 Accuracy 0.2223\n",
      "Epoch 5 Batch 250 Loss 0.6651 Accuracy 0.2219\n",
      "Epoch 5 Batch 300 Loss 0.6667 Accuracy 0.2220\n",
      "Epoch 5 Batch 350 Loss 0.6685 Accuracy 0.2216\n",
      "Epoch 5 Batch 400 Loss 0.6698 Accuracy 0.2217\n",
      "Epoch 5 Batch 450 Loss 0.6709 Accuracy 0.2214\n",
      "Epoch 5 Batch 500 Loss 0.6725 Accuracy 0.2215\n",
      "Epoch 5 Batch 550 Loss 0.6715 Accuracy 0.2213\n",
      "Epoch 5 Batch 600 Loss 0.6713 Accuracy 0.2213\n",
      "Epoch 5 Batch 650 Loss 0.6705 Accuracy 0.2212\n",
      "Epoch 5 Batch 700 Loss 0.6727 Accuracy 0.2213\n",
      "Epoch 5 Batch 750 Loss 0.6737 Accuracy 0.2213\n",
      "Epoch 5 Batch 800 Loss 0.6736 Accuracy 0.2213\n",
      "Epoch 5 Batch 850 Loss 0.6754 Accuracy 0.2212\n",
      "Epoch 5 Batch 900 Loss 0.6760 Accuracy 0.2210\n",
      "Epoch 5 Batch 950 Loss 0.6759 Accuracy 0.2210\n",
      "Epoch 5 Batch 1000 Loss 0.6772 Accuracy 0.2209\n",
      "Epoch 5 Batch 1050 Loss 0.6777 Accuracy 0.2210\n",
      "Epoch 5 Batch 1100 Loss 0.6786 Accuracy 0.2210\n",
      "Epoch 5 Batch 1150 Loss 0.6803 Accuracy 0.2210\n",
      "Epoch 5 Batch 1200 Loss 0.6808 Accuracy 0.2210\n",
      "Epoch 5 Batch 1250 Loss 0.6822 Accuracy 0.2210\n",
      "Epoch 5 Batch 1300 Loss 0.6826 Accuracy 0.2209\n",
      "Epoch 5 Batch 1350 Loss 0.6841 Accuracy 0.2209\n",
      "Epoch 5 Batch 1400 Loss 0.6848 Accuracy 0.2209\n",
      "Epoch 5 Batch 1450 Loss 0.6861 Accuracy 0.2208\n",
      "Epoch 5 Batch 1500 Loss 0.6866 Accuracy 0.2208\n",
      "Epoch 5 Batch 1550 Loss 0.6881 Accuracy 0.2208\n",
      "Epoch 5 Batch 1600 Loss 0.6886 Accuracy 0.2208\n",
      "Epoch 5 Batch 1650 Loss 0.6893 Accuracy 0.2208\n",
      "Epoch 5 Batch 1700 Loss 0.6895 Accuracy 0.2208\n",
      "Epoch 5 Batch 1750 Loss 0.6898 Accuracy 0.2208\n",
      "Epoch 5 Batch 1800 Loss 0.6909 Accuracy 0.2207\n",
      "Epoch 5 Batch 1850 Loss 0.6914 Accuracy 0.2207\n",
      "Epoch 5 Batch 1900 Loss 0.6922 Accuracy 0.2207\n",
      "Epoch 5 Batch 1950 Loss 0.6930 Accuracy 0.2208\n",
      "Epoch 5 Batch 2000 Loss 0.6929 Accuracy 0.2208\n",
      "Epoch 5 Batch 2050 Loss 0.6932 Accuracy 0.2208\n",
      "Epoch 5 Batch 2100 Loss 0.6934 Accuracy 0.2209\n",
      "Epoch 5 Batch 2150 Loss 0.6942 Accuracy 0.2208\n",
      "Epoch 5 Batch 2200 Loss 0.6951 Accuracy 0.2208\n",
      "Epoch 5 Batch 2250 Loss 0.6956 Accuracy 0.2208\n",
      "Epoch 5 Batch 2300 Loss 0.6967 Accuracy 0.2207\n",
      "Epoch 5 Batch 2350 Loss 0.6975 Accuracy 0.2207\n",
      "Epoch 5 Batch 2400 Loss 0.6979 Accuracy 0.2206\n",
      "Epoch 5 Batch 2450 Loss 0.6985 Accuracy 0.2206\n",
      "Epoch 5 Batch 2500 Loss 0.6987 Accuracy 0.2206\n",
      "Epoch 5 Batch 2550 Loss 0.6991 Accuracy 0.2206\n",
      "Epoch 5 Batch 2600 Loss 0.6997 Accuracy 0.2206\n",
      "Epoch 5 Batch 2650 Loss 0.7006 Accuracy 0.2205\n",
      "Epoch 5 Batch 2700 Loss 0.7015 Accuracy 0.2205\n",
      "Epoch 5 Batch 2750 Loss 0.7018 Accuracy 0.2204\n",
      "Epoch 5 Batch 2800 Loss 0.7022 Accuracy 0.2204\n",
      "Saving checkpoint for epoch 5 at ./checkpoints_deu/train_test/ckpt-1\n",
      "Epoch 5 Loss 0.7023 Accuracy 0.2204\n",
      "Time taken for 1 epoch: 1019.1068141460419 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7116 Accuracy 0.2168\n",
      "Epoch 6 Batch 50 Loss 0.6562 Accuracy 0.2216\n",
      "Epoch 6 Batch 100 Loss 0.6589 Accuracy 0.2220\n",
      "Epoch 6 Batch 150 Loss 0.6523 Accuracy 0.2223\n",
      "Epoch 6 Batch 200 Loss 0.6604 Accuracy 0.2223\n",
      "Epoch 6 Batch 250 Loss 0.6624 Accuracy 0.2224\n",
      "Epoch 6 Batch 300 Loss 0.6640 Accuracy 0.2221\n",
      "Epoch 6 Batch 350 Loss 0.6658 Accuracy 0.2223\n",
      "Epoch 6 Batch 400 Loss 0.6654 Accuracy 0.2220\n",
      "Epoch 6 Batch 450 Loss 0.6707 Accuracy 0.2220\n",
      "Epoch 6 Batch 500 Loss 0.6712 Accuracy 0.2219\n",
      "Epoch 6 Batch 550 Loss 0.6705 Accuracy 0.2220\n",
      "Epoch 6 Batch 600 Loss 0.6715 Accuracy 0.2220\n",
      "Epoch 6 Batch 650 Loss 0.6713 Accuracy 0.2219\n",
      "Epoch 6 Batch 700 Loss 0.6735 Accuracy 0.2219\n",
      "Epoch 6 Batch 750 Loss 0.6740 Accuracy 0.2218\n",
      "Epoch 6 Batch 800 Loss 0.6750 Accuracy 0.2218\n",
      "Epoch 6 Batch 850 Loss 0.6759 Accuracy 0.2219\n",
      "Epoch 6 Batch 900 Loss 0.6772 Accuracy 0.2220\n",
      "Epoch 6 Batch 950 Loss 0.6772 Accuracy 0.2219\n",
      "Epoch 6 Batch 1000 Loss 0.6779 Accuracy 0.2219\n",
      "Epoch 6 Batch 1050 Loss 0.6789 Accuracy 0.2218\n",
      "Epoch 6 Batch 1100 Loss 0.6795 Accuracy 0.2218\n",
      "Epoch 6 Batch 1150 Loss 0.6810 Accuracy 0.2217\n",
      "Epoch 6 Batch 1200 Loss 0.6811 Accuracy 0.2217\n",
      "Epoch 6 Batch 1250 Loss 0.6817 Accuracy 0.2216\n",
      "Epoch 6 Batch 1300 Loss 0.6817 Accuracy 0.2215\n",
      "Epoch 6 Batch 1350 Loss 0.6829 Accuracy 0.2215\n",
      "Epoch 6 Batch 1400 Loss 0.6830 Accuracy 0.2215\n",
      "Epoch 6 Batch 1450 Loss 0.6829 Accuracy 0.2213\n",
      "Epoch 6 Batch 1500 Loss 0.6839 Accuracy 0.2213\n",
      "Epoch 6 Batch 1550 Loss 0.6846 Accuracy 0.2212\n",
      "Epoch 6 Batch 1600 Loss 0.6856 Accuracy 0.2212\n",
      "Epoch 6 Batch 1650 Loss 0.6862 Accuracy 0.2212\n",
      "Epoch 6 Batch 1700 Loss 0.6863 Accuracy 0.2211\n",
      "Epoch 6 Batch 1750 Loss 0.6869 Accuracy 0.2211\n",
      "Epoch 6 Batch 1800 Loss 0.6868 Accuracy 0.2211\n",
      "Epoch 6 Batch 1850 Loss 0.6872 Accuracy 0.2211\n",
      "Epoch 6 Batch 1900 Loss 0.6882 Accuracy 0.2210\n",
      "Epoch 6 Batch 1950 Loss 0.6884 Accuracy 0.2209\n",
      "Epoch 6 Batch 2000 Loss 0.6880 Accuracy 0.2209\n",
      "Epoch 6 Batch 2050 Loss 0.6883 Accuracy 0.2209\n",
      "Epoch 6 Batch 2100 Loss 0.6886 Accuracy 0.2209\n",
      "Epoch 6 Batch 2150 Loss 0.6892 Accuracy 0.2209\n",
      "Epoch 6 Batch 2200 Loss 0.6894 Accuracy 0.2208\n",
      "Epoch 6 Batch 2250 Loss 0.6901 Accuracy 0.2208\n",
      "Epoch 6 Batch 2300 Loss 0.6909 Accuracy 0.2208\n",
      "Epoch 6 Batch 2350 Loss 0.6910 Accuracy 0.2208\n",
      "Epoch 6 Batch 2400 Loss 0.6913 Accuracy 0.2208\n",
      "Epoch 6 Batch 2450 Loss 0.6915 Accuracy 0.2208\n",
      "Epoch 6 Batch 2500 Loss 0.6917 Accuracy 0.2208\n",
      "Epoch 6 Batch 2550 Loss 0.6921 Accuracy 0.2208\n",
      "Epoch 6 Batch 2600 Loss 0.6927 Accuracy 0.2208\n",
      "Epoch 6 Batch 2650 Loss 0.6933 Accuracy 0.2207\n",
      "Epoch 6 Batch 2700 Loss 0.6938 Accuracy 0.2207\n",
      "Epoch 6 Batch 2750 Loss 0.6941 Accuracy 0.2207\n",
      "Epoch 6 Batch 2800 Loss 0.6945 Accuracy 0.2207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.6946 Accuracy 0.2207\n",
      "Time taken for 1 epoch: 1033.7624320983887 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.7654 Accuracy 0.2184\n",
      "Epoch 7 Batch 50 Loss 0.6507 Accuracy 0.2216\n",
      "Epoch 7 Batch 100 Loss 0.6614 Accuracy 0.2219\n",
      "Epoch 7 Batch 150 Loss 0.6496 Accuracy 0.2216\n",
      "Epoch 7 Batch 200 Loss 0.6513 Accuracy 0.2214\n",
      "Epoch 7 Batch 250 Loss 0.6530 Accuracy 0.2217\n",
      "Epoch 7 Batch 300 Loss 0.6522 Accuracy 0.2216\n",
      "Epoch 7 Batch 350 Loss 0.6517 Accuracy 0.2217\n",
      "Epoch 7 Batch 400 Loss 0.6538 Accuracy 0.2217\n",
      "Epoch 7 Batch 450 Loss 0.6548 Accuracy 0.2219\n",
      "Epoch 7 Batch 500 Loss 0.6540 Accuracy 0.2220\n",
      "Epoch 7 Batch 550 Loss 0.6552 Accuracy 0.2219\n",
      "Epoch 7 Batch 600 Loss 0.6564 Accuracy 0.2217\n",
      "Epoch 7 Batch 650 Loss 0.6586 Accuracy 0.2216\n",
      "Epoch 7 Batch 700 Loss 0.6598 Accuracy 0.2216\n",
      "Epoch 7 Batch 750 Loss 0.6608 Accuracy 0.2217\n",
      "Epoch 7 Batch 800 Loss 0.6617 Accuracy 0.2217\n",
      "Epoch 7 Batch 850 Loss 0.6639 Accuracy 0.2217\n",
      "Epoch 7 Batch 900 Loss 0.6638 Accuracy 0.2216\n",
      "Epoch 7 Batch 950 Loss 0.6651 Accuracy 0.2216\n",
      "Epoch 7 Batch 1000 Loss 0.6670 Accuracy 0.2216\n",
      "Epoch 7 Batch 1050 Loss 0.6677 Accuracy 0.2215\n",
      "Epoch 7 Batch 1100 Loss 0.6679 Accuracy 0.2215\n",
      "Epoch 7 Batch 1150 Loss 0.6692 Accuracy 0.2214\n",
      "Epoch 7 Batch 1200 Loss 0.6693 Accuracy 0.2214\n",
      "Epoch 7 Batch 1250 Loss 0.6704 Accuracy 0.2213\n",
      "Epoch 7 Batch 1300 Loss 0.6711 Accuracy 0.2213\n",
      "Epoch 7 Batch 1350 Loss 0.6725 Accuracy 0.2212\n",
      "Epoch 7 Batch 1400 Loss 0.6731 Accuracy 0.2212\n",
      "Epoch 7 Batch 1450 Loss 0.6738 Accuracy 0.2211\n",
      "Epoch 7 Batch 1500 Loss 0.6747 Accuracy 0.2212\n",
      "Epoch 7 Batch 1550 Loss 0.6763 Accuracy 0.2211\n",
      "Epoch 7 Batch 1600 Loss 0.6773 Accuracy 0.2211\n",
      "Epoch 7 Batch 1650 Loss 0.6779 Accuracy 0.2211\n",
      "Epoch 7 Batch 1700 Loss 0.6786 Accuracy 0.2211\n",
      "Epoch 7 Batch 1750 Loss 0.6793 Accuracy 0.2211\n",
      "Epoch 7 Batch 1800 Loss 0.6798 Accuracy 0.2211\n",
      "Epoch 7 Batch 1850 Loss 0.6798 Accuracy 0.2211\n",
      "Epoch 7 Batch 1900 Loss 0.6805 Accuracy 0.2211\n",
      "Epoch 7 Batch 1950 Loss 0.6816 Accuracy 0.2210\n",
      "Epoch 7 Batch 2000 Loss 0.6815 Accuracy 0.2210\n",
      "Epoch 7 Batch 2050 Loss 0.6821 Accuracy 0.2210\n",
      "Epoch 7 Batch 2100 Loss 0.6826 Accuracy 0.2210\n",
      "Epoch 7 Batch 2150 Loss 0.6828 Accuracy 0.2210\n",
      "Epoch 7 Batch 2200 Loss 0.6833 Accuracy 0.2210\n",
      "Epoch 7 Batch 2250 Loss 0.6839 Accuracy 0.2210\n",
      "Epoch 7 Batch 2300 Loss 0.6840 Accuracy 0.2210\n",
      "Epoch 7 Batch 2350 Loss 0.6847 Accuracy 0.2210\n",
      "Epoch 7 Batch 2400 Loss 0.6853 Accuracy 0.2209\n",
      "Epoch 7 Batch 2450 Loss 0.6856 Accuracy 0.2209\n",
      "Epoch 7 Batch 2500 Loss 0.6870 Accuracy 0.2208\n",
      "Epoch 7 Batch 2550 Loss 0.6873 Accuracy 0.2208\n",
      "Epoch 7 Batch 2600 Loss 0.6877 Accuracy 0.2208\n",
      "Epoch 7 Batch 2650 Loss 0.6886 Accuracy 0.2208\n",
      "Epoch 7 Batch 2700 Loss 0.6889 Accuracy 0.2208\n",
      "Epoch 7 Batch 2750 Loss 0.6897 Accuracy 0.2208\n",
      "Epoch 7 Batch 2800 Loss 0.6905 Accuracy 0.2208\n",
      "Epoch 7 Loss 0.6905 Accuracy 0.2208\n",
      "Time taken for 1 epoch: 1035.3864908218384 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.8115 Accuracy 0.2275\n",
      "Epoch 8 Batch 50 Loss 0.6598 Accuracy 0.2215\n",
      "Epoch 8 Batch 100 Loss 0.6483 Accuracy 0.2211\n",
      "Epoch 8 Batch 150 Loss 0.6420 Accuracy 0.2223\n",
      "Epoch 8 Batch 200 Loss 0.6433 Accuracy 0.2221\n",
      "Epoch 8 Batch 250 Loss 0.6466 Accuracy 0.2219\n",
      "Epoch 8 Batch 300 Loss 0.6517 Accuracy 0.2221\n",
      "Epoch 8 Batch 350 Loss 0.6514 Accuracy 0.2219\n",
      "Epoch 8 Batch 400 Loss 0.6553 Accuracy 0.2218\n",
      "Epoch 8 Batch 450 Loss 0.6570 Accuracy 0.2218\n",
      "Epoch 8 Batch 500 Loss 0.6590 Accuracy 0.2217\n",
      "Epoch 8 Batch 550 Loss 0.6596 Accuracy 0.2216\n",
      "Epoch 8 Batch 600 Loss 0.6603 Accuracy 0.2215\n",
      "Epoch 8 Batch 650 Loss 0.6600 Accuracy 0.2215\n",
      "Epoch 8 Batch 700 Loss 0.6612 Accuracy 0.2214\n",
      "Epoch 8 Batch 750 Loss 0.6614 Accuracy 0.2215\n",
      "Epoch 8 Batch 800 Loss 0.6627 Accuracy 0.2215\n",
      "Epoch 8 Batch 850 Loss 0.6640 Accuracy 0.2212\n",
      "Epoch 8 Batch 900 Loss 0.6653 Accuracy 0.2213\n",
      "Epoch 8 Batch 950 Loss 0.6656 Accuracy 0.2214\n",
      "Epoch 8 Batch 1000 Loss 0.6666 Accuracy 0.2214\n",
      "Epoch 8 Batch 1050 Loss 0.6674 Accuracy 0.2214\n",
      "Epoch 8 Batch 1100 Loss 0.6676 Accuracy 0.2215\n",
      "Epoch 8 Batch 1150 Loss 0.6681 Accuracy 0.2215\n",
      "Epoch 8 Batch 1200 Loss 0.6674 Accuracy 0.2214\n",
      "Epoch 8 Batch 1250 Loss 0.6683 Accuracy 0.2215\n",
      "Epoch 8 Batch 1300 Loss 0.6684 Accuracy 0.2215\n",
      "Epoch 8 Batch 1350 Loss 0.6698 Accuracy 0.2215\n",
      "Epoch 8 Batch 1400 Loss 0.6702 Accuracy 0.2215\n",
      "Epoch 8 Batch 1450 Loss 0.6706 Accuracy 0.2215\n",
      "Epoch 8 Batch 1500 Loss 0.6719 Accuracy 0.2214\n",
      "Epoch 8 Batch 1550 Loss 0.6722 Accuracy 0.2213\n",
      "Epoch 8 Batch 1600 Loss 0.6731 Accuracy 0.2213\n",
      "Epoch 8 Batch 1650 Loss 0.6734 Accuracy 0.2213\n",
      "Epoch 8 Batch 1700 Loss 0.6738 Accuracy 0.2212\n",
      "Epoch 8 Batch 1750 Loss 0.6747 Accuracy 0.2212\n",
      "Epoch 8 Batch 1800 Loss 0.6753 Accuracy 0.2212\n",
      "Epoch 8 Batch 1850 Loss 0.6757 Accuracy 0.2211\n",
      "Epoch 8 Batch 1900 Loss 0.6772 Accuracy 0.2211\n",
      "Epoch 8 Batch 1950 Loss 0.6776 Accuracy 0.2212\n",
      "Epoch 8 Batch 2000 Loss 0.6780 Accuracy 0.2212\n",
      "Epoch 8 Batch 2050 Loss 0.6788 Accuracy 0.2212\n",
      "Epoch 8 Batch 2100 Loss 0.6792 Accuracy 0.2211\n",
      "Epoch 8 Batch 2150 Loss 0.6801 Accuracy 0.2211\n",
      "Epoch 8 Batch 2200 Loss 0.6807 Accuracy 0.2211\n",
      "Epoch 8 Batch 2250 Loss 0.6818 Accuracy 0.2211\n",
      "Epoch 8 Batch 2300 Loss 0.6828 Accuracy 0.2210\n",
      "Epoch 8 Batch 2350 Loss 0.6832 Accuracy 0.2210\n",
      "Epoch 8 Batch 2400 Loss 0.6843 Accuracy 0.2210\n",
      "Epoch 8 Batch 2450 Loss 0.6846 Accuracy 0.2210\n",
      "Epoch 8 Batch 2500 Loss 0.6847 Accuracy 0.2210\n",
      "Epoch 8 Batch 2550 Loss 0.6851 Accuracy 0.2210\n",
      "Epoch 8 Batch 2600 Loss 0.6856 Accuracy 0.2210\n",
      "Epoch 8 Batch 2650 Loss 0.6861 Accuracy 0.2209\n",
      "Epoch 8 Batch 2700 Loss 0.6867 Accuracy 0.2209\n",
      "Epoch 8 Batch 2750 Loss 0.6870 Accuracy 0.2209\n",
      "Epoch 8 Batch 2800 Loss 0.6873 Accuracy 0.2209\n",
      "Epoch 8 Loss 0.6874 Accuracy 0.2209\n",
      "Time taken for 1 epoch: 1027.0934200286865 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5850 Accuracy 0.2325\n",
      "Epoch 9 Batch 50 Loss 0.6278 Accuracy 0.2243\n",
      "Epoch 9 Batch 100 Loss 0.6334 Accuracy 0.2240\n",
      "Epoch 9 Batch 150 Loss 0.6301 Accuracy 0.2239\n",
      "Epoch 9 Batch 200 Loss 0.6306 Accuracy 0.2238\n",
      "Epoch 9 Batch 250 Loss 0.6362 Accuracy 0.2233\n",
      "Epoch 9 Batch 300 Loss 0.6404 Accuracy 0.2230\n",
      "Epoch 9 Batch 350 Loss 0.6418 Accuracy 0.2232\n",
      "Epoch 9 Batch 400 Loss 0.6398 Accuracy 0.2229\n",
      "Epoch 9 Batch 450 Loss 0.6432 Accuracy 0.2226\n",
      "Epoch 9 Batch 500 Loss 0.6443 Accuracy 0.2225\n",
      "Epoch 9 Batch 550 Loss 0.6467 Accuracy 0.2225\n",
      "Epoch 9 Batch 600 Loss 0.6485 Accuracy 0.2224\n",
      "Epoch 9 Batch 650 Loss 0.6522 Accuracy 0.2224\n",
      "Epoch 9 Batch 700 Loss 0.6522 Accuracy 0.2221\n",
      "Epoch 9 Batch 750 Loss 0.6519 Accuracy 0.2221\n",
      "Epoch 9 Batch 800 Loss 0.6546 Accuracy 0.2219\n",
      "Epoch 9 Batch 850 Loss 0.6569 Accuracy 0.2219\n",
      "Epoch 9 Batch 900 Loss 0.6579 Accuracy 0.2219\n",
      "Epoch 9 Batch 950 Loss 0.6582 Accuracy 0.2219\n",
      "Epoch 9 Batch 1000 Loss 0.6579 Accuracy 0.2218\n",
      "Epoch 9 Batch 1050 Loss 0.6601 Accuracy 0.2218\n",
      "Epoch 9 Batch 1100 Loss 0.6624 Accuracy 0.2218\n",
      "Epoch 9 Batch 1150 Loss 0.6632 Accuracy 0.2218\n",
      "Epoch 9 Batch 1200 Loss 0.6636 Accuracy 0.2218\n",
      "Epoch 9 Batch 1250 Loss 0.6644 Accuracy 0.2217\n",
      "Epoch 9 Batch 1300 Loss 0.6653 Accuracy 0.2217\n",
      "Epoch 9 Batch 1350 Loss 0.6661 Accuracy 0.2216\n",
      "Epoch 9 Batch 1400 Loss 0.6666 Accuracy 0.2216\n",
      "Epoch 9 Batch 1450 Loss 0.6680 Accuracy 0.2216\n",
      "Epoch 9 Batch 1500 Loss 0.6693 Accuracy 0.2215\n",
      "Epoch 9 Batch 1550 Loss 0.6701 Accuracy 0.2215\n",
      "Epoch 9 Batch 1600 Loss 0.6705 Accuracy 0.2215\n",
      "Epoch 9 Batch 1650 Loss 0.6708 Accuracy 0.2215\n",
      "Epoch 9 Batch 1700 Loss 0.6713 Accuracy 0.2214\n",
      "Epoch 9 Batch 1750 Loss 0.6719 Accuracy 0.2215\n",
      "Epoch 9 Batch 1800 Loss 0.6721 Accuracy 0.2215\n",
      "Epoch 9 Batch 1850 Loss 0.6727 Accuracy 0.2215\n",
      "Epoch 9 Batch 1900 Loss 0.6738 Accuracy 0.2215\n",
      "Epoch 9 Batch 1950 Loss 0.6740 Accuracy 0.2215\n",
      "Epoch 9 Batch 2000 Loss 0.6744 Accuracy 0.2215\n",
      "Epoch 9 Batch 2050 Loss 0.6751 Accuracy 0.2214\n",
      "Epoch 9 Batch 2100 Loss 0.6761 Accuracy 0.2214\n",
      "Epoch 9 Batch 2150 Loss 0.6771 Accuracy 0.2214\n",
      "Epoch 9 Batch 2200 Loss 0.6774 Accuracy 0.2213\n",
      "Epoch 9 Batch 2250 Loss 0.6778 Accuracy 0.2213\n",
      "Epoch 9 Batch 2300 Loss 0.6786 Accuracy 0.2213\n",
      "Epoch 9 Batch 2350 Loss 0.6786 Accuracy 0.2213\n",
      "Epoch 9 Batch 2400 Loss 0.6794 Accuracy 0.2212\n",
      "Epoch 9 Batch 2450 Loss 0.6801 Accuracy 0.2212\n",
      "Epoch 9 Batch 2500 Loss 0.6803 Accuracy 0.2212\n",
      "Epoch 9 Batch 2550 Loss 0.6810 Accuracy 0.2212\n",
      "Epoch 9 Batch 2600 Loss 0.6817 Accuracy 0.2211\n",
      "Epoch 9 Batch 2650 Loss 0.6824 Accuracy 0.2211\n",
      "Epoch 9 Batch 2700 Loss 0.6826 Accuracy 0.2211\n",
      "Epoch 9 Batch 2750 Loss 0.6830 Accuracy 0.2211\n",
      "Epoch 9 Batch 2800 Loss 0.6829 Accuracy 0.2211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss 0.6829 Accuracy 0.2211\n",
      "Time taken for 1 epoch: 1028.964898109436 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6308 Accuracy 0.2252\n",
      "Epoch 10 Batch 50 Loss 0.6351 Accuracy 0.2254\n",
      "Epoch 10 Batch 100 Loss 0.6361 Accuracy 0.2244\n",
      "Epoch 10 Batch 150 Loss 0.6279 Accuracy 0.2243\n",
      "Epoch 10 Batch 200 Loss 0.6345 Accuracy 0.2237\n",
      "Epoch 10 Batch 250 Loss 0.6365 Accuracy 0.2235\n",
      "Epoch 10 Batch 300 Loss 0.6422 Accuracy 0.2232\n",
      "Epoch 10 Batch 350 Loss 0.6408 Accuracy 0.2231\n",
      "Epoch 10 Batch 400 Loss 0.6421 Accuracy 0.2231\n",
      "Epoch 10 Batch 450 Loss 0.6414 Accuracy 0.2227\n",
      "Epoch 10 Batch 500 Loss 0.6432 Accuracy 0.2227\n",
      "Epoch 10 Batch 550 Loss 0.6429 Accuracy 0.2226\n",
      "Epoch 10 Batch 600 Loss 0.6455 Accuracy 0.2225\n",
      "Epoch 10 Batch 650 Loss 0.6449 Accuracy 0.2224\n",
      "Epoch 10 Batch 700 Loss 0.6457 Accuracy 0.2224\n",
      "Epoch 10 Batch 750 Loss 0.6474 Accuracy 0.2224\n",
      "Epoch 10 Batch 800 Loss 0.6484 Accuracy 0.2225\n",
      "Epoch 10 Batch 850 Loss 0.6480 Accuracy 0.2224\n",
      "Epoch 10 Batch 900 Loss 0.6486 Accuracy 0.2223\n",
      "Epoch 10 Batch 950 Loss 0.6502 Accuracy 0.2222\n",
      "Epoch 10 Batch 1000 Loss 0.6508 Accuracy 0.2221\n",
      "Epoch 10 Batch 1050 Loss 0.6515 Accuracy 0.2221\n",
      "Epoch 10 Batch 1100 Loss 0.6520 Accuracy 0.2222\n",
      "Epoch 10 Batch 1150 Loss 0.6536 Accuracy 0.2221\n",
      "Epoch 10 Batch 1200 Loss 0.6549 Accuracy 0.2221\n",
      "Epoch 10 Batch 1250 Loss 0.6558 Accuracy 0.2220\n",
      "Epoch 10 Batch 1300 Loss 0.6562 Accuracy 0.2220\n",
      "Epoch 10 Batch 1350 Loss 0.6571 Accuracy 0.2218\n",
      "Epoch 10 Batch 1400 Loss 0.6583 Accuracy 0.2219\n",
      "Epoch 10 Batch 1450 Loss 0.6592 Accuracy 0.2219\n",
      "Epoch 10 Batch 1500 Loss 0.6603 Accuracy 0.2219\n",
      "Epoch 10 Batch 1550 Loss 0.6614 Accuracy 0.2219\n",
      "Epoch 10 Batch 1600 Loss 0.6632 Accuracy 0.2219\n",
      "Epoch 10 Batch 1650 Loss 0.6638 Accuracy 0.2219\n",
      "Epoch 10 Batch 1700 Loss 0.6645 Accuracy 0.2219\n",
      "Epoch 10 Batch 1750 Loss 0.6652 Accuracy 0.2218\n",
      "Epoch 10 Batch 1800 Loss 0.6651 Accuracy 0.2218\n",
      "Epoch 10 Batch 1850 Loss 0.6657 Accuracy 0.2218\n",
      "Epoch 10 Batch 1900 Loss 0.6661 Accuracy 0.2217\n",
      "Epoch 10 Batch 1950 Loss 0.6669 Accuracy 0.2217\n",
      "Epoch 10 Batch 2000 Loss 0.6674 Accuracy 0.2216\n",
      "Epoch 10 Batch 2050 Loss 0.6682 Accuracy 0.2216\n",
      "Epoch 10 Batch 2100 Loss 0.6688 Accuracy 0.2216\n",
      "Epoch 10 Batch 2150 Loss 0.6696 Accuracy 0.2215\n",
      "Epoch 10 Batch 2200 Loss 0.6703 Accuracy 0.2214\n",
      "Epoch 10 Batch 2250 Loss 0.6714 Accuracy 0.2214\n",
      "Epoch 10 Batch 2300 Loss 0.6715 Accuracy 0.2214\n",
      "Epoch 10 Batch 2350 Loss 0.6725 Accuracy 0.2213\n",
      "Epoch 10 Batch 2400 Loss 0.6727 Accuracy 0.2213\n",
      "Epoch 10 Batch 2450 Loss 0.6738 Accuracy 0.2213\n",
      "Epoch 10 Batch 2500 Loss 0.6742 Accuracy 0.2213\n",
      "Epoch 10 Batch 2550 Loss 0.6747 Accuracy 0.2213\n",
      "Epoch 10 Batch 2600 Loss 0.6750 Accuracy 0.2213\n",
      "Epoch 10 Batch 2650 Loss 0.6757 Accuracy 0.2213\n",
      "Epoch 10 Batch 2700 Loss 0.6757 Accuracy 0.2213\n",
      "Epoch 10 Batch 2750 Loss 0.6757 Accuracy 0.2214\n",
      "Epoch 10 Batch 2800 Loss 0.6767 Accuracy 0.2213\n",
      "Saving checkpoint for epoch 10 at ./checkpoints_deu/train_test/ckpt-2\n",
      "Epoch 10 Loss 0.6768 Accuracy 0.2213\n",
      "Time taken for 1 epoch: 1026.8544969558716 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.5335 Accuracy 0.2268\n",
      "Epoch 11 Batch 50 Loss 0.6219 Accuracy 0.2231\n",
      "Epoch 11 Batch 100 Loss 0.6267 Accuracy 0.2231\n",
      "Epoch 11 Batch 150 Loss 0.6265 Accuracy 0.2231\n",
      "Epoch 11 Batch 200 Loss 0.6267 Accuracy 0.2232\n",
      "Epoch 11 Batch 250 Loss 0.6285 Accuracy 0.2229\n",
      "Epoch 11 Batch 300 Loss 0.6284 Accuracy 0.2231\n",
      "Epoch 11 Batch 350 Loss 0.6298 Accuracy 0.2229\n",
      "Epoch 11 Batch 400 Loss 0.6318 Accuracy 0.2228\n",
      "Epoch 11 Batch 450 Loss 0.6342 Accuracy 0.2227\n",
      "Epoch 11 Batch 500 Loss 0.6352 Accuracy 0.2227\n",
      "Epoch 11 Batch 550 Loss 0.6384 Accuracy 0.2226\n",
      "Epoch 11 Batch 600 Loss 0.6391 Accuracy 0.2226\n",
      "Epoch 11 Batch 650 Loss 0.6410 Accuracy 0.2226\n",
      "Epoch 11 Batch 700 Loss 0.6418 Accuracy 0.2226\n",
      "Epoch 11 Batch 750 Loss 0.6423 Accuracy 0.2227\n",
      "Epoch 11 Batch 800 Loss 0.6434 Accuracy 0.2227\n",
      "Epoch 11 Batch 850 Loss 0.6450 Accuracy 0.2226\n",
      "Epoch 11 Batch 900 Loss 0.6459 Accuracy 0.2226\n",
      "Epoch 11 Batch 950 Loss 0.6477 Accuracy 0.2226\n",
      "Epoch 11 Batch 1000 Loss 0.6487 Accuracy 0.2224\n",
      "Epoch 11 Batch 1050 Loss 0.6510 Accuracy 0.2224\n",
      "Epoch 11 Batch 1100 Loss 0.6512 Accuracy 0.2223\n",
      "Epoch 11 Batch 1150 Loss 0.6524 Accuracy 0.2223\n",
      "Epoch 11 Batch 1200 Loss 0.6528 Accuracy 0.2223\n",
      "Epoch 11 Batch 1250 Loss 0.6533 Accuracy 0.2222\n",
      "Epoch 11 Batch 1300 Loss 0.6537 Accuracy 0.2222\n",
      "Epoch 11 Batch 1350 Loss 0.6551 Accuracy 0.2221\n",
      "Epoch 11 Batch 1400 Loss 0.6560 Accuracy 0.2220\n",
      "Epoch 11 Batch 1450 Loss 0.6575 Accuracy 0.2220\n",
      "Epoch 11 Batch 1500 Loss 0.6583 Accuracy 0.2220\n",
      "Epoch 11 Batch 1550 Loss 0.6595 Accuracy 0.2220\n",
      "Epoch 11 Batch 1600 Loss 0.6609 Accuracy 0.2220\n",
      "Epoch 11 Batch 1650 Loss 0.6614 Accuracy 0.2221\n",
      "Epoch 11 Batch 1700 Loss 0.6623 Accuracy 0.2220\n",
      "Epoch 11 Batch 1750 Loss 0.6631 Accuracy 0.2219\n",
      "Epoch 11 Batch 1800 Loss 0.6642 Accuracy 0.2219\n",
      "Epoch 11 Batch 1850 Loss 0.6643 Accuracy 0.2218\n",
      "Epoch 11 Batch 1900 Loss 0.6650 Accuracy 0.2218\n",
      "Epoch 11 Batch 1950 Loss 0.6657 Accuracy 0.2218\n",
      "Epoch 11 Batch 2000 Loss 0.6659 Accuracy 0.2218\n",
      "Epoch 11 Batch 2050 Loss 0.6666 Accuracy 0.2218\n",
      "Epoch 11 Batch 2100 Loss 0.6669 Accuracy 0.2218\n",
      "Epoch 11 Batch 2150 Loss 0.6671 Accuracy 0.2218\n",
      "Epoch 11 Batch 2200 Loss 0.6676 Accuracy 0.2217\n",
      "Epoch 11 Batch 2250 Loss 0.6684 Accuracy 0.2217\n",
      "Epoch 11 Batch 2300 Loss 0.6687 Accuracy 0.2217\n",
      "Epoch 11 Batch 2350 Loss 0.6694 Accuracy 0.2217\n",
      "Epoch 11 Batch 2400 Loss 0.6707 Accuracy 0.2217\n",
      "Epoch 11 Batch 2450 Loss 0.6712 Accuracy 0.2217\n",
      "Epoch 11 Batch 2500 Loss 0.6717 Accuracy 0.2217\n",
      "Epoch 11 Batch 2550 Loss 0.6718 Accuracy 0.2217\n",
      "Epoch 11 Batch 2600 Loss 0.6727 Accuracy 0.2216\n",
      "Epoch 11 Batch 2650 Loss 0.6730 Accuracy 0.2216\n",
      "Epoch 11 Batch 2700 Loss 0.6735 Accuracy 0.2216\n",
      "Epoch 11 Batch 2750 Loss 0.6739 Accuracy 0.2216\n",
      "Epoch 11 Batch 2800 Loss 0.6740 Accuracy 0.2215\n",
      "Epoch 11 Loss 0.6740 Accuracy 0.2215\n",
      "Time taken for 1 epoch: 1039.9330759048462 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.5681 Accuracy 0.2226\n",
      "Epoch 12 Batch 50 Loss 0.6203 Accuracy 0.2212\n",
      "Epoch 12 Batch 100 Loss 0.6198 Accuracy 0.2224\n",
      "Epoch 12 Batch 150 Loss 0.6227 Accuracy 0.2224\n",
      "Epoch 12 Batch 200 Loss 0.6269 Accuracy 0.2224\n",
      "Epoch 12 Batch 250 Loss 0.6367 Accuracy 0.2224\n",
      "Epoch 12 Batch 300 Loss 0.6369 Accuracy 0.2223\n",
      "Epoch 12 Batch 350 Loss 0.6363 Accuracy 0.2224\n",
      "Epoch 12 Batch 400 Loss 0.6364 Accuracy 0.2222\n",
      "Epoch 12 Batch 450 Loss 0.6380 Accuracy 0.2223\n",
      "Epoch 12 Batch 500 Loss 0.6388 Accuracy 0.2221\n",
      "Epoch 12 Batch 550 Loss 0.6393 Accuracy 0.2222\n",
      "Epoch 12 Batch 600 Loss 0.6411 Accuracy 0.2223\n",
      "Epoch 12 Batch 650 Loss 0.6432 Accuracy 0.2224\n",
      "Epoch 12 Batch 700 Loss 0.6442 Accuracy 0.2225\n",
      "Epoch 12 Batch 750 Loss 0.6449 Accuracy 0.2224\n",
      "Epoch 12 Batch 800 Loss 0.6438 Accuracy 0.2224\n",
      "Epoch 12 Batch 850 Loss 0.6442 Accuracy 0.2222\n",
      "Epoch 12 Batch 900 Loss 0.6458 Accuracy 0.2222\n",
      "Epoch 12 Batch 950 Loss 0.6462 Accuracy 0.2222\n",
      "Epoch 12 Batch 1000 Loss 0.6470 Accuracy 0.2221\n",
      "Epoch 12 Batch 1050 Loss 0.6488 Accuracy 0.2220\n",
      "Epoch 12 Batch 1100 Loss 0.6493 Accuracy 0.2219\n",
      "Epoch 12 Batch 1150 Loss 0.6497 Accuracy 0.2219\n",
      "Epoch 12 Batch 1200 Loss 0.6516 Accuracy 0.2219\n",
      "Epoch 12 Batch 1250 Loss 0.6522 Accuracy 0.2219\n",
      "Epoch 12 Batch 1300 Loss 0.6530 Accuracy 0.2220\n",
      "Epoch 12 Batch 1350 Loss 0.6533 Accuracy 0.2220\n",
      "Epoch 12 Batch 1400 Loss 0.6541 Accuracy 0.2220\n",
      "Epoch 12 Batch 1450 Loss 0.6552 Accuracy 0.2220\n",
      "Epoch 12 Batch 1500 Loss 0.6565 Accuracy 0.2221\n",
      "Epoch 12 Batch 1550 Loss 0.6575 Accuracy 0.2221\n",
      "Epoch 12 Batch 1600 Loss 0.6576 Accuracy 0.2220\n",
      "Epoch 12 Batch 1650 Loss 0.6584 Accuracy 0.2221\n",
      "Epoch 12 Batch 1700 Loss 0.6591 Accuracy 0.2220\n",
      "Epoch 12 Batch 1750 Loss 0.6596 Accuracy 0.2220\n",
      "Epoch 12 Batch 1800 Loss 0.6605 Accuracy 0.2220\n",
      "Epoch 12 Batch 1850 Loss 0.6610 Accuracy 0.2219\n",
      "Epoch 12 Batch 1900 Loss 0.6619 Accuracy 0.2219\n",
      "Epoch 12 Batch 1950 Loss 0.6628 Accuracy 0.2219\n",
      "Epoch 12 Batch 2000 Loss 0.6634 Accuracy 0.2218\n",
      "Epoch 12 Batch 2050 Loss 0.6637 Accuracy 0.2218\n",
      "Epoch 12 Batch 2100 Loss 0.6653 Accuracy 0.2218\n",
      "Epoch 12 Batch 2150 Loss 0.6658 Accuracy 0.2218\n",
      "Epoch 12 Batch 2200 Loss 0.6662 Accuracy 0.2218\n",
      "Epoch 12 Batch 2250 Loss 0.6669 Accuracy 0.2218\n",
      "Epoch 12 Batch 2300 Loss 0.6674 Accuracy 0.2217\n",
      "Epoch 12 Batch 2350 Loss 0.6679 Accuracy 0.2217\n",
      "Epoch 12 Batch 2400 Loss 0.6686 Accuracy 0.2217\n",
      "Epoch 12 Batch 2450 Loss 0.6696 Accuracy 0.2216\n",
      "Epoch 12 Batch 2500 Loss 0.6699 Accuracy 0.2216\n",
      "Epoch 12 Batch 2550 Loss 0.6699 Accuracy 0.2216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 2600 Loss 0.6703 Accuracy 0.2215\n",
      "Epoch 12 Batch 2650 Loss 0.6707 Accuracy 0.2216\n",
      "Epoch 12 Batch 2700 Loss 0.6708 Accuracy 0.2215\n",
      "Epoch 12 Batch 2750 Loss 0.6716 Accuracy 0.2215\n",
      "Epoch 12 Batch 2800 Loss 0.6724 Accuracy 0.2215\n",
      "Epoch 12 Loss 0.6725 Accuracy 0.2215\n",
      "Time taken for 1 epoch: 1030.0560247898102 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.5989 Accuracy 0.2168\n",
      "Epoch 13 Batch 50 Loss 0.5912 Accuracy 0.2250\n",
      "Epoch 13 Batch 100 Loss 0.6142 Accuracy 0.2247\n",
      "Epoch 13 Batch 150 Loss 0.6194 Accuracy 0.2244\n",
      "Epoch 13 Batch 200 Loss 0.6235 Accuracy 0.2244\n",
      "Epoch 13 Batch 250 Loss 0.6243 Accuracy 0.2241\n",
      "Epoch 13 Batch 300 Loss 0.6251 Accuracy 0.2241\n",
      "Epoch 13 Batch 350 Loss 0.6263 Accuracy 0.2241\n",
      "Epoch 13 Batch 400 Loss 0.6273 Accuracy 0.2240\n",
      "Epoch 13 Batch 450 Loss 0.6271 Accuracy 0.2241\n",
      "Epoch 13 Batch 500 Loss 0.6273 Accuracy 0.2240\n",
      "Epoch 13 Batch 550 Loss 0.6304 Accuracy 0.2239\n",
      "Epoch 13 Batch 600 Loss 0.6328 Accuracy 0.2236\n",
      "Epoch 13 Batch 650 Loss 0.6348 Accuracy 0.2235\n",
      "Epoch 13 Batch 700 Loss 0.6375 Accuracy 0.2234\n",
      "Epoch 13 Batch 750 Loss 0.6389 Accuracy 0.2231\n",
      "Epoch 13 Batch 800 Loss 0.6400 Accuracy 0.2230\n",
      "Epoch 13 Batch 850 Loss 0.6402 Accuracy 0.2230\n",
      "Epoch 13 Batch 900 Loss 0.6406 Accuracy 0.2229\n",
      "Epoch 13 Batch 950 Loss 0.6425 Accuracy 0.2227\n",
      "Epoch 13 Batch 1000 Loss 0.6437 Accuracy 0.2227\n",
      "Epoch 13 Batch 1050 Loss 0.6448 Accuracy 0.2226\n",
      "Epoch 13 Batch 1100 Loss 0.6459 Accuracy 0.2225\n",
      "Epoch 13 Batch 1150 Loss 0.6475 Accuracy 0.2225\n",
      "Epoch 13 Batch 1200 Loss 0.6493 Accuracy 0.2225\n",
      "Epoch 13 Batch 1250 Loss 0.6498 Accuracy 0.2225\n",
      "Epoch 13 Batch 1300 Loss 0.6503 Accuracy 0.2224\n",
      "Epoch 13 Batch 1350 Loss 0.6517 Accuracy 0.2223\n",
      "Epoch 13 Batch 1400 Loss 0.6523 Accuracy 0.2223\n",
      "Epoch 13 Batch 1450 Loss 0.6527 Accuracy 0.2222\n",
      "Epoch 13 Batch 1500 Loss 0.6531 Accuracy 0.2222\n",
      "Epoch 13 Batch 1550 Loss 0.6541 Accuracy 0.2222\n",
      "Epoch 13 Batch 1600 Loss 0.6550 Accuracy 0.2222\n",
      "Epoch 13 Batch 1650 Loss 0.6572 Accuracy 0.2222\n",
      "Epoch 13 Batch 1700 Loss 0.6576 Accuracy 0.2221\n",
      "Epoch 13 Batch 1750 Loss 0.6581 Accuracy 0.2220\n",
      "Epoch 13 Batch 1800 Loss 0.6590 Accuracy 0.2219\n",
      "Epoch 13 Batch 1850 Loss 0.6606 Accuracy 0.2219\n",
      "Epoch 13 Batch 1900 Loss 0.6613 Accuracy 0.2219\n",
      "Epoch 13 Batch 1950 Loss 0.6617 Accuracy 0.2219\n",
      "Epoch 13 Batch 2000 Loss 0.6626 Accuracy 0.2219\n",
      "Epoch 13 Batch 2050 Loss 0.6634 Accuracy 0.2219\n",
      "Epoch 13 Batch 2100 Loss 0.6633 Accuracy 0.2219\n",
      "Epoch 13 Batch 2150 Loss 0.6636 Accuracy 0.2218\n",
      "Epoch 13 Batch 2200 Loss 0.6636 Accuracy 0.2218\n",
      "Epoch 13 Batch 2250 Loss 0.6640 Accuracy 0.2218\n",
      "Epoch 13 Batch 2300 Loss 0.6648 Accuracy 0.2218\n",
      "Epoch 13 Batch 2350 Loss 0.6649 Accuracy 0.2217\n",
      "Epoch 13 Batch 2400 Loss 0.6653 Accuracy 0.2217\n",
      "Epoch 13 Batch 2450 Loss 0.6654 Accuracy 0.2217\n",
      "Epoch 13 Batch 2500 Loss 0.6657 Accuracy 0.2217\n",
      "Epoch 13 Batch 2550 Loss 0.6657 Accuracy 0.2217\n",
      "Epoch 13 Batch 2600 Loss 0.6665 Accuracy 0.2216\n",
      "Epoch 13 Batch 2650 Loss 0.6672 Accuracy 0.2217\n",
      "Epoch 13 Batch 2700 Loss 0.6680 Accuracy 0.2217\n",
      "Epoch 13 Batch 2750 Loss 0.6688 Accuracy 0.2217\n",
      "Epoch 13 Batch 2800 Loss 0.6692 Accuracy 0.2217\n",
      "Epoch 13 Loss 0.6693 Accuracy 0.2216\n",
      "Time taken for 1 epoch: 1049.2562952041626 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.6265 Accuracy 0.2035\n",
      "Epoch 14 Batch 50 Loss 0.6306 Accuracy 0.2248\n",
      "Epoch 14 Batch 100 Loss 0.6160 Accuracy 0.2245\n",
      "Epoch 14 Batch 150 Loss 0.6244 Accuracy 0.2232\n",
      "Epoch 14 Batch 200 Loss 0.6300 Accuracy 0.2236\n",
      "Epoch 14 Batch 250 Loss 0.6279 Accuracy 0.2232\n",
      "Epoch 14 Batch 300 Loss 0.6257 Accuracy 0.2229\n",
      "Epoch 14 Batch 350 Loss 0.6280 Accuracy 0.2228\n",
      "Epoch 14 Batch 400 Loss 0.6305 Accuracy 0.2228\n",
      "Epoch 14 Batch 450 Loss 0.6339 Accuracy 0.2228\n",
      "Epoch 14 Batch 500 Loss 0.6362 Accuracy 0.2228\n",
      "Epoch 14 Batch 550 Loss 0.6371 Accuracy 0.2227\n",
      "Epoch 14 Batch 600 Loss 0.6382 Accuracy 0.2227\n",
      "Epoch 14 Batch 650 Loss 0.6400 Accuracy 0.2228\n",
      "Epoch 14 Batch 700 Loss 0.6387 Accuracy 0.2228\n",
      "Epoch 14 Batch 750 Loss 0.6408 Accuracy 0.2228\n",
      "Epoch 14 Batch 800 Loss 0.6419 Accuracy 0.2227\n",
      "Epoch 14 Batch 850 Loss 0.6432 Accuracy 0.2226\n",
      "Epoch 14 Batch 900 Loss 0.6435 Accuracy 0.2225\n",
      "Epoch 14 Batch 950 Loss 0.6442 Accuracy 0.2225\n",
      "Epoch 14 Batch 1000 Loss 0.6450 Accuracy 0.2226\n",
      "Epoch 14 Batch 1050 Loss 0.6462 Accuracy 0.2224\n",
      "Epoch 14 Batch 1100 Loss 0.6471 Accuracy 0.2225\n",
      "Epoch 14 Batch 1150 Loss 0.6474 Accuracy 0.2224\n",
      "Epoch 14 Batch 1200 Loss 0.6478 Accuracy 0.2224\n",
      "Epoch 14 Batch 1250 Loss 0.6490 Accuracy 0.2224\n",
      "Epoch 14 Batch 1300 Loss 0.6501 Accuracy 0.2223\n",
      "Epoch 14 Batch 1350 Loss 0.6513 Accuracy 0.2222\n",
      "Epoch 14 Batch 1400 Loss 0.6520 Accuracy 0.2222\n",
      "Epoch 14 Batch 1450 Loss 0.6530 Accuracy 0.2221\n",
      "Epoch 14 Batch 1500 Loss 0.6528 Accuracy 0.2221\n",
      "Epoch 14 Batch 1550 Loss 0.6525 Accuracy 0.2221\n",
      "Epoch 14 Batch 1600 Loss 0.6530 Accuracy 0.2221\n",
      "Epoch 14 Batch 1650 Loss 0.6539 Accuracy 0.2220\n",
      "Epoch 14 Batch 1700 Loss 0.6552 Accuracy 0.2220\n",
      "Epoch 14 Batch 1750 Loss 0.6552 Accuracy 0.2220\n",
      "Epoch 14 Batch 1800 Loss 0.6559 Accuracy 0.2220\n",
      "Epoch 14 Batch 1850 Loss 0.6564 Accuracy 0.2220\n",
      "Epoch 14 Batch 1900 Loss 0.6575 Accuracy 0.2220\n",
      "Epoch 14 Batch 1950 Loss 0.6581 Accuracy 0.2219\n",
      "Epoch 14 Batch 2000 Loss 0.6582 Accuracy 0.2220\n",
      "Epoch 14 Batch 2050 Loss 0.6584 Accuracy 0.2219\n",
      "Epoch 14 Batch 2100 Loss 0.6594 Accuracy 0.2220\n",
      "Epoch 14 Batch 2150 Loss 0.6598 Accuracy 0.2220\n",
      "Epoch 14 Batch 2200 Loss 0.6600 Accuracy 0.2220\n",
      "Epoch 14 Batch 2250 Loss 0.6605 Accuracy 0.2219\n",
      "Epoch 14 Batch 2300 Loss 0.6607 Accuracy 0.2219\n",
      "Epoch 14 Batch 2350 Loss 0.6619 Accuracy 0.2219\n",
      "Epoch 14 Batch 2400 Loss 0.6621 Accuracy 0.2219\n",
      "Epoch 14 Batch 2450 Loss 0.6625 Accuracy 0.2219\n",
      "Epoch 14 Batch 2500 Loss 0.6627 Accuracy 0.2219\n",
      "Epoch 14 Batch 2550 Loss 0.6634 Accuracy 0.2219\n",
      "Epoch 14 Batch 2600 Loss 0.6639 Accuracy 0.2218\n",
      "Epoch 14 Batch 2650 Loss 0.6642 Accuracy 0.2218\n",
      "Epoch 14 Batch 2700 Loss 0.6642 Accuracy 0.2218\n",
      "Epoch 14 Batch 2750 Loss 0.6646 Accuracy 0.2218\n",
      "Epoch 14 Batch 2800 Loss 0.6653 Accuracy 0.2218\n",
      "Epoch 14 Loss 0.6653 Accuracy 0.2218\n",
      "Time taken for 1 epoch: 1052.7042217254639 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.5867 Accuracy 0.2279\n",
      "Epoch 15 Batch 50 Loss 0.6099 Accuracy 0.2252\n",
      "Epoch 15 Batch 100 Loss 0.6175 Accuracy 0.2246\n",
      "Epoch 15 Batch 150 Loss 0.6250 Accuracy 0.2242\n",
      "Epoch 15 Batch 200 Loss 0.6252 Accuracy 0.2243\n",
      "Epoch 15 Batch 250 Loss 0.6283 Accuracy 0.2242\n",
      "Epoch 15 Batch 300 Loss 0.6310 Accuracy 0.2239\n",
      "Epoch 15 Batch 350 Loss 0.6289 Accuracy 0.2236\n",
      "Epoch 15 Batch 400 Loss 0.6282 Accuracy 0.2235\n",
      "Epoch 15 Batch 450 Loss 0.6287 Accuracy 0.2232\n",
      "Epoch 15 Batch 500 Loss 0.6298 Accuracy 0.2229\n",
      "Epoch 15 Batch 550 Loss 0.6308 Accuracy 0.2227\n",
      "Epoch 15 Batch 600 Loss 0.6310 Accuracy 0.2225\n",
      "Epoch 15 Batch 650 Loss 0.6327 Accuracy 0.2224\n",
      "Epoch 15 Batch 700 Loss 0.6328 Accuracy 0.2224\n",
      "Epoch 15 Batch 750 Loss 0.6358 Accuracy 0.2224\n",
      "Epoch 15 Batch 800 Loss 0.6371 Accuracy 0.2223\n",
      "Epoch 15 Batch 850 Loss 0.6384 Accuracy 0.2224\n",
      "Epoch 15 Batch 900 Loss 0.6409 Accuracy 0.2224\n",
      "Epoch 15 Batch 950 Loss 0.6423 Accuracy 0.2223\n",
      "Epoch 15 Batch 1000 Loss 0.6433 Accuracy 0.2223\n",
      "Epoch 15 Batch 1050 Loss 0.6444 Accuracy 0.2223\n",
      "Epoch 15 Batch 1100 Loss 0.6458 Accuracy 0.2221\n",
      "Epoch 15 Batch 1150 Loss 0.6463 Accuracy 0.2222\n",
      "Epoch 15 Batch 1200 Loss 0.6467 Accuracy 0.2222\n",
      "Epoch 15 Batch 1250 Loss 0.6470 Accuracy 0.2222\n",
      "Epoch 15 Batch 1300 Loss 0.6473 Accuracy 0.2221\n",
      "Epoch 15 Batch 1350 Loss 0.6484 Accuracy 0.2222\n",
      "Epoch 15 Batch 1400 Loss 0.6496 Accuracy 0.2222\n",
      "Epoch 15 Batch 1450 Loss 0.6500 Accuracy 0.2222\n",
      "Epoch 15 Batch 1500 Loss 0.6505 Accuracy 0.2222\n",
      "Epoch 15 Batch 1550 Loss 0.6504 Accuracy 0.2223\n",
      "Epoch 15 Batch 1600 Loss 0.6499 Accuracy 0.2223\n",
      "Epoch 15 Batch 1650 Loss 0.6504 Accuracy 0.2222\n",
      "Epoch 15 Batch 1700 Loss 0.6511 Accuracy 0.2222\n",
      "Epoch 15 Batch 1750 Loss 0.6525 Accuracy 0.2221\n",
      "Epoch 15 Batch 1800 Loss 0.6527 Accuracy 0.2221\n",
      "Epoch 15 Batch 1850 Loss 0.6529 Accuracy 0.2221\n",
      "Epoch 15 Batch 1900 Loss 0.6532 Accuracy 0.2221\n",
      "Epoch 15 Batch 1950 Loss 0.6532 Accuracy 0.2221\n",
      "Epoch 15 Batch 2000 Loss 0.6537 Accuracy 0.2221\n",
      "Epoch 15 Batch 2050 Loss 0.6543 Accuracy 0.2221\n",
      "Epoch 15 Batch 2100 Loss 0.6549 Accuracy 0.2221\n",
      "Epoch 15 Batch 2150 Loss 0.6551 Accuracy 0.2221\n",
      "Epoch 15 Batch 2200 Loss 0.6558 Accuracy 0.2221\n",
      "Epoch 15 Batch 2250 Loss 0.6562 Accuracy 0.2221\n",
      "Epoch 15 Batch 2300 Loss 0.6566 Accuracy 0.2220\n",
      "Epoch 15 Batch 2350 Loss 0.6577 Accuracy 0.2220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 2400 Loss 0.6581 Accuracy 0.2220\n",
      "Epoch 15 Batch 2450 Loss 0.6584 Accuracy 0.2220\n",
      "Epoch 15 Batch 2500 Loss 0.6591 Accuracy 0.2220\n",
      "Epoch 15 Batch 2550 Loss 0.6599 Accuracy 0.2220\n",
      "Epoch 15 Batch 2600 Loss 0.6606 Accuracy 0.2220\n",
      "Epoch 15 Batch 2650 Loss 0.6609 Accuracy 0.2220\n",
      "Epoch 15 Batch 2700 Loss 0.6613 Accuracy 0.2219\n",
      "Epoch 15 Batch 2750 Loss 0.6619 Accuracy 0.2219\n",
      "Epoch 15 Batch 2800 Loss 0.6623 Accuracy 0.2219\n",
      "Saving checkpoint for epoch 15 at ./checkpoints_deu/train_test/ckpt-3\n",
      "Epoch 15 Loss 0.6623 Accuracy 0.2219\n",
      "Time taken for 1 epoch: 1107.8165571689606 secs\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.4358 Accuracy 0.2149\n",
      "Epoch 16 Batch 50 Loss 0.6044 Accuracy 0.2250\n",
      "Epoch 16 Batch 100 Loss 0.6122 Accuracy 0.2244\n",
      "Epoch 16 Batch 150 Loss 0.6114 Accuracy 0.2242\n",
      "Epoch 16 Batch 200 Loss 0.6154 Accuracy 0.2253\n",
      "Epoch 16 Batch 250 Loss 0.6106 Accuracy 0.2252\n",
      "Epoch 16 Batch 300 Loss 0.6152 Accuracy 0.2247\n",
      "Epoch 16 Batch 350 Loss 0.6152 Accuracy 0.2244\n",
      "Epoch 16 Batch 400 Loss 0.6153 Accuracy 0.2245\n",
      "Epoch 16 Batch 450 Loss 0.6191 Accuracy 0.2241\n",
      "Epoch 16 Batch 500 Loss 0.6231 Accuracy 0.2241\n",
      "Epoch 16 Batch 550 Loss 0.6243 Accuracy 0.2238\n",
      "Epoch 16 Batch 600 Loss 0.6268 Accuracy 0.2237\n",
      "Epoch 16 Batch 650 Loss 0.6291 Accuracy 0.2235\n",
      "Epoch 16 Batch 700 Loss 0.6303 Accuracy 0.2235\n",
      "Epoch 16 Batch 750 Loss 0.6321 Accuracy 0.2233\n",
      "Epoch 16 Batch 800 Loss 0.6321 Accuracy 0.2232\n",
      "Epoch 16 Batch 850 Loss 0.6324 Accuracy 0.2232\n",
      "Epoch 16 Batch 900 Loss 0.6335 Accuracy 0.2231\n",
      "Epoch 16 Batch 950 Loss 0.6343 Accuracy 0.2231\n",
      "Epoch 16 Batch 1000 Loss 0.6353 Accuracy 0.2230\n",
      "Epoch 16 Batch 1050 Loss 0.6376 Accuracy 0.2229\n",
      "Epoch 16 Batch 1100 Loss 0.6395 Accuracy 0.2229\n",
      "Epoch 16 Batch 1150 Loss 0.6394 Accuracy 0.2229\n",
      "Epoch 16 Batch 1200 Loss 0.6403 Accuracy 0.2230\n",
      "Epoch 16 Batch 1250 Loss 0.6411 Accuracy 0.2229\n",
      "Epoch 16 Batch 1300 Loss 0.6419 Accuracy 0.2229\n",
      "Epoch 16 Batch 1350 Loss 0.6425 Accuracy 0.2229\n",
      "Epoch 16 Batch 1400 Loss 0.6429 Accuracy 0.2228\n",
      "Epoch 16 Batch 1450 Loss 0.6431 Accuracy 0.2229\n",
      "Epoch 16 Batch 1500 Loss 0.6437 Accuracy 0.2229\n",
      "Epoch 16 Batch 1550 Loss 0.6443 Accuracy 0.2229\n",
      "Epoch 16 Batch 1600 Loss 0.6449 Accuracy 0.2228\n",
      "Epoch 16 Batch 1650 Loss 0.6466 Accuracy 0.2228\n",
      "Epoch 16 Batch 1700 Loss 0.6481 Accuracy 0.2227\n",
      "Epoch 16 Batch 1750 Loss 0.6484 Accuracy 0.2227\n",
      "Epoch 16 Batch 1800 Loss 0.6493 Accuracy 0.2226\n",
      "Epoch 16 Batch 1850 Loss 0.6500 Accuracy 0.2225\n",
      "Epoch 16 Batch 1900 Loss 0.6502 Accuracy 0.2225\n",
      "Epoch 16 Batch 1950 Loss 0.6508 Accuracy 0.2224\n",
      "Epoch 16 Batch 2000 Loss 0.6513 Accuracy 0.2224\n",
      "Epoch 16 Batch 2050 Loss 0.6518 Accuracy 0.2224\n",
      "Epoch 16 Batch 2100 Loss 0.6524 Accuracy 0.2224\n",
      "Epoch 16 Batch 2150 Loss 0.6526 Accuracy 0.2224\n",
      "Epoch 16 Batch 2200 Loss 0.6536 Accuracy 0.2223\n",
      "Epoch 16 Batch 2250 Loss 0.6541 Accuracy 0.2223\n",
      "Epoch 16 Batch 2300 Loss 0.6548 Accuracy 0.2223\n",
      "Epoch 16 Batch 2350 Loss 0.6553 Accuracy 0.2223\n",
      "Epoch 16 Batch 2400 Loss 0.6556 Accuracy 0.2223\n",
      "Epoch 16 Batch 2450 Loss 0.6559 Accuracy 0.2223\n",
      "Epoch 16 Batch 2500 Loss 0.6566 Accuracy 0.2222\n",
      "Epoch 16 Batch 2550 Loss 0.6565 Accuracy 0.2222\n",
      "Epoch 16 Batch 2600 Loss 0.6569 Accuracy 0.2222\n",
      "Epoch 16 Batch 2650 Loss 0.6574 Accuracy 0.2222\n",
      "Epoch 16 Batch 2700 Loss 0.6584 Accuracy 0.2221\n",
      "Epoch 16 Batch 2750 Loss 0.6592 Accuracy 0.2221\n",
      "Epoch 16 Batch 2800 Loss 0.6596 Accuracy 0.2220\n",
      "Epoch 16 Loss 0.6597 Accuracy 0.2220\n",
      "Time taken for 1 epoch: 1055.1872577667236 secs\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.6283 Accuracy 0.2172\n",
      "Epoch 17 Batch 50 Loss 0.6135 Accuracy 0.2269\n",
      "Epoch 17 Batch 100 Loss 0.6139 Accuracy 0.2251\n",
      "Epoch 17 Batch 150 Loss 0.6112 Accuracy 0.2247\n",
      "Epoch 17 Batch 200 Loss 0.6124 Accuracy 0.2250\n",
      "Epoch 17 Batch 250 Loss 0.6145 Accuracy 0.2248\n",
      "Epoch 17 Batch 300 Loss 0.6181 Accuracy 0.2247\n",
      "Epoch 17 Batch 350 Loss 0.6180 Accuracy 0.2245\n",
      "Epoch 17 Batch 400 Loss 0.6199 Accuracy 0.2243\n",
      "Epoch 17 Batch 450 Loss 0.6216 Accuracy 0.2242\n",
      "Epoch 17 Batch 500 Loss 0.6263 Accuracy 0.2241\n",
      "Epoch 17 Batch 550 Loss 0.6273 Accuracy 0.2241\n",
      "Epoch 17 Batch 600 Loss 0.6292 Accuracy 0.2240\n",
      "Epoch 17 Batch 650 Loss 0.6305 Accuracy 0.2238\n",
      "Epoch 17 Batch 700 Loss 0.6301 Accuracy 0.2239\n",
      "Epoch 17 Batch 750 Loss 0.6304 Accuracy 0.2239\n",
      "Epoch 17 Batch 800 Loss 0.6308 Accuracy 0.2238\n",
      "Epoch 17 Batch 850 Loss 0.6318 Accuracy 0.2238\n",
      "Epoch 17 Batch 900 Loss 0.6330 Accuracy 0.2237\n",
      "Epoch 17 Batch 950 Loss 0.6353 Accuracy 0.2236\n",
      "Epoch 17 Batch 1000 Loss 0.6359 Accuracy 0.2235\n",
      "Epoch 17 Batch 1050 Loss 0.6367 Accuracy 0.2234\n",
      "Epoch 17 Batch 1100 Loss 0.6363 Accuracy 0.2234\n",
      "Epoch 17 Batch 1150 Loss 0.6381 Accuracy 0.2233\n",
      "Epoch 17 Batch 1200 Loss 0.6386 Accuracy 0.2232\n",
      "Epoch 17 Batch 1250 Loss 0.6397 Accuracy 0.2232\n",
      "Epoch 17 Batch 1300 Loss 0.6414 Accuracy 0.2230\n",
      "Epoch 17 Batch 1350 Loss 0.6422 Accuracy 0.2230\n",
      "Epoch 17 Batch 1400 Loss 0.6422 Accuracy 0.2230\n",
      "Epoch 17 Batch 1450 Loss 0.6423 Accuracy 0.2230\n",
      "Epoch 17 Batch 1500 Loss 0.6430 Accuracy 0.2229\n",
      "Epoch 17 Batch 1550 Loss 0.6441 Accuracy 0.2229\n",
      "Epoch 17 Batch 1600 Loss 0.6438 Accuracy 0.2230\n",
      "Epoch 17 Batch 1650 Loss 0.6443 Accuracy 0.2229\n",
      "Epoch 17 Batch 1700 Loss 0.6450 Accuracy 0.2228\n",
      "Epoch 17 Batch 1750 Loss 0.6460 Accuracy 0.2227\n",
      "Epoch 17 Batch 1800 Loss 0.6462 Accuracy 0.2227\n",
      "Epoch 17 Batch 1850 Loss 0.6467 Accuracy 0.2227\n",
      "Epoch 17 Batch 1900 Loss 0.6475 Accuracy 0.2227\n",
      "Epoch 17 Batch 1950 Loss 0.6490 Accuracy 0.2226\n",
      "Epoch 17 Batch 2000 Loss 0.6490 Accuracy 0.2226\n",
      "Epoch 17 Batch 2050 Loss 0.6491 Accuracy 0.2226\n",
      "Epoch 17 Batch 2100 Loss 0.6500 Accuracy 0.2226\n",
      "Epoch 17 Batch 2150 Loss 0.6508 Accuracy 0.2225\n",
      "Epoch 17 Batch 2200 Loss 0.6510 Accuracy 0.2224\n",
      "Epoch 17 Batch 2250 Loss 0.6524 Accuracy 0.2224\n",
      "Epoch 17 Batch 2300 Loss 0.6532 Accuracy 0.2223\n",
      "Epoch 17 Batch 2350 Loss 0.6536 Accuracy 0.2223\n",
      "Epoch 17 Batch 2400 Loss 0.6539 Accuracy 0.2223\n",
      "Epoch 17 Batch 2450 Loss 0.6548 Accuracy 0.2223\n",
      "Epoch 17 Batch 2500 Loss 0.6551 Accuracy 0.2223\n",
      "Epoch 17 Batch 2550 Loss 0.6558 Accuracy 0.2223\n",
      "Epoch 17 Batch 2600 Loss 0.6561 Accuracy 0.2222\n",
      "Epoch 17 Batch 2650 Loss 0.6566 Accuracy 0.2222\n",
      "Epoch 17 Batch 2700 Loss 0.6572 Accuracy 0.2222\n",
      "Epoch 17 Batch 2750 Loss 0.6577 Accuracy 0.2222\n",
      "Epoch 17 Batch 2800 Loss 0.6578 Accuracy 0.2221\n",
      "Epoch 17 Loss 0.6580 Accuracy 0.2221\n",
      "Time taken for 1 epoch: 1132.0976767539978 secs\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.5144 Accuracy 0.2500\n",
      "Epoch 18 Batch 50 Loss 0.6023 Accuracy 0.2235\n",
      "Epoch 18 Batch 100 Loss 0.5968 Accuracy 0.2242\n",
      "Epoch 18 Batch 150 Loss 0.6052 Accuracy 0.2237\n",
      "Epoch 18 Batch 200 Loss 0.6078 Accuracy 0.2241\n",
      "Epoch 18 Batch 250 Loss 0.6120 Accuracy 0.2239\n",
      "Epoch 18 Batch 300 Loss 0.6140 Accuracy 0.2237\n",
      "Epoch 18 Batch 350 Loss 0.6150 Accuracy 0.2237\n",
      "Epoch 18 Batch 400 Loss 0.6173 Accuracy 0.2234\n",
      "Epoch 18 Batch 450 Loss 0.6215 Accuracy 0.2235\n",
      "Epoch 18 Batch 500 Loss 0.6258 Accuracy 0.2232\n",
      "Epoch 18 Batch 550 Loss 0.6281 Accuracy 0.2232\n",
      "Epoch 18 Batch 600 Loss 0.6301 Accuracy 0.2231\n",
      "Epoch 18 Batch 650 Loss 0.6299 Accuracy 0.2232\n",
      "Epoch 18 Batch 700 Loss 0.6298 Accuracy 0.2233\n",
      "Epoch 18 Batch 750 Loss 0.6302 Accuracy 0.2232\n",
      "Epoch 18 Batch 800 Loss 0.6328 Accuracy 0.2232\n",
      "Epoch 18 Batch 850 Loss 0.6334 Accuracy 0.2230\n",
      "Epoch 18 Batch 900 Loss 0.6341 Accuracy 0.2229\n",
      "Epoch 18 Batch 950 Loss 0.6343 Accuracy 0.2228\n",
      "Epoch 18 Batch 1000 Loss 0.6341 Accuracy 0.2229\n",
      "Epoch 18 Batch 1050 Loss 0.6350 Accuracy 0.2227\n",
      "Epoch 18 Batch 1100 Loss 0.6364 Accuracy 0.2228\n",
      "Epoch 18 Batch 1150 Loss 0.6377 Accuracy 0.2229\n",
      "Epoch 18 Batch 1200 Loss 0.6385 Accuracy 0.2228\n",
      "Epoch 18 Batch 1250 Loss 0.6390 Accuracy 0.2228\n",
      "Epoch 18 Batch 1300 Loss 0.6400 Accuracy 0.2229\n",
      "Epoch 18 Batch 1350 Loss 0.6403 Accuracy 0.2229\n",
      "Epoch 18 Batch 1400 Loss 0.6411 Accuracy 0.2229\n",
      "Epoch 18 Batch 1450 Loss 0.6418 Accuracy 0.2229\n",
      "Epoch 18 Batch 1500 Loss 0.6433 Accuracy 0.2229\n",
      "Epoch 18 Batch 1550 Loss 0.6442 Accuracy 0.2228\n",
      "Epoch 18 Batch 1600 Loss 0.6447 Accuracy 0.2228\n",
      "Epoch 18 Batch 1650 Loss 0.6450 Accuracy 0.2228\n",
      "Epoch 18 Batch 1700 Loss 0.6456 Accuracy 0.2227\n",
      "Epoch 18 Batch 1750 Loss 0.6460 Accuracy 0.2228\n",
      "Epoch 18 Batch 1800 Loss 0.6462 Accuracy 0.2227\n",
      "Epoch 18 Batch 1850 Loss 0.6460 Accuracy 0.2227\n",
      "Epoch 18 Batch 1900 Loss 0.6461 Accuracy 0.2226\n",
      "Epoch 18 Batch 1950 Loss 0.6465 Accuracy 0.2227\n",
      "Epoch 18 Batch 2000 Loss 0.6472 Accuracy 0.2226\n",
      "Epoch 18 Batch 2050 Loss 0.6481 Accuracy 0.2226\n",
      "Epoch 18 Batch 2100 Loss 0.6485 Accuracy 0.2226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 2150 Loss 0.6486 Accuracy 0.2226\n",
      "Epoch 18 Batch 2200 Loss 0.6490 Accuracy 0.2226\n",
      "Epoch 18 Batch 2250 Loss 0.6497 Accuracy 0.2226\n",
      "Epoch 18 Batch 2300 Loss 0.6511 Accuracy 0.2226\n",
      "Epoch 18 Batch 2350 Loss 0.6513 Accuracy 0.2225\n",
      "Epoch 18 Batch 2400 Loss 0.6524 Accuracy 0.2225\n",
      "Epoch 18 Batch 2450 Loss 0.6525 Accuracy 0.2225\n",
      "Epoch 18 Batch 2500 Loss 0.6527 Accuracy 0.2224\n",
      "Epoch 18 Batch 2550 Loss 0.6532 Accuracy 0.2223\n",
      "Epoch 18 Batch 2600 Loss 0.6536 Accuracy 0.2223\n",
      "Epoch 18 Batch 2650 Loss 0.6541 Accuracy 0.2223\n",
      "Epoch 18 Batch 2700 Loss 0.6542 Accuracy 0.2223\n",
      "Epoch 18 Batch 2750 Loss 0.6546 Accuracy 0.2223\n",
      "Epoch 18 Batch 2800 Loss 0.6550 Accuracy 0.2223\n",
      "Epoch 18 Loss 0.6551 Accuracy 0.2223\n",
      "Time taken for 1 epoch: 1114.3307218551636 secs\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.7097 Accuracy 0.2172\n",
      "Epoch 19 Batch 50 Loss 0.6255 Accuracy 0.2237\n",
      "Epoch 19 Batch 100 Loss 0.6233 Accuracy 0.2234\n",
      "Epoch 19 Batch 150 Loss 0.6187 Accuracy 0.2231\n",
      "Epoch 19 Batch 200 Loss 0.6173 Accuracy 0.2232\n",
      "Epoch 19 Batch 250 Loss 0.6220 Accuracy 0.2237\n",
      "Epoch 19 Batch 300 Loss 0.6211 Accuracy 0.2233\n",
      "Epoch 19 Batch 350 Loss 0.6224 Accuracy 0.2234\n",
      "Epoch 19 Batch 400 Loss 0.6214 Accuracy 0.2235\n",
      "Epoch 19 Batch 450 Loss 0.6188 Accuracy 0.2238\n",
      "Epoch 19 Batch 500 Loss 0.6206 Accuracy 0.2237\n",
      "Epoch 19 Batch 550 Loss 0.6219 Accuracy 0.2235\n",
      "Epoch 19 Batch 600 Loss 0.6226 Accuracy 0.2233\n",
      "Epoch 19 Batch 650 Loss 0.6253 Accuracy 0.2233\n",
      "Epoch 19 Batch 700 Loss 0.6267 Accuracy 0.2233\n",
      "Epoch 19 Batch 750 Loss 0.6261 Accuracy 0.2232\n",
      "Epoch 19 Batch 800 Loss 0.6254 Accuracy 0.2231\n",
      "Epoch 19 Batch 850 Loss 0.6263 Accuracy 0.2232\n",
      "Epoch 19 Batch 900 Loss 0.6274 Accuracy 0.2231\n",
      "Epoch 19 Batch 950 Loss 0.6276 Accuracy 0.2231\n",
      "Epoch 19 Batch 1000 Loss 0.6277 Accuracy 0.2230\n",
      "Epoch 19 Batch 1050 Loss 0.6282 Accuracy 0.2229\n",
      "Epoch 19 Batch 1100 Loss 0.6296 Accuracy 0.2229\n",
      "Epoch 19 Batch 1150 Loss 0.6303 Accuracy 0.2229\n",
      "Epoch 19 Batch 1200 Loss 0.6311 Accuracy 0.2229\n",
      "Epoch 19 Batch 1250 Loss 0.6328 Accuracy 0.2229\n",
      "Epoch 19 Batch 1300 Loss 0.6338 Accuracy 0.2228\n",
      "Epoch 19 Batch 1350 Loss 0.6343 Accuracy 0.2228\n",
      "Epoch 19 Batch 1400 Loss 0.6357 Accuracy 0.2228\n",
      "Epoch 19 Batch 1450 Loss 0.6373 Accuracy 0.2228\n",
      "Epoch 19 Batch 1500 Loss 0.6381 Accuracy 0.2227\n",
      "Epoch 19 Batch 1550 Loss 0.6381 Accuracy 0.2228\n",
      "Epoch 19 Batch 1600 Loss 0.6384 Accuracy 0.2227\n",
      "Epoch 19 Batch 1650 Loss 0.6394 Accuracy 0.2227\n",
      "Epoch 19 Batch 1700 Loss 0.6407 Accuracy 0.2227\n",
      "Epoch 19 Batch 1750 Loss 0.6412 Accuracy 0.2228\n",
      "Epoch 19 Batch 1800 Loss 0.6419 Accuracy 0.2228\n",
      "Epoch 19 Batch 1850 Loss 0.6422 Accuracy 0.2227\n",
      "Epoch 19 Batch 1900 Loss 0.6427 Accuracy 0.2227\n",
      "Epoch 19 Batch 1950 Loss 0.6434 Accuracy 0.2227\n",
      "Epoch 19 Batch 2000 Loss 0.6441 Accuracy 0.2226\n",
      "Epoch 19 Batch 2050 Loss 0.6441 Accuracy 0.2226\n",
      "Epoch 19 Batch 2100 Loss 0.6447 Accuracy 0.2226\n",
      "Epoch 19 Batch 2150 Loss 0.6455 Accuracy 0.2226\n",
      "Epoch 19 Batch 2200 Loss 0.6464 Accuracy 0.2226\n",
      "Epoch 19 Batch 2250 Loss 0.6471 Accuracy 0.2225\n",
      "Epoch 19 Batch 2300 Loss 0.6471 Accuracy 0.2225\n",
      "Epoch 19 Batch 2350 Loss 0.6477 Accuracy 0.2225\n",
      "Epoch 19 Batch 2400 Loss 0.6483 Accuracy 0.2225\n",
      "Epoch 19 Batch 2450 Loss 0.6490 Accuracy 0.2225\n",
      "Epoch 19 Batch 2500 Loss 0.6497 Accuracy 0.2224\n",
      "Epoch 19 Batch 2550 Loss 0.6504 Accuracy 0.2224\n",
      "Epoch 19 Batch 2600 Loss 0.6508 Accuracy 0.2223\n",
      "Epoch 19 Batch 2650 Loss 0.6514 Accuracy 0.2223\n",
      "Epoch 19 Batch 2700 Loss 0.6518 Accuracy 0.2223\n",
      "Epoch 19 Batch 2750 Loss 0.6523 Accuracy 0.2222\n",
      "Epoch 19 Batch 2800 Loss 0.6526 Accuracy 0.2222\n",
      "Epoch 19 Loss 0.6526 Accuracy 0.2222\n",
      "Time taken for 1 epoch: 1085.113135099411 secs\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.4673 Accuracy 0.2348\n",
      "Epoch 20 Batch 50 Loss 0.5992 Accuracy 0.2243\n",
      "Epoch 20 Batch 100 Loss 0.5875 Accuracy 0.2240\n",
      "Epoch 20 Batch 150 Loss 0.5957 Accuracy 0.2241\n",
      "Epoch 20 Batch 200 Loss 0.6028 Accuracy 0.2237\n",
      "Epoch 20 Batch 250 Loss 0.6057 Accuracy 0.2236\n",
      "Epoch 20 Batch 300 Loss 0.6088 Accuracy 0.2234\n",
      "Epoch 20 Batch 350 Loss 0.6089 Accuracy 0.2232\n",
      "Epoch 20 Batch 400 Loss 0.6128 Accuracy 0.2232\n",
      "Epoch 20 Batch 450 Loss 0.6115 Accuracy 0.2232\n",
      "Epoch 20 Batch 500 Loss 0.6119 Accuracy 0.2233\n",
      "Epoch 20 Batch 550 Loss 0.6134 Accuracy 0.2232\n",
      "Epoch 20 Batch 600 Loss 0.6170 Accuracy 0.2233\n",
      "Epoch 20 Batch 650 Loss 0.6198 Accuracy 0.2234\n",
      "Epoch 20 Batch 700 Loss 0.6208 Accuracy 0.2232\n",
      "Epoch 20 Batch 750 Loss 0.6220 Accuracy 0.2233\n",
      "Epoch 20 Batch 800 Loss 0.6234 Accuracy 0.2233\n",
      "Epoch 20 Batch 850 Loss 0.6261 Accuracy 0.2232\n",
      "Epoch 20 Batch 900 Loss 0.6259 Accuracy 0.2232\n",
      "Epoch 20 Batch 950 Loss 0.6268 Accuracy 0.2233\n",
      "Epoch 20 Batch 1000 Loss 0.6280 Accuracy 0.2232\n",
      "Epoch 20 Batch 1050 Loss 0.6278 Accuracy 0.2233\n",
      "Epoch 20 Batch 1100 Loss 0.6280 Accuracy 0.2232\n",
      "Epoch 20 Batch 1150 Loss 0.6292 Accuracy 0.2231\n",
      "Epoch 20 Batch 1200 Loss 0.6303 Accuracy 0.2230\n",
      "Epoch 20 Batch 1250 Loss 0.6319 Accuracy 0.2231\n",
      "Epoch 20 Batch 1300 Loss 0.6329 Accuracy 0.2231\n",
      "Epoch 20 Batch 1350 Loss 0.6333 Accuracy 0.2231\n",
      "Epoch 20 Batch 1400 Loss 0.6340 Accuracy 0.2230\n",
      "Epoch 20 Batch 1450 Loss 0.6344 Accuracy 0.2229\n",
      "Epoch 20 Batch 1500 Loss 0.6357 Accuracy 0.2229\n",
      "Epoch 20 Batch 1550 Loss 0.6369 Accuracy 0.2229\n",
      "Epoch 20 Batch 1600 Loss 0.6379 Accuracy 0.2229\n",
      "Epoch 20 Batch 1650 Loss 0.6388 Accuracy 0.2228\n",
      "Epoch 20 Batch 1700 Loss 0.6394 Accuracy 0.2229\n",
      "Epoch 20 Batch 1750 Loss 0.6401 Accuracy 0.2228\n",
      "Epoch 20 Batch 1800 Loss 0.6402 Accuracy 0.2228\n",
      "Epoch 20 Batch 1850 Loss 0.6405 Accuracy 0.2227\n",
      "Epoch 20 Batch 1900 Loss 0.6418 Accuracy 0.2226\n",
      "Epoch 20 Batch 1950 Loss 0.6423 Accuracy 0.2226\n",
      "Epoch 20 Batch 2000 Loss 0.6427 Accuracy 0.2225\n",
      "Epoch 20 Batch 2050 Loss 0.6431 Accuracy 0.2226\n",
      "Epoch 20 Batch 2100 Loss 0.6436 Accuracy 0.2225\n",
      "Epoch 20 Batch 2150 Loss 0.6446 Accuracy 0.2225\n",
      "Epoch 20 Batch 2200 Loss 0.6453 Accuracy 0.2225\n",
      "Epoch 20 Batch 2250 Loss 0.6460 Accuracy 0.2224\n",
      "Epoch 20 Batch 2300 Loss 0.6470 Accuracy 0.2224\n",
      "Epoch 20 Batch 2350 Loss 0.6479 Accuracy 0.2224\n",
      "Epoch 20 Batch 2400 Loss 0.6483 Accuracy 0.2224\n",
      "Epoch 20 Batch 2450 Loss 0.6490 Accuracy 0.2224\n",
      "Epoch 20 Batch 2500 Loss 0.6494 Accuracy 0.2223\n",
      "Epoch 20 Batch 2550 Loss 0.6495 Accuracy 0.2224\n",
      "Epoch 20 Batch 2600 Loss 0.6500 Accuracy 0.2223\n",
      "Epoch 20 Batch 2650 Loss 0.6501 Accuracy 0.2223\n",
      "Epoch 20 Batch 2700 Loss 0.6504 Accuracy 0.2224\n",
      "Epoch 20 Batch 2750 Loss 0.6511 Accuracy 0.2223\n",
      "Epoch 20 Batch 2800 Loss 0.6518 Accuracy 0.2224\n",
      "Saving checkpoint for epoch 20 at ./checkpoints_deu/train_test/ckpt-4\n",
      "Epoch 20 Loss 0.6518 Accuracy 0.2224\n",
      "Time taken for 1 epoch: 1046.5133211612701 secs\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.5108 Accuracy 0.2115\n",
      "Epoch 21 Batch 50 Loss 0.5977 Accuracy 0.2250\n",
      "Epoch 21 Batch 100 Loss 0.6018 Accuracy 0.2255\n",
      "Epoch 21 Batch 150 Loss 0.5969 Accuracy 0.2257\n",
      "Epoch 21 Batch 200 Loss 0.6041 Accuracy 0.2248\n",
      "Epoch 21 Batch 250 Loss 0.6105 Accuracy 0.2248\n",
      "Epoch 21 Batch 300 Loss 0.6120 Accuracy 0.2244\n",
      "Epoch 21 Batch 350 Loss 0.6127 Accuracy 0.2243\n",
      "Epoch 21 Batch 400 Loss 0.6127 Accuracy 0.2243\n",
      "Epoch 21 Batch 450 Loss 0.6145 Accuracy 0.2242\n",
      "Epoch 21 Batch 500 Loss 0.6150 Accuracy 0.2241\n",
      "Epoch 21 Batch 550 Loss 0.6160 Accuracy 0.2242\n",
      "Epoch 21 Batch 600 Loss 0.6174 Accuracy 0.2241\n",
      "Epoch 21 Batch 650 Loss 0.6183 Accuracy 0.2240\n",
      "Epoch 21 Batch 700 Loss 0.6183 Accuracy 0.2240\n",
      "Epoch 21 Batch 750 Loss 0.6201 Accuracy 0.2239\n",
      "Epoch 21 Batch 800 Loss 0.6199 Accuracy 0.2239\n",
      "Epoch 21 Batch 850 Loss 0.6197 Accuracy 0.2239\n",
      "Epoch 21 Batch 900 Loss 0.6203 Accuracy 0.2240\n",
      "Epoch 21 Batch 950 Loss 0.6222 Accuracy 0.2238\n",
      "Epoch 21 Batch 1000 Loss 0.6227 Accuracy 0.2238\n",
      "Epoch 21 Batch 1050 Loss 0.6248 Accuracy 0.2238\n",
      "Epoch 21 Batch 1100 Loss 0.6265 Accuracy 0.2238\n",
      "Epoch 21 Batch 1150 Loss 0.6276 Accuracy 0.2237\n",
      "Epoch 21 Batch 1200 Loss 0.6289 Accuracy 0.2237\n",
      "Epoch 21 Batch 1250 Loss 0.6294 Accuracy 0.2236\n",
      "Epoch 21 Batch 1300 Loss 0.6301 Accuracy 0.2235\n",
      "Epoch 21 Batch 1350 Loss 0.6307 Accuracy 0.2235\n",
      "Epoch 21 Batch 1400 Loss 0.6313 Accuracy 0.2235\n",
      "Epoch 21 Batch 1450 Loss 0.6318 Accuracy 0.2236\n",
      "Epoch 21 Batch 1500 Loss 0.6328 Accuracy 0.2235\n",
      "Epoch 21 Batch 1550 Loss 0.6341 Accuracy 0.2234\n",
      "Epoch 21 Batch 1600 Loss 0.6354 Accuracy 0.2234\n",
      "Epoch 21 Batch 1650 Loss 0.6363 Accuracy 0.2234\n",
      "Epoch 21 Batch 1700 Loss 0.6374 Accuracy 0.2233\n",
      "Epoch 21 Batch 1750 Loss 0.6383 Accuracy 0.2232\n",
      "Epoch 21 Batch 1800 Loss 0.6386 Accuracy 0.2231\n",
      "Epoch 21 Batch 1850 Loss 0.6388 Accuracy 0.2231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 1900 Loss 0.6394 Accuracy 0.2231\n",
      "Epoch 21 Batch 1950 Loss 0.6407 Accuracy 0.2231\n",
      "Epoch 21 Batch 2000 Loss 0.6413 Accuracy 0.2230\n",
      "Epoch 21 Batch 2050 Loss 0.6415 Accuracy 0.2229\n",
      "Epoch 21 Batch 2100 Loss 0.6421 Accuracy 0.2229\n",
      "Epoch 21 Batch 2150 Loss 0.6423 Accuracy 0.2229\n",
      "Epoch 21 Batch 2200 Loss 0.6425 Accuracy 0.2228\n",
      "Epoch 21 Batch 2250 Loss 0.6432 Accuracy 0.2228\n",
      "Epoch 21 Batch 2300 Loss 0.6441 Accuracy 0.2227\n",
      "Epoch 21 Batch 2350 Loss 0.6445 Accuracy 0.2228\n",
      "Epoch 21 Batch 2400 Loss 0.6454 Accuracy 0.2227\n",
      "Epoch 21 Batch 2450 Loss 0.6456 Accuracy 0.2227\n",
      "Epoch 21 Batch 2500 Loss 0.6461 Accuracy 0.2226\n",
      "Epoch 21 Batch 2550 Loss 0.6464 Accuracy 0.2226\n",
      "Epoch 21 Batch 2600 Loss 0.6467 Accuracy 0.2226\n",
      "Epoch 21 Batch 2650 Loss 0.6470 Accuracy 0.2226\n",
      "Epoch 21 Batch 2700 Loss 0.6475 Accuracy 0.2226\n",
      "Epoch 21 Batch 2750 Loss 0.6475 Accuracy 0.2226\n",
      "Epoch 21 Batch 2800 Loss 0.6477 Accuracy 0.2226\n",
      "Epoch 21 Loss 0.6477 Accuracy 0.2226\n",
      "Time taken for 1 epoch: 1044.5922532081604 secs\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.6297 Accuracy 0.2321\n",
      "Epoch 22 Batch 50 Loss 0.5992 Accuracy 0.2257\n",
      "Epoch 22 Batch 100 Loss 0.6047 Accuracy 0.2239\n",
      "Epoch 22 Batch 150 Loss 0.6067 Accuracy 0.2238\n",
      "Epoch 22 Batch 200 Loss 0.6111 Accuracy 0.2237\n",
      "Epoch 22 Batch 250 Loss 0.6049 Accuracy 0.2236\n",
      "Epoch 22 Batch 300 Loss 0.6067 Accuracy 0.2237\n",
      "Epoch 22 Batch 350 Loss 0.6069 Accuracy 0.2237\n",
      "Epoch 22 Batch 400 Loss 0.6077 Accuracy 0.2239\n",
      "Epoch 22 Batch 450 Loss 0.6078 Accuracy 0.2237\n",
      "Epoch 22 Batch 500 Loss 0.6106 Accuracy 0.2234\n",
      "Epoch 22 Batch 550 Loss 0.6112 Accuracy 0.2236\n",
      "Epoch 22 Batch 600 Loss 0.6138 Accuracy 0.2236\n",
      "Epoch 22 Batch 650 Loss 0.6159 Accuracy 0.2236\n",
      "Epoch 22 Batch 700 Loss 0.6172 Accuracy 0.2235\n",
      "Epoch 22 Batch 750 Loss 0.6177 Accuracy 0.2236\n",
      "Epoch 22 Batch 800 Loss 0.6191 Accuracy 0.2235\n",
      "Epoch 22 Batch 850 Loss 0.6200 Accuracy 0.2236\n",
      "Epoch 22 Batch 900 Loss 0.6206 Accuracy 0.2235\n",
      "Epoch 22 Batch 950 Loss 0.6209 Accuracy 0.2235\n",
      "Epoch 22 Batch 1000 Loss 0.6206 Accuracy 0.2234\n",
      "Epoch 22 Batch 1050 Loss 0.6214 Accuracy 0.2235\n",
      "Epoch 22 Batch 1100 Loss 0.6229 Accuracy 0.2235\n",
      "Epoch 22 Batch 1150 Loss 0.6241 Accuracy 0.2235\n",
      "Epoch 22 Batch 1200 Loss 0.6251 Accuracy 0.2234\n",
      "Epoch 22 Batch 1250 Loss 0.6261 Accuracy 0.2233\n",
      "Epoch 22 Batch 1300 Loss 0.6271 Accuracy 0.2233\n",
      "Epoch 22 Batch 1350 Loss 0.6288 Accuracy 0.2233\n",
      "Epoch 22 Batch 1400 Loss 0.6297 Accuracy 0.2232\n",
      "Epoch 22 Batch 1450 Loss 0.6302 Accuracy 0.2231\n",
      "Epoch 22 Batch 1500 Loss 0.6307 Accuracy 0.2231\n",
      "Epoch 22 Batch 1550 Loss 0.6309 Accuracy 0.2232\n",
      "Epoch 22 Batch 1600 Loss 0.6315 Accuracy 0.2231\n",
      "Epoch 22 Batch 1650 Loss 0.6313 Accuracy 0.2231\n",
      "Epoch 22 Batch 1700 Loss 0.6323 Accuracy 0.2231\n",
      "Epoch 22 Batch 1750 Loss 0.6327 Accuracy 0.2231\n",
      "Epoch 22 Batch 1800 Loss 0.6342 Accuracy 0.2230\n",
      "Epoch 22 Batch 1850 Loss 0.6351 Accuracy 0.2230\n",
      "Epoch 22 Batch 1900 Loss 0.6359 Accuracy 0.2231\n",
      "Epoch 22 Batch 1950 Loss 0.6366 Accuracy 0.2230\n",
      "Epoch 22 Batch 2000 Loss 0.6370 Accuracy 0.2229\n",
      "Epoch 22 Batch 2050 Loss 0.6378 Accuracy 0.2229\n",
      "Epoch 22 Batch 2100 Loss 0.6385 Accuracy 0.2229\n",
      "Epoch 22 Batch 2150 Loss 0.6395 Accuracy 0.2228\n",
      "Epoch 22 Batch 2200 Loss 0.6398 Accuracy 0.2228\n",
      "Epoch 22 Batch 2250 Loss 0.6404 Accuracy 0.2228\n",
      "Epoch 22 Batch 2300 Loss 0.6407 Accuracy 0.2228\n",
      "Epoch 22 Batch 2350 Loss 0.6411 Accuracy 0.2227\n",
      "Epoch 22 Batch 2400 Loss 0.6419 Accuracy 0.2227\n",
      "Epoch 22 Batch 2450 Loss 0.6421 Accuracy 0.2227\n",
      "Epoch 22 Batch 2500 Loss 0.6426 Accuracy 0.2227\n",
      "Epoch 22 Batch 2550 Loss 0.6430 Accuracy 0.2227\n",
      "Epoch 22 Batch 2600 Loss 0.6434 Accuracy 0.2227\n",
      "Epoch 22 Batch 2650 Loss 0.6434 Accuracy 0.2227\n",
      "Epoch 22 Batch 2700 Loss 0.6441 Accuracy 0.2227\n",
      "Epoch 22 Batch 2750 Loss 0.6444 Accuracy 0.2227\n",
      "Epoch 22 Batch 2800 Loss 0.6446 Accuracy 0.2227\n",
      "Epoch 22 Loss 0.6447 Accuracy 0.2227\n",
      "Time taken for 1 epoch: 1036.5426008701324 secs\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.3881 Accuracy 0.2252\n",
      "Epoch 23 Batch 50 Loss 0.5807 Accuracy 0.2225\n",
      "Epoch 23 Batch 100 Loss 0.5984 Accuracy 0.2232\n",
      "Epoch 23 Batch 150 Loss 0.6012 Accuracy 0.2229\n",
      "Epoch 23 Batch 200 Loss 0.5983 Accuracy 0.2228\n",
      "Epoch 23 Batch 250 Loss 0.5986 Accuracy 0.2231\n",
      "Epoch 23 Batch 300 Loss 0.6054 Accuracy 0.2236\n",
      "Epoch 23 Batch 350 Loss 0.6062 Accuracy 0.2237\n",
      "Epoch 23 Batch 400 Loss 0.6061 Accuracy 0.2238\n",
      "Epoch 23 Batch 450 Loss 0.6066 Accuracy 0.2238\n",
      "Epoch 23 Batch 500 Loss 0.6080 Accuracy 0.2238\n",
      "Epoch 23 Batch 550 Loss 0.6080 Accuracy 0.2236\n",
      "Epoch 23 Batch 600 Loss 0.6093 Accuracy 0.2236\n",
      "Epoch 23 Batch 650 Loss 0.6115 Accuracy 0.2238\n",
      "Epoch 23 Batch 700 Loss 0.6129 Accuracy 0.2237\n",
      "Epoch 23 Batch 750 Loss 0.6147 Accuracy 0.2237\n",
      "Epoch 23 Batch 800 Loss 0.6157 Accuracy 0.2237\n",
      "Epoch 23 Batch 850 Loss 0.6176 Accuracy 0.2237\n",
      "Epoch 23 Batch 900 Loss 0.6191 Accuracy 0.2236\n",
      "Epoch 23 Batch 950 Loss 0.6213 Accuracy 0.2236\n",
      "Epoch 23 Batch 1000 Loss 0.6224 Accuracy 0.2236\n",
      "Epoch 23 Batch 1050 Loss 0.6231 Accuracy 0.2235\n",
      "Epoch 23 Batch 1100 Loss 0.6240 Accuracy 0.2235\n",
      "Epoch 23 Batch 1150 Loss 0.6246 Accuracy 0.2235\n",
      "Epoch 23 Batch 1200 Loss 0.6251 Accuracy 0.2234\n",
      "Epoch 23 Batch 1250 Loss 0.6257 Accuracy 0.2233\n",
      "Epoch 23 Batch 1300 Loss 0.6266 Accuracy 0.2232\n",
      "Epoch 23 Batch 1350 Loss 0.6272 Accuracy 0.2233\n",
      "Epoch 23 Batch 1400 Loss 0.6282 Accuracy 0.2232\n",
      "Epoch 23 Batch 1450 Loss 0.6292 Accuracy 0.2232\n",
      "Epoch 23 Batch 1500 Loss 0.6292 Accuracy 0.2232\n",
      "Epoch 23 Batch 1550 Loss 0.6294 Accuracy 0.2232\n",
      "Epoch 23 Batch 1600 Loss 0.6293 Accuracy 0.2231\n",
      "Epoch 23 Batch 1650 Loss 0.6301 Accuracy 0.2232\n",
      "Epoch 23 Batch 1700 Loss 0.6310 Accuracy 0.2232\n",
      "Epoch 23 Batch 1750 Loss 0.6317 Accuracy 0.2231\n",
      "Epoch 23 Batch 1800 Loss 0.6325 Accuracy 0.2231\n",
      "Epoch 23 Batch 1850 Loss 0.6327 Accuracy 0.2231\n",
      "Epoch 23 Batch 1900 Loss 0.6338 Accuracy 0.2231\n",
      "Epoch 23 Batch 1950 Loss 0.6346 Accuracy 0.2230\n",
      "Epoch 23 Batch 2000 Loss 0.6354 Accuracy 0.2230\n",
      "Epoch 23 Batch 2050 Loss 0.6361 Accuracy 0.2230\n",
      "Epoch 23 Batch 2100 Loss 0.6364 Accuracy 0.2230\n",
      "Epoch 23 Batch 2150 Loss 0.6369 Accuracy 0.2229\n",
      "Epoch 23 Batch 2200 Loss 0.6380 Accuracy 0.2229\n",
      "Epoch 23 Batch 2250 Loss 0.6383 Accuracy 0.2229\n",
      "Epoch 23 Batch 2300 Loss 0.6388 Accuracy 0.2228\n",
      "Epoch 23 Batch 2350 Loss 0.6393 Accuracy 0.2228\n",
      "Epoch 23 Batch 2400 Loss 0.6396 Accuracy 0.2228\n",
      "Epoch 23 Batch 2450 Loss 0.6400 Accuracy 0.2228\n",
      "Epoch 23 Batch 2500 Loss 0.6406 Accuracy 0.2228\n",
      "Epoch 23 Batch 2550 Loss 0.6413 Accuracy 0.2228\n",
      "Epoch 23 Batch 2600 Loss 0.6416 Accuracy 0.2228\n",
      "Epoch 23 Batch 2650 Loss 0.6422 Accuracy 0.2228\n",
      "Epoch 23 Batch 2700 Loss 0.6428 Accuracy 0.2227\n",
      "Epoch 23 Batch 2750 Loss 0.6433 Accuracy 0.2227\n",
      "Epoch 23 Batch 2800 Loss 0.6438 Accuracy 0.2227\n",
      "Epoch 23 Loss 0.6438 Accuracy 0.2227\n",
      "Time taken for 1 epoch: 1040.2725319862366 secs\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.4271 Accuracy 0.2511\n",
      "Epoch 24 Batch 50 Loss 0.5937 Accuracy 0.2256\n",
      "Epoch 24 Batch 100 Loss 0.5980 Accuracy 0.2249\n",
      "Epoch 24 Batch 150 Loss 0.6013 Accuracy 0.2246\n",
      "Epoch 24 Batch 200 Loss 0.6028 Accuracy 0.2241\n",
      "Epoch 24 Batch 250 Loss 0.6044 Accuracy 0.2236\n",
      "Epoch 24 Batch 300 Loss 0.6059 Accuracy 0.2236\n",
      "Epoch 24 Batch 350 Loss 0.6069 Accuracy 0.2234\n",
      "Epoch 24 Batch 400 Loss 0.6087 Accuracy 0.2238\n",
      "Epoch 24 Batch 450 Loss 0.6130 Accuracy 0.2237\n",
      "Epoch 24 Batch 500 Loss 0.6136 Accuracy 0.2236\n",
      "Epoch 24 Batch 550 Loss 0.6166 Accuracy 0.2235\n",
      "Epoch 24 Batch 600 Loss 0.6184 Accuracy 0.2236\n",
      "Epoch 24 Batch 650 Loss 0.6177 Accuracy 0.2235\n",
      "Epoch 24 Batch 700 Loss 0.6193 Accuracy 0.2234\n",
      "Epoch 24 Batch 750 Loss 0.6194 Accuracy 0.2233\n",
      "Epoch 24 Batch 800 Loss 0.6197 Accuracy 0.2233\n",
      "Epoch 24 Batch 850 Loss 0.6213 Accuracy 0.2233\n",
      "Epoch 24 Batch 900 Loss 0.6215 Accuracy 0.2234\n",
      "Epoch 24 Batch 950 Loss 0.6218 Accuracy 0.2233\n",
      "Epoch 24 Batch 1000 Loss 0.6231 Accuracy 0.2232\n",
      "Epoch 24 Batch 1050 Loss 0.6227 Accuracy 0.2231\n",
      "Epoch 24 Batch 1100 Loss 0.6235 Accuracy 0.2231\n",
      "Epoch 24 Batch 1150 Loss 0.6240 Accuracy 0.2232\n",
      "Epoch 24 Batch 1200 Loss 0.6246 Accuracy 0.2233\n",
      "Epoch 24 Batch 1250 Loss 0.6248 Accuracy 0.2232\n",
      "Epoch 24 Batch 1300 Loss 0.6251 Accuracy 0.2231\n",
      "Epoch 24 Batch 1350 Loss 0.6256 Accuracy 0.2231\n",
      "Epoch 24 Batch 1400 Loss 0.6260 Accuracy 0.2231\n",
      "Epoch 24 Batch 1450 Loss 0.6265 Accuracy 0.2230\n",
      "Epoch 24 Batch 1500 Loss 0.6269 Accuracy 0.2230\n",
      "Epoch 24 Batch 1550 Loss 0.6276 Accuracy 0.2231\n",
      "Epoch 24 Batch 1600 Loss 0.6285 Accuracy 0.2231\n",
      "Epoch 24 Batch 1650 Loss 0.6295 Accuracy 0.2231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 1700 Loss 0.6312 Accuracy 0.2231\n",
      "Epoch 24 Batch 1750 Loss 0.6311 Accuracy 0.2231\n",
      "Epoch 24 Batch 1800 Loss 0.6315 Accuracy 0.2231\n",
      "Epoch 24 Batch 1850 Loss 0.6318 Accuracy 0.2231\n",
      "Epoch 24 Batch 1900 Loss 0.6323 Accuracy 0.2230\n",
      "Epoch 24 Batch 1950 Loss 0.6330 Accuracy 0.2230\n",
      "Epoch 24 Batch 2000 Loss 0.6333 Accuracy 0.2230\n",
      "Epoch 24 Batch 2050 Loss 0.6336 Accuracy 0.2230\n",
      "Epoch 24 Batch 2100 Loss 0.6341 Accuracy 0.2230\n",
      "Epoch 24 Batch 2150 Loss 0.6343 Accuracy 0.2230\n",
      "Epoch 24 Batch 2200 Loss 0.6351 Accuracy 0.2230\n",
      "Epoch 24 Batch 2250 Loss 0.6359 Accuracy 0.2230\n",
      "Epoch 24 Batch 2300 Loss 0.6361 Accuracy 0.2229\n",
      "Epoch 24 Batch 2350 Loss 0.6368 Accuracy 0.2229\n",
      "Epoch 24 Batch 2400 Loss 0.6377 Accuracy 0.2229\n",
      "Epoch 24 Batch 2450 Loss 0.6378 Accuracy 0.2228\n",
      "Epoch 24 Batch 2500 Loss 0.6379 Accuracy 0.2228\n",
      "Epoch 24 Batch 2550 Loss 0.6386 Accuracy 0.2229\n",
      "Epoch 24 Batch 2600 Loss 0.6392 Accuracy 0.2229\n",
      "Epoch 24 Batch 2650 Loss 0.6395 Accuracy 0.2229\n",
      "Epoch 24 Batch 2700 Loss 0.6399 Accuracy 0.2229\n",
      "Epoch 24 Batch 2750 Loss 0.6402 Accuracy 0.2228\n",
      "Epoch 24 Batch 2800 Loss 0.6410 Accuracy 0.2228\n",
      "Epoch 24 Loss 0.6410 Accuracy 0.2228\n",
      "Time taken for 1 epoch: 1033.4655120372772 secs\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.7569 Accuracy 0.2359\n",
      "Epoch 25 Batch 50 Loss 0.6080 Accuracy 0.2232\n",
      "Epoch 25 Batch 100 Loss 0.5989 Accuracy 0.2235\n",
      "Epoch 25 Batch 150 Loss 0.6000 Accuracy 0.2237\n",
      "Epoch 25 Batch 200 Loss 0.5954 Accuracy 0.2242\n",
      "Epoch 25 Batch 250 Loss 0.5958 Accuracy 0.2237\n",
      "Epoch 25 Batch 300 Loss 0.5964 Accuracy 0.2236\n",
      "Epoch 25 Batch 350 Loss 0.5995 Accuracy 0.2237\n",
      "Epoch 25 Batch 400 Loss 0.6045 Accuracy 0.2238\n",
      "Epoch 25 Batch 450 Loss 0.6035 Accuracy 0.2237\n",
      "Epoch 25 Batch 500 Loss 0.6053 Accuracy 0.2238\n",
      "Epoch 25 Batch 550 Loss 0.6064 Accuracy 0.2238\n",
      "Epoch 25 Batch 600 Loss 0.6102 Accuracy 0.2237\n",
      "Epoch 25 Batch 650 Loss 0.6112 Accuracy 0.2236\n",
      "Epoch 25 Batch 700 Loss 0.6130 Accuracy 0.2236\n",
      "Epoch 25 Batch 750 Loss 0.6138 Accuracy 0.2236\n",
      "Epoch 25 Batch 800 Loss 0.6141 Accuracy 0.2236\n",
      "Epoch 25 Batch 850 Loss 0.6158 Accuracy 0.2234\n",
      "Epoch 25 Batch 900 Loss 0.6166 Accuracy 0.2234\n",
      "Epoch 25 Batch 950 Loss 0.6176 Accuracy 0.2235\n",
      "Epoch 25 Batch 1000 Loss 0.6184 Accuracy 0.2236\n",
      "Epoch 25 Batch 1050 Loss 0.6198 Accuracy 0.2236\n",
      "Epoch 25 Batch 1100 Loss 0.6209 Accuracy 0.2235\n",
      "Epoch 25 Batch 1150 Loss 0.6230 Accuracy 0.2235\n",
      "Epoch 25 Batch 1200 Loss 0.6233 Accuracy 0.2235\n",
      "Epoch 25 Batch 1250 Loss 0.6237 Accuracy 0.2234\n",
      "Epoch 25 Batch 1300 Loss 0.6245 Accuracy 0.2235\n",
      "Epoch 25 Batch 1350 Loss 0.6243 Accuracy 0.2235\n",
      "Epoch 25 Batch 1400 Loss 0.6246 Accuracy 0.2234\n",
      "Epoch 25 Batch 1450 Loss 0.6251 Accuracy 0.2234\n",
      "Epoch 25 Batch 1500 Loss 0.6261 Accuracy 0.2234\n",
      "Epoch 25 Batch 1550 Loss 0.6271 Accuracy 0.2234\n",
      "Epoch 25 Batch 1600 Loss 0.6277 Accuracy 0.2234\n",
      "Epoch 25 Batch 1650 Loss 0.6284 Accuracy 0.2233\n",
      "Epoch 25 Batch 1700 Loss 0.6286 Accuracy 0.2232\n",
      "Epoch 25 Batch 1750 Loss 0.6294 Accuracy 0.2232\n",
      "Epoch 25 Batch 1800 Loss 0.6299 Accuracy 0.2232\n",
      "Epoch 25 Batch 1850 Loss 0.6305 Accuracy 0.2232\n",
      "Epoch 25 Batch 1900 Loss 0.6310 Accuracy 0.2232\n",
      "Epoch 25 Batch 1950 Loss 0.6316 Accuracy 0.2232\n",
      "Epoch 25 Batch 2000 Loss 0.6327 Accuracy 0.2232\n",
      "Epoch 25 Batch 2050 Loss 0.6330 Accuracy 0.2232\n",
      "Epoch 25 Batch 2100 Loss 0.6331 Accuracy 0.2231\n",
      "Epoch 25 Batch 2150 Loss 0.6332 Accuracy 0.2231\n",
      "Epoch 25 Batch 2200 Loss 0.6341 Accuracy 0.2231\n",
      "Epoch 25 Batch 2250 Loss 0.6342 Accuracy 0.2230\n",
      "Epoch 25 Batch 2300 Loss 0.6353 Accuracy 0.2230\n",
      "Epoch 25 Batch 2350 Loss 0.6359 Accuracy 0.2230\n",
      "Epoch 25 Batch 2400 Loss 0.6360 Accuracy 0.2230\n",
      "Epoch 25 Batch 2450 Loss 0.6365 Accuracy 0.2230\n",
      "Epoch 25 Batch 2500 Loss 0.6369 Accuracy 0.2230\n",
      "Epoch 25 Batch 2550 Loss 0.6373 Accuracy 0.2230\n",
      "Epoch 25 Batch 2600 Loss 0.6378 Accuracy 0.2230\n",
      "Epoch 25 Batch 2650 Loss 0.6385 Accuracy 0.2229\n",
      "Epoch 25 Batch 2700 Loss 0.6393 Accuracy 0.2229\n",
      "Epoch 25 Batch 2750 Loss 0.6398 Accuracy 0.2229\n",
      "Epoch 25 Batch 2800 Loss 0.6397 Accuracy 0.2229\n",
      "Saving checkpoint for epoch 25 at ./checkpoints_deu/train_test/ckpt-5\n",
      "Epoch 25 Loss 0.6398 Accuracy 0.2229\n",
      "Time taken for 1 epoch: 1055.1240479946136 secs\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.5271 Accuracy 0.2046\n",
      "Epoch 26 Batch 50 Loss 0.5896 Accuracy 0.2248\n",
      "Epoch 26 Batch 100 Loss 0.5861 Accuracy 0.2251\n",
      "Epoch 26 Batch 150 Loss 0.5887 Accuracy 0.2241\n",
      "Epoch 26 Batch 200 Loss 0.5914 Accuracy 0.2241\n",
      "Epoch 26 Batch 250 Loss 0.5929 Accuracy 0.2241\n",
      "Epoch 26 Batch 300 Loss 0.5970 Accuracy 0.2240\n",
      "Epoch 26 Batch 350 Loss 0.5982 Accuracy 0.2244\n",
      "Epoch 26 Batch 400 Loss 0.5995 Accuracy 0.2246\n",
      "Epoch 26 Batch 450 Loss 0.6015 Accuracy 0.2245\n",
      "Epoch 26 Batch 500 Loss 0.6026 Accuracy 0.2247\n",
      "Epoch 26 Batch 550 Loss 0.6023 Accuracy 0.2246\n",
      "Epoch 26 Batch 600 Loss 0.6040 Accuracy 0.2245\n",
      "Epoch 26 Batch 650 Loss 0.6055 Accuracy 0.2243\n",
      "Epoch 26 Batch 700 Loss 0.6072 Accuracy 0.2242\n",
      "Epoch 26 Batch 750 Loss 0.6089 Accuracy 0.2242\n",
      "Epoch 26 Batch 800 Loss 0.6084 Accuracy 0.2240\n",
      "Epoch 26 Batch 850 Loss 0.6090 Accuracy 0.2241\n",
      "Epoch 26 Batch 900 Loss 0.6101 Accuracy 0.2241\n",
      "Epoch 26 Batch 950 Loss 0.6106 Accuracy 0.2240\n",
      "Epoch 26 Batch 1000 Loss 0.6116 Accuracy 0.2240\n",
      "Epoch 26 Batch 1050 Loss 0.6130 Accuracy 0.2239\n",
      "Epoch 26 Batch 1100 Loss 0.6136 Accuracy 0.2238\n",
      "Epoch 26 Batch 1150 Loss 0.6139 Accuracy 0.2238\n",
      "Epoch 26 Batch 1200 Loss 0.6149 Accuracy 0.2238\n",
      "Epoch 26 Batch 1250 Loss 0.6155 Accuracy 0.2237\n",
      "Epoch 26 Batch 1300 Loss 0.6166 Accuracy 0.2236\n",
      "Epoch 26 Batch 1350 Loss 0.6178 Accuracy 0.2237\n",
      "Epoch 26 Batch 1400 Loss 0.6191 Accuracy 0.2236\n",
      "Epoch 26 Batch 1450 Loss 0.6200 Accuracy 0.2236\n",
      "Epoch 26 Batch 1500 Loss 0.6207 Accuracy 0.2236\n",
      "Epoch 26 Batch 1550 Loss 0.6222 Accuracy 0.2235\n",
      "Epoch 26 Batch 1600 Loss 0.6236 Accuracy 0.2234\n",
      "Epoch 26 Batch 1650 Loss 0.6243 Accuracy 0.2234\n",
      "Epoch 26 Batch 1700 Loss 0.6247 Accuracy 0.2234\n",
      "Epoch 26 Batch 1750 Loss 0.6248 Accuracy 0.2234\n",
      "Epoch 26 Batch 1800 Loss 0.6255 Accuracy 0.2234\n",
      "Epoch 26 Batch 1850 Loss 0.6267 Accuracy 0.2233\n",
      "Epoch 26 Batch 1900 Loss 0.6272 Accuracy 0.2233\n",
      "Epoch 26 Batch 1950 Loss 0.6276 Accuracy 0.2233\n",
      "Epoch 26 Batch 2000 Loss 0.6289 Accuracy 0.2232\n",
      "Epoch 26 Batch 2050 Loss 0.6297 Accuracy 0.2232\n",
      "Epoch 26 Batch 2100 Loss 0.6306 Accuracy 0.2232\n",
      "Epoch 26 Batch 2150 Loss 0.6313 Accuracy 0.2232\n",
      "Epoch 26 Batch 2200 Loss 0.6312 Accuracy 0.2232\n",
      "Epoch 26 Batch 2250 Loss 0.6315 Accuracy 0.2232\n",
      "Epoch 26 Batch 2300 Loss 0.6323 Accuracy 0.2232\n",
      "Epoch 26 Batch 2350 Loss 0.6329 Accuracy 0.2231\n",
      "Epoch 26 Batch 2400 Loss 0.6334 Accuracy 0.2231\n",
      "Epoch 26 Batch 2450 Loss 0.6347 Accuracy 0.2231\n",
      "Epoch 26 Batch 2500 Loss 0.6350 Accuracy 0.2230\n",
      "Epoch 26 Batch 2550 Loss 0.6359 Accuracy 0.2230\n",
      "Epoch 26 Batch 2600 Loss 0.6365 Accuracy 0.2230\n",
      "Epoch 26 Batch 2650 Loss 0.6370 Accuracy 0.2230\n",
      "Epoch 26 Batch 2700 Loss 0.6375 Accuracy 0.2230\n",
      "Epoch 26 Batch 2750 Loss 0.6378 Accuracy 0.2229\n",
      "Epoch 26 Batch 2800 Loss 0.6383 Accuracy 0.2229\n",
      "Epoch 26 Loss 0.6383 Accuracy 0.2229\n",
      "Time taken for 1 epoch: 1081.5204019546509 secs\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.4972 Accuracy 0.2298\n",
      "Epoch 27 Batch 50 Loss 0.5692 Accuracy 0.2276\n",
      "Epoch 27 Batch 100 Loss 0.5703 Accuracy 0.2263\n",
      "Epoch 27 Batch 150 Loss 0.5932 Accuracy 0.2258\n",
      "Epoch 27 Batch 200 Loss 0.5957 Accuracy 0.2252\n",
      "Epoch 27 Batch 250 Loss 0.5976 Accuracy 0.2246\n",
      "Epoch 27 Batch 300 Loss 0.6030 Accuracy 0.2240\n",
      "Epoch 27 Batch 350 Loss 0.6047 Accuracy 0.2242\n",
      "Epoch 27 Batch 400 Loss 0.6044 Accuracy 0.2242\n",
      "Epoch 27 Batch 450 Loss 0.6043 Accuracy 0.2242\n",
      "Epoch 27 Batch 500 Loss 0.6049 Accuracy 0.2240\n",
      "Epoch 27 Batch 550 Loss 0.6079 Accuracy 0.2241\n",
      "Epoch 27 Batch 600 Loss 0.6090 Accuracy 0.2241\n",
      "Epoch 27 Batch 650 Loss 0.6112 Accuracy 0.2241\n",
      "Epoch 27 Batch 700 Loss 0.6119 Accuracy 0.2241\n",
      "Epoch 27 Batch 750 Loss 0.6127 Accuracy 0.2240\n",
      "Epoch 27 Batch 800 Loss 0.6131 Accuracy 0.2239\n",
      "Epoch 27 Batch 850 Loss 0.6132 Accuracy 0.2240\n",
      "Epoch 27 Batch 900 Loss 0.6135 Accuracy 0.2238\n",
      "Epoch 27 Batch 950 Loss 0.6140 Accuracy 0.2237\n",
      "Epoch 27 Batch 1000 Loss 0.6141 Accuracy 0.2237\n",
      "Epoch 27 Batch 1050 Loss 0.6152 Accuracy 0.2237\n",
      "Epoch 27 Batch 1100 Loss 0.6161 Accuracy 0.2237\n",
      "Epoch 27 Batch 1150 Loss 0.6173 Accuracy 0.2237\n",
      "Epoch 27 Batch 1200 Loss 0.6183 Accuracy 0.2237\n",
      "Epoch 27 Batch 1250 Loss 0.6185 Accuracy 0.2236\n",
      "Epoch 27 Batch 1300 Loss 0.6190 Accuracy 0.2236\n",
      "Epoch 27 Batch 1350 Loss 0.6188 Accuracy 0.2236\n",
      "Epoch 27 Batch 1400 Loss 0.6205 Accuracy 0.2236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch 1450 Loss 0.6210 Accuracy 0.2235\n",
      "Epoch 27 Batch 1500 Loss 0.6216 Accuracy 0.2235\n",
      "Epoch 27 Batch 1550 Loss 0.6223 Accuracy 0.2234\n",
      "Epoch 27 Batch 1600 Loss 0.6230 Accuracy 0.2234\n",
      "Epoch 27 Batch 1650 Loss 0.6241 Accuracy 0.2233\n",
      "Epoch 27 Batch 1700 Loss 0.6247 Accuracy 0.2233\n",
      "Epoch 27 Batch 1750 Loss 0.6263 Accuracy 0.2233\n",
      "Epoch 27 Batch 1800 Loss 0.6263 Accuracy 0.2232\n",
      "Epoch 27 Batch 1850 Loss 0.6268 Accuracy 0.2232\n",
      "Epoch 27 Batch 1900 Loss 0.6277 Accuracy 0.2233\n",
      "Epoch 27 Batch 1950 Loss 0.6278 Accuracy 0.2232\n",
      "Epoch 27 Batch 2000 Loss 0.6283 Accuracy 0.2232\n",
      "Epoch 27 Batch 2050 Loss 0.6281 Accuracy 0.2232\n",
      "Epoch 27 Batch 2100 Loss 0.6282 Accuracy 0.2232\n",
      "Epoch 27 Batch 2150 Loss 0.6284 Accuracy 0.2232\n",
      "Epoch 27 Batch 2200 Loss 0.6287 Accuracy 0.2232\n",
      "Epoch 27 Batch 2250 Loss 0.6296 Accuracy 0.2232\n",
      "Epoch 27 Batch 2300 Loss 0.6304 Accuracy 0.2231\n",
      "Epoch 27 Batch 2350 Loss 0.6312 Accuracy 0.2231\n",
      "Epoch 27 Batch 2400 Loss 0.6317 Accuracy 0.2231\n",
      "Epoch 27 Batch 2450 Loss 0.6322 Accuracy 0.2231\n",
      "Epoch 27 Batch 2500 Loss 0.6325 Accuracy 0.2231\n",
      "Epoch 27 Batch 2550 Loss 0.6325 Accuracy 0.2231\n",
      "Epoch 27 Batch 2600 Loss 0.6334 Accuracy 0.2231\n",
      "Epoch 27 Batch 2650 Loss 0.6338 Accuracy 0.2232\n",
      "Epoch 27 Batch 2700 Loss 0.6341 Accuracy 0.2231\n",
      "Epoch 27 Batch 2750 Loss 0.6351 Accuracy 0.2231\n",
      "Epoch 27 Batch 2800 Loss 0.6358 Accuracy 0.2231\n",
      "Epoch 27 Loss 0.6357 Accuracy 0.2230\n",
      "Time taken for 1 epoch: 1121.6785027980804 secs\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.6030 Accuracy 0.2016\n",
      "Epoch 28 Batch 50 Loss 0.6035 Accuracy 0.2256\n",
      "Epoch 28 Batch 100 Loss 0.6070 Accuracy 0.2243\n",
      "Epoch 28 Batch 150 Loss 0.6019 Accuracy 0.2247\n",
      "Epoch 28 Batch 200 Loss 0.6023 Accuracy 0.2249\n",
      "Epoch 28 Batch 250 Loss 0.5987 Accuracy 0.2255\n",
      "Epoch 28 Batch 300 Loss 0.5968 Accuracy 0.2250\n",
      "Epoch 28 Batch 350 Loss 0.5978 Accuracy 0.2251\n",
      "Epoch 28 Batch 400 Loss 0.5978 Accuracy 0.2249\n",
      "Epoch 28 Batch 450 Loss 0.5978 Accuracy 0.2246\n",
      "Epoch 28 Batch 500 Loss 0.5994 Accuracy 0.2246\n",
      "Epoch 28 Batch 550 Loss 0.6009 Accuracy 0.2245\n",
      "Epoch 28 Batch 600 Loss 0.6024 Accuracy 0.2246\n",
      "Epoch 28 Batch 650 Loss 0.6037 Accuracy 0.2244\n",
      "Epoch 28 Batch 700 Loss 0.6044 Accuracy 0.2244\n",
      "Epoch 28 Batch 750 Loss 0.6062 Accuracy 0.2243\n",
      "Epoch 28 Batch 800 Loss 0.6062 Accuracy 0.2242\n",
      "Epoch 28 Batch 850 Loss 0.6061 Accuracy 0.2241\n",
      "Epoch 28 Batch 900 Loss 0.6064 Accuracy 0.2240\n",
      "Epoch 28 Batch 950 Loss 0.6080 Accuracy 0.2241\n",
      "Epoch 28 Batch 1000 Loss 0.6085 Accuracy 0.2240\n",
      "Epoch 28 Batch 1050 Loss 0.6090 Accuracy 0.2239\n",
      "Epoch 28 Batch 1100 Loss 0.6109 Accuracy 0.2239\n",
      "Epoch 28 Batch 1150 Loss 0.6124 Accuracy 0.2238\n",
      "Epoch 28 Batch 1200 Loss 0.6140 Accuracy 0.2238\n",
      "Epoch 28 Batch 1250 Loss 0.6147 Accuracy 0.2238\n",
      "Epoch 28 Batch 1300 Loss 0.6151 Accuracy 0.2239\n",
      "Epoch 28 Batch 1350 Loss 0.6158 Accuracy 0.2239\n",
      "Epoch 28 Batch 1400 Loss 0.6169 Accuracy 0.2238\n",
      "Epoch 28 Batch 1450 Loss 0.6181 Accuracy 0.2238\n",
      "Epoch 28 Batch 1500 Loss 0.6190 Accuracy 0.2238\n",
      "Epoch 28 Batch 1550 Loss 0.6200 Accuracy 0.2237\n",
      "Epoch 28 Batch 1600 Loss 0.6208 Accuracy 0.2237\n",
      "Epoch 28 Batch 1650 Loss 0.6212 Accuracy 0.2236\n",
      "Epoch 28 Batch 1700 Loss 0.6221 Accuracy 0.2236\n",
      "Epoch 28 Batch 1750 Loss 0.6225 Accuracy 0.2237\n",
      "Epoch 28 Batch 1800 Loss 0.6229 Accuracy 0.2236\n",
      "Epoch 28 Batch 1850 Loss 0.6231 Accuracy 0.2235\n",
      "Epoch 28 Batch 1900 Loss 0.6239 Accuracy 0.2235\n",
      "Epoch 28 Batch 1950 Loss 0.6242 Accuracy 0.2235\n",
      "Epoch 28 Batch 2000 Loss 0.6247 Accuracy 0.2234\n",
      "Epoch 28 Batch 2050 Loss 0.6258 Accuracy 0.2234\n",
      "Epoch 28 Batch 2100 Loss 0.6267 Accuracy 0.2235\n",
      "Epoch 28 Batch 2150 Loss 0.6268 Accuracy 0.2234\n",
      "Epoch 28 Batch 2200 Loss 0.6276 Accuracy 0.2234\n",
      "Epoch 28 Batch 2250 Loss 0.6281 Accuracy 0.2234\n",
      "Epoch 28 Batch 2300 Loss 0.6285 Accuracy 0.2234\n",
      "Epoch 28 Batch 2350 Loss 0.6289 Accuracy 0.2233\n",
      "Epoch 28 Batch 2400 Loss 0.6297 Accuracy 0.2233\n",
      "Epoch 28 Batch 2450 Loss 0.6302 Accuracy 0.2233\n",
      "Epoch 28 Batch 2500 Loss 0.6310 Accuracy 0.2232\n",
      "Epoch 28 Batch 2550 Loss 0.6311 Accuracy 0.2232\n",
      "Epoch 28 Batch 2600 Loss 0.6315 Accuracy 0.2232\n",
      "Epoch 28 Batch 2650 Loss 0.6317 Accuracy 0.2232\n",
      "Epoch 28 Batch 2700 Loss 0.6323 Accuracy 0.2232\n",
      "Epoch 28 Batch 2750 Loss 0.6325 Accuracy 0.2231\n",
      "Epoch 28 Batch 2800 Loss 0.6332 Accuracy 0.2231\n",
      "Epoch 28 Loss 0.6332 Accuracy 0.2231\n",
      "Time taken for 1 epoch: 1110.6995539665222 secs\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.5492 Accuracy 0.2252\n",
      "Epoch 29 Batch 50 Loss 0.5854 Accuracy 0.2242\n",
      "Epoch 29 Batch 100 Loss 0.5927 Accuracy 0.2244\n",
      "Epoch 29 Batch 150 Loss 0.5829 Accuracy 0.2246\n",
      "Epoch 29 Batch 200 Loss 0.5911 Accuracy 0.2244\n",
      "Epoch 29 Batch 250 Loss 0.5957 Accuracy 0.2245\n",
      "Epoch 29 Batch 300 Loss 0.5998 Accuracy 0.2246\n",
      "Epoch 29 Batch 350 Loss 0.5992 Accuracy 0.2245\n",
      "Epoch 29 Batch 400 Loss 0.6011 Accuracy 0.2243\n",
      "Epoch 29 Batch 450 Loss 0.6006 Accuracy 0.2244\n",
      "Epoch 29 Batch 500 Loss 0.6003 Accuracy 0.2245\n",
      "Epoch 29 Batch 550 Loss 0.6017 Accuracy 0.2243\n",
      "Epoch 29 Batch 600 Loss 0.6014 Accuracy 0.2244\n",
      "Epoch 29 Batch 650 Loss 0.6056 Accuracy 0.2242\n",
      "Epoch 29 Batch 700 Loss 0.6046 Accuracy 0.2243\n",
      "Epoch 29 Batch 750 Loss 0.6055 Accuracy 0.2243\n",
      "Epoch 29 Batch 800 Loss 0.6060 Accuracy 0.2244\n",
      "Epoch 29 Batch 850 Loss 0.6083 Accuracy 0.2242\n",
      "Epoch 29 Batch 900 Loss 0.6082 Accuracy 0.2243\n",
      "Epoch 29 Batch 950 Loss 0.6089 Accuracy 0.2241\n",
      "Epoch 29 Batch 1000 Loss 0.6100 Accuracy 0.2241\n",
      "Epoch 29 Batch 1050 Loss 0.6114 Accuracy 0.2240\n",
      "Epoch 29 Batch 1100 Loss 0.6119 Accuracy 0.2240\n",
      "Epoch 29 Batch 1150 Loss 0.6128 Accuracy 0.2240\n",
      "Epoch 29 Batch 1200 Loss 0.6132 Accuracy 0.2239\n",
      "Epoch 29 Batch 1250 Loss 0.6135 Accuracy 0.2240\n",
      "Epoch 29 Batch 1300 Loss 0.6141 Accuracy 0.2239\n",
      "Epoch 29 Batch 1350 Loss 0.6154 Accuracy 0.2238\n",
      "Epoch 29 Batch 1400 Loss 0.6158 Accuracy 0.2239\n",
      "Epoch 29 Batch 1450 Loss 0.6166 Accuracy 0.2238\n",
      "Epoch 29 Batch 1500 Loss 0.6175 Accuracy 0.2239\n",
      "Epoch 29 Batch 1550 Loss 0.6189 Accuracy 0.2238\n",
      "Epoch 29 Batch 1600 Loss 0.6198 Accuracy 0.2238\n",
      "Epoch 29 Batch 1650 Loss 0.6201 Accuracy 0.2238\n",
      "Epoch 29 Batch 1700 Loss 0.6213 Accuracy 0.2238\n",
      "Epoch 29 Batch 1750 Loss 0.6218 Accuracy 0.2237\n",
      "Epoch 29 Batch 1800 Loss 0.6225 Accuracy 0.2237\n",
      "Epoch 29 Batch 1850 Loss 0.6234 Accuracy 0.2237\n",
      "Epoch 29 Batch 1900 Loss 0.6235 Accuracy 0.2237\n",
      "Epoch 29 Batch 1950 Loss 0.6243 Accuracy 0.2237\n",
      "Epoch 29 Batch 2000 Loss 0.6243 Accuracy 0.2237\n",
      "Epoch 29 Batch 2050 Loss 0.6252 Accuracy 0.2237\n",
      "Epoch 29 Batch 2100 Loss 0.6253 Accuracy 0.2236\n",
      "Epoch 29 Batch 2150 Loss 0.6253 Accuracy 0.2236\n",
      "Epoch 29 Batch 2200 Loss 0.6259 Accuracy 0.2236\n",
      "Epoch 29 Batch 2250 Loss 0.6265 Accuracy 0.2236\n",
      "Epoch 29 Batch 2300 Loss 0.6274 Accuracy 0.2236\n",
      "Epoch 29 Batch 2350 Loss 0.6276 Accuracy 0.2235\n",
      "Epoch 29 Batch 2400 Loss 0.6280 Accuracy 0.2235\n",
      "Epoch 29 Batch 2450 Loss 0.6282 Accuracy 0.2234\n",
      "Epoch 29 Batch 2500 Loss 0.6290 Accuracy 0.2234\n",
      "Epoch 29 Batch 2550 Loss 0.6297 Accuracy 0.2234\n",
      "Epoch 29 Batch 2600 Loss 0.6299 Accuracy 0.2233\n",
      "Epoch 29 Batch 2650 Loss 0.6306 Accuracy 0.2234\n",
      "Epoch 29 Batch 2700 Loss 0.6316 Accuracy 0.2233\n",
      "Epoch 29 Batch 2750 Loss 0.6318 Accuracy 0.2233\n",
      "Epoch 29 Batch 2800 Loss 0.6319 Accuracy 0.2233\n",
      "Epoch 29 Loss 0.6320 Accuracy 0.2233\n",
      "Time taken for 1 epoch: 1070.867463350296 secs\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.5680 Accuracy 0.2298\n",
      "Epoch 30 Batch 50 Loss 0.5869 Accuracy 0.2257\n",
      "Epoch 30 Batch 100 Loss 0.5951 Accuracy 0.2262\n",
      "Epoch 30 Batch 150 Loss 0.5958 Accuracy 0.2261\n",
      "Epoch 30 Batch 200 Loss 0.5957 Accuracy 0.2259\n",
      "Epoch 30 Batch 250 Loss 0.5955 Accuracy 0.2256\n",
      "Epoch 30 Batch 300 Loss 0.5998 Accuracy 0.2253\n",
      "Epoch 30 Batch 350 Loss 0.6013 Accuracy 0.2251\n",
      "Epoch 30 Batch 400 Loss 0.6018 Accuracy 0.2250\n",
      "Epoch 30 Batch 450 Loss 0.6029 Accuracy 0.2249\n",
      "Epoch 30 Batch 500 Loss 0.6051 Accuracy 0.2248\n",
      "Epoch 30 Batch 550 Loss 0.6039 Accuracy 0.2249\n",
      "Epoch 30 Batch 600 Loss 0.6058 Accuracy 0.2249\n",
      "Epoch 30 Batch 650 Loss 0.6062 Accuracy 0.2247\n",
      "Epoch 30 Batch 700 Loss 0.6072 Accuracy 0.2246\n",
      "Epoch 30 Batch 750 Loss 0.6059 Accuracy 0.2245\n",
      "Epoch 30 Batch 800 Loss 0.6083 Accuracy 0.2244\n",
      "Epoch 30 Batch 850 Loss 0.6091 Accuracy 0.2245\n",
      "Epoch 30 Batch 900 Loss 0.6088 Accuracy 0.2244\n",
      "Epoch 30 Batch 950 Loss 0.6094 Accuracy 0.2244\n",
      "Epoch 30 Batch 1000 Loss 0.6105 Accuracy 0.2243\n",
      "Epoch 30 Batch 1050 Loss 0.6101 Accuracy 0.2243\n",
      "Epoch 30 Batch 1100 Loss 0.6101 Accuracy 0.2243\n",
      "Epoch 30 Batch 1150 Loss 0.6117 Accuracy 0.2243\n",
      "Epoch 30 Batch 1200 Loss 0.6123 Accuracy 0.2241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Batch 1250 Loss 0.6133 Accuracy 0.2241\n",
      "Epoch 30 Batch 1300 Loss 0.6141 Accuracy 0.2240\n",
      "Epoch 30 Batch 1350 Loss 0.6145 Accuracy 0.2240\n",
      "Epoch 30 Batch 1400 Loss 0.6152 Accuracy 0.2240\n",
      "Epoch 30 Batch 1450 Loss 0.6159 Accuracy 0.2239\n",
      "Epoch 30 Batch 1500 Loss 0.6166 Accuracy 0.2238\n",
      "Epoch 30 Batch 1550 Loss 0.6170 Accuracy 0.2237\n",
      "Epoch 30 Batch 1600 Loss 0.6178 Accuracy 0.2237\n",
      "Epoch 30 Batch 1650 Loss 0.6176 Accuracy 0.2237\n",
      "Epoch 30 Batch 1700 Loss 0.6175 Accuracy 0.2237\n",
      "Epoch 30 Batch 1750 Loss 0.6185 Accuracy 0.2237\n",
      "Epoch 30 Batch 1800 Loss 0.6188 Accuracy 0.2237\n",
      "Epoch 30 Batch 1850 Loss 0.6189 Accuracy 0.2237\n",
      "Epoch 30 Batch 1900 Loss 0.6197 Accuracy 0.2237\n",
      "Epoch 30 Batch 1950 Loss 0.6207 Accuracy 0.2236\n",
      "Epoch 30 Batch 2000 Loss 0.6213 Accuracy 0.2235\n",
      "Epoch 30 Batch 2050 Loss 0.6219 Accuracy 0.2235\n",
      "Epoch 30 Batch 2100 Loss 0.6225 Accuracy 0.2235\n",
      "Epoch 30 Batch 2150 Loss 0.6237 Accuracy 0.2234\n",
      "Epoch 30 Batch 2200 Loss 0.6242 Accuracy 0.2234\n",
      "Epoch 30 Batch 2250 Loss 0.6249 Accuracy 0.2234\n",
      "Epoch 30 Batch 2300 Loss 0.6250 Accuracy 0.2234\n",
      "Epoch 30 Batch 2350 Loss 0.6257 Accuracy 0.2234\n",
      "Epoch 30 Batch 2400 Loss 0.6261 Accuracy 0.2234\n",
      "Epoch 30 Batch 2450 Loss 0.6266 Accuracy 0.2234\n",
      "Epoch 30 Batch 2500 Loss 0.6271 Accuracy 0.2233\n",
      "Epoch 30 Batch 2550 Loss 0.6278 Accuracy 0.2233\n",
      "Epoch 30 Batch 2600 Loss 0.6285 Accuracy 0.2233\n",
      "Epoch 30 Batch 2650 Loss 0.6290 Accuracy 0.2232\n",
      "Epoch 30 Batch 2700 Loss 0.6295 Accuracy 0.2233\n",
      "Epoch 30 Batch 2750 Loss 0.6299 Accuracy 0.2233\n",
      "Epoch 30 Batch 2800 Loss 0.6307 Accuracy 0.2233\n",
      "Saving checkpoint for epoch 30 at ./checkpoints_deu/train_test/ckpt-6\n",
      "Epoch 30 Loss 0.6307 Accuracy 0.2233\n",
      "Time taken for 1 epoch: 1112.7932929992676 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "with tf.device('/GPU:0'):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "      start = time.time()\n",
    "  \n",
    "      train_loss.reset_states()\n",
    "      train_accuracy.reset_states()\n",
    "  \n",
    "      # inp -> portuguese, tar -> english\n",
    "      for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 50 == 0:\n",
    "          print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "      if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "      print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "      print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "s_qNSzzyaCbD"
   ],
   "name": "transformer.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
