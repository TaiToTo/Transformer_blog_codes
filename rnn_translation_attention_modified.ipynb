{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    " I borrowed a lot from Tensorflow official tutorial\n",
    " https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "\n",
    "#################### Ignore from this part to ####################\n",
    "\n",
    "\n",
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "en, sp = create_dataset(path_to_file, None)\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "    \n",
    "\n",
    "#################### this part. ####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24000, 16), (6000, 16), (24000, 11), (6000, 11))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " My article is not on NLP, so please just suppose that you get the four tensors below. \n",
    " These shapes mean that the max length of the input sentences is 16, \n",
    " and the max length of the target sentneces is 11. \n",
    "'''\n",
    "input_tensor_train.shape, input_tensor_val.shape, target_tensor_train.shape, target_tensor_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    6,   82, ...,    0,    0,    0],\n",
       "       [   1, 1766,   14, ...,    0,    0,    0],\n",
       "       [   1, 1134,   18, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    8,   38, ...,    0,    0,    0],\n",
       "       [   1,  348,   13, ...,    0,    0,    0],\n",
       "       [   1,    4,    8, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " Each sentence is implemented like below.\n",
    " Each row denotes a sentence, and each integer denotes a token, in this case a word.\n",
    "'''\n",
    "input_tensor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,    6,   82,  443, 6993,    5,    2,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " Let's see the first Spanish sentence of training data. \n",
    " The integer '1' and '2' correspond to \"<start>\" and \"<end>\" respectively. \n",
    "'''\n",
    "input_tensor_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> <start>\n",
      "6 ----> 多\n",
      "82 ----> usted\n",
      "443 ----> come\n",
      "6993 ----> pulpo\n",
      "5 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " You can see what each code denotes with convert()\n",
    "'''\n",
    "convert(inp_lang, input_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start>', 1),\n",
       " ('<end>', 2),\n",
       " ('.', 3),\n",
       " ('tom', 4),\n",
       " ('?', 5),\n",
       " ('多', 6),\n",
       " ('es', 7),\n",
       " ('no', 8)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " Let's see the first and last 8 tokens in the dictionary. \n",
    "'''\n",
    "list(inp_lang.word_index.items())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caminamos', 9406),\n",
       " ('divertir', 9407),\n",
       " ('divertiremos', 9408),\n",
       " ('divertirnos', 9409),\n",
       " ('decepcionaremos', 9410),\n",
       " ('viviremos', 9411),\n",
       " ('reyes', 9412),\n",
       " ('perderemos', 9413)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inp_lang.word_index.items())[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 74, 514, 19, 237, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " Let's conversely encode a Spanish sentence into integer codes. \n",
    " For the neural net which we are going to make, \"Todo sobre mi madre.\" is comprehensed as \n",
    " [1, 74, 514, 19, 237, 3, 2]\n",
    "''' \n",
    "sentence = preprocess_sentence('Todo sobre mi madre.')\n",
    "inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "\n",
    "''' You compress the 9414 dimensional input vectors into 256 dimensional vectors. '''\n",
    "embedding_dim = 256\n",
    "''' The dimension of the hidden state/vector. '''\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1 # 9414 \n",
    "vocab_tar_size = len(targ_lang.word_index)+1 # 4935 \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  In this implementation, during training the 'Encoder' class gets a (64, 16) tensor as an input, \n",
    "  and it gives out a (64, 16, 1024) tensor as an output, regrdless of how many words the inputs have. \n",
    "  That means the class gets the whole sentence as a sequence of integers, and gives out a 1024-dim \n",
    "  vector every every times step, I mean each token. \n",
    "'''\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz # 64 \n",
    "    self.enc_units = enc_units  # 24000 // 64 = 375 \n",
    "    \n",
    "    '''\n",
    "      As I explained in the last article, you propagate input 9414 dimensional vectors to 256 embedding vectors. \n",
    "    '''\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # (9414, 256)\n",
    "    \n",
    "    '''\n",
    "    We use a RNN model named GRU for this seq2seq translation model. \n",
    "    All you have to keep in mind is, in this implentation, at time step t, one GRU cell takes 'embedding_dim'(=256) \n",
    "    dimensional vector as an input, and gives out a 16 dimensional (the maximum size of input sentences) output vector \n",
    "    and succeeds a hidden state/vector to the next GRU cell.  \n",
    "    '''\n",
    "    \n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, # 1024 *the dimension of the hidden vector/state. \n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    \n",
    "    '''\n",
    "     tf.keras.layers.GRU class gets [batch, timesteps, feature] sized tensors as inputs. \n",
    "     https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "    '''\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "\n",
    "'''\n",
    "  You construct an 'Encoder' calss as below. \n",
    "  One cell get a 9414 dimensional one-hot vector, and i\n",
    "'''\n",
    "encoder = Encoder(vocab_inp_size, # 9414 \n",
    "                  embedding_dim, # 256 \n",
    "                  units, # 1024 \n",
    "                  BATCH_SIZE # 24000 \n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    '''\n",
    "      In the decoder part, you get an embedding vector for an input token every time step. \n",
    "      You have to calculate attentions using this class at EVERY TIME STEP. \n",
    "      'query' is the hidden state of an RNN cell at the time in the decoder part, whose size is (batch_size, 1, 1024).\n",
    "      'values' is the outputs of the encoder part, whose size is (batch_size, 16, 1024). \n",
    "      (*The length of the input is not necessarily 16.)\n",
    "      \n",
    "      Attention mehcanism calculates relevances of a query and values with a certain function. \n",
    "      There are several functions for calculating the relevances, and in this implementation \n",
    "      we use Bahdanau\"s additive style. \n",
    "    '''\n",
    "    \n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    \n",
    "    '''\n",
    "      In this implementation, you always need to consider time steps. \n",
    "    '''\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    '''\n",
    "      You get the attentions between the query and outputs of the encoder below.\n",
    "      In short, you compare the a word in the decoder with the input. \n",
    "      This is equivalent to finding the corresponding words in the original language, \n",
    "      when you are going to write a word in the target language. \n",
    "    '''\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(query_with_time_axis)))\n",
    "\n",
    "    '''\n",
    "      You normalize the score calculated above with a softmax function so that the usm of its values is 1. \n",
    "    '''\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    '''\n",
    "      You reweight the outputs of the encoder with attention scores.\n",
    "      The shape of the resulitng 'context_vector' is (64, 16, 1024)\n",
    "    '''\n",
    "    context_vector = attention_weights * values \n",
    "    '''\n",
    "      You calculate the weighted average of the reweighted vectors above. \n",
    "      Thus the size of the shape of the resulting 'context_vector' is (64, 1024). \n",
    "    '''\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1) # You take a weighted average of c\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    '''\n",
    "     As well as 'Encoder' class, the shape of inputs of 'Decoder' is [batch, timesteps, feature]. \n",
    "     https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU\n",
    "     But you have to keep it in mind that you input a token every time step, the input is \n",
    "     (batch_size, 1, embedding_dim). \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "     You first calculate a 'context_vector' by comparing the hidden layer of the LAST cell, \n",
    "     with the outputs of the encoder because you use Bahdanau's additive style attention mechanism. \n",
    "     You usually use the hidden layer of the current cell.\n",
    "     \n",
    "    '''\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    '''\n",
    "     You combine the 'context_vector' with the embedding vector of the decoder input at this time step. \n",
    "     And the RNN cell at current time step gives out a predicted word, given the combined input. \n",
    "    '''\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    \n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    '''\n",
    "      x: a vector whose dimension is the size of the output vocabulary size. The index of the maximum\n",
    "         element of this vector is the index of the predicted word at this time step. \n",
    "      state: the hidden state at this time step. This is the query of the next time step in Bahdanau's \n",
    "             additive style. \n",
    "    '''\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  '''\n",
    "    You input a (batch size, max input length) (=(64, 16)) tensor as an input\n",
    "    and a (batch size, max output length) (=(64, 11)) as an output, and get a loss. \n",
    "  '''\n",
    "    \n",
    "  with tf.GradientTape() as tape:\n",
    "    '''    \n",
    "      You put a batch of input sentences as a (64, 16) tensor. \n",
    "    '''\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    '''\n",
    "      You pass the last hidden state/vector of the encoder to the decoder as its \n",
    "      inittial layer.\n",
    "    '''\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    \n",
    "    '''\n",
    "    In the encoder part you pass the whole sentence as an input, \n",
    "    whereas in the decoder part, you pass a word every time step in the loop below.  \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "      The loop below shows that you \n",
    "    '''\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      \n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, \n",
    "                                           dec_hidden, \n",
    "                                           enc_output) # You need encoder outputs to calculate attentions. \n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  '''\n",
    "  Updating the weigths with the three lines below.   \n",
    "  '''\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.7843\n",
      "Epoch 1 Batch 100 Loss 2.2204\n",
      "Epoch 1 Batch 200 Loss 1.8215\n",
      "Epoch 1 Batch 300 Loss 1.7292\n",
      "Epoch 1 Loss 2.0258\n",
      "Time taken for 1 epoch 464.16598296165466 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5006\n",
      "Epoch 2 Batch 100 Loss 1.4862\n",
      "Epoch 2 Batch 200 Loss 1.3319\n",
      "Epoch 2 Batch 300 Loss 1.3300\n",
      "Epoch 2 Loss 1.3848\n",
      "Time taken for 1 epoch 474.94750022888184 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.1139\n",
      "Epoch 3 Batch 100 Loss 1.0351\n",
      "Epoch 3 Batch 200 Loss 0.9529\n",
      "Epoch 3 Batch 300 Loss 0.9118\n",
      "Epoch 3 Loss 0.9765\n",
      "Time taken for 1 epoch 452.630313873291 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6739\n",
      "Epoch 4 Batch 100 Loss 0.6981\n",
      "Epoch 4 Batch 200 Loss 0.6996\n",
      "Epoch 4 Batch 300 Loss 0.5117\n",
      "Epoch 4 Loss 0.6656\n",
      "Time taken for 1 epoch 447.1635730266571 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.4473\n",
      "Epoch 5 Batch 100 Loss 0.4837\n",
      "Epoch 5 Batch 200 Loss 0.4390\n",
      "Epoch 5 Batch 300 Loss 0.5328\n",
      "Epoch 5 Loss 0.4542\n",
      "Time taken for 1 epoch 448.39619612693787 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2834\n",
      "Epoch 6 Batch 100 Loss 0.3295\n",
      "Epoch 6 Batch 200 Loss 0.2884\n",
      "Epoch 6 Batch 300 Loss 0.3472\n",
      "Epoch 6 Loss 0.3134\n",
      "Time taken for 1 epoch 451.608145236969 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1885\n",
      "Epoch 7 Batch 100 Loss 0.1714\n",
      "Epoch 7 Batch 200 Loss 0.1982\n",
      "Epoch 7 Batch 300 Loss 0.2308\n",
      "Epoch 7 Loss 0.2252\n",
      "Time taken for 1 epoch 441.54005098342896 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1702\n",
      "Epoch 8 Batch 100 Loss 0.1551\n",
      "Epoch 8 Batch 200 Loss 0.1954\n",
      "Epoch 8 Batch 300 Loss 0.1828\n",
      "Epoch 8 Loss 0.1677\n",
      "Time taken for 1 epoch 419.2389259338379 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1103\n",
      "Epoch 9 Batch 100 Loss 0.1305\n",
      "Epoch 9 Batch 200 Loss 0.1503\n",
      "Epoch 9 Batch 300 Loss 0.0974\n",
      "Epoch 9 Loss 0.1297\n",
      "Time taken for 1 epoch 404.9650180339813 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1150\n",
      "Epoch 10 Batch 100 Loss 0.0819\n",
      "Epoch 10 Batch 200 Loss 0.1171\n",
      "Epoch 10 Batch 300 Loss 0.0979\n",
      "Epoch 10 Loss 0.1057\n",
      "Time taken for 1 epoch 397.5028669834137 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  '''\n",
    "   You initialize the 'unit' dimensional hidden layer (1024 dimensional) as a 'unit' dimensional zero vector. \n",
    "  '''\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    '''\n",
    "     You input a (batch size, max input length) (=(64, 16)) matrix as an input\n",
    "     and a (batch size, max output length) (=(64, 11)) as an output, and get a loss. \n",
    "     'enc_hidden' is the last 'units' dimensional hidden state vector (1024 dimensional) of the encoder. \n",
    "    '''\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  '''\n",
    "    Preparing an array to display attention scores. \n",
    "  '''\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  '''\n",
    "    Preparing inputs and outputs. \n",
    "  '''\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  '''\n",
    "    Initializing the hidden state of the encoder and getting outputs \n",
    "    the encoder.\n",
    "  '''\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  '''\n",
    "    \n",
    "  '''\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "      You get a 'prediction' at every time step in the decoder part. \n",
    "      The index of the maximum element is the index of the predicted word. \n",
    "    '''\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "    \n",
    "    '''\n",
    "      When the decoder generate the token '<end>', RNNs stop decoding. \n",
    "    '''\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  '''\n",
    "    You delete unnecessary attention scores below, and display the reduced attention scores. \n",
    "  '''\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fef7adb6150>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring checkpoint_dir \n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tamurataito/opt/anaconda3/envs/transformer_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if __name__ == '__main__':\n",
      "/Users/tamurataito/opt/anaconda3/envs/transformer_env/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
