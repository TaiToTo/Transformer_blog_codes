{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I borrowed a lot from Tensorflow official tutorial\n",
    "# https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "#################### Ignore from this part to ####################\n",
    "\n",
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
    "\n",
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "en, sp = create_dataset(path_to_file, None)\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "\n",
    "#################### this part. ####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24000, 16), (6000, 16), (24000, 11), (6000, 11))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My article is not on NLP, so please just suppose that you get the four tensors below. \n",
    "# These shapes mean that the max length of the input sentences is 16, \n",
    "# and the max length of the target sentneces is 11. \n",
    "input_tensor_train.shape, input_tensor_val.shape, target_tensor_train.shape, target_tensor_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  155, 3708, ...,    0,    0,    0],\n",
       "       [   1,   90,  281, ...,    0,    0,    0],\n",
       "       [   1,    4,  104, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,  137, 4849, ...,    0,    0,    0],\n",
       "       [   1,   30,    7, ...,    0,    0,    0],\n",
       "       [   1,    6,    7, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each sentence is implemented like below.\n",
    "# Each row denotes a sentence, and each integer denotes a token, in this case a word.\n",
    "input_tensor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  155, 3708,    3,    2,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the first Spanish sentence of training data. \n",
    "# The integer '1' and '2' correspond to \"<start>\" and \"<end>\" respectively. \n",
    "input_tensor_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> <start>\n",
      "155 ----> dejame\n",
      "3708 ----> sentirlo\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# You can see what ea\n",
    "convert(inp_lang, input_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start>', 1),\n",
       " ('<end>', 2),\n",
       " ('.', 3),\n",
       " ('tom', 4),\n",
       " ('?', 5),\n",
       " ('¿', 6),\n",
       " ('es', 7),\n",
       " ('no', 8)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the first and last 8 tokens in the dictionary. \n",
    "list(inp_lang.word_index.items())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('caminamos', 9406),\n",
       " ('divertir', 9407),\n",
       " ('divertiremos', 9408),\n",
       " ('divertirnos', 9409),\n",
       " ('decepcionaremos', 9410),\n",
       " ('viviremos', 9411),\n",
       " ('reyes', 9412),\n",
       " ('perderemos', 9413)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inp_lang.word_index.items())[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 74, 514, 19, 237, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "# Let's conversely encode a Spanish sentence into integer codes. \n",
    "sentence = preprocess_sentence('Todo sobre mi madre.')\n",
    "inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "# For the neural net which we are going to make, \"Todo sobre mi madre.\" is comprehensed as \n",
    "# [1, 74, 514, 19, 237, 3, 2]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256 # You compress the 9414 dimensional input vectors into 256 dimensional vectors. \n",
    "units = 1024 # The dimension of the hidden state/vector\n",
    "vocab_inp_size = len(inp_lang.word_index)+1 # 9414\n",
    "vocab_tar_size = len(targ_lang.word_index)+1 # 4935\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 16), (64, 11)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<start>': 1,\n",
       " '<end>': 2,\n",
       " '.': 3,\n",
       " 'tom': 4,\n",
       " '?': 5,\n",
       " '¿': 6,\n",
       " 'es': 7,\n",
       " 'no': 8,\n",
       " 'el': 9,\n",
       " 'a': 10,\n",
       " 'que': 11,\n",
       " 'me': 12,\n",
       " 'la': 13,\n",
       " 'de': 14,\n",
       " 'un': 15,\n",
       " 'esta': 16,\n",
       " 'se': 17,\n",
       " 'lo': 18,\n",
       " 'mi': 19,\n",
       " 'en': 20,\n",
       " 'una': 21,\n",
       " 'por': 22,\n",
       " 'te': 23,\n",
       " 'estoy': 24,\n",
       " 'ella': 25,\n",
       " 'yo': 26,\n",
       " '!': 27,\n",
       " 'eso': 28,\n",
       " 'le': 29,\n",
       " 'esto': 30,\n",
       " 'tu': 31,\n",
       " ',': 32,\n",
       " 'los': 33,\n",
       " 'aqui': 34,\n",
       " 'soy': 35,\n",
       " 'muy': 36,\n",
       " 'tengo': 37,\n",
       " 'puedo': 38,\n",
       " 'las': 39,\n",
       " 'gusta': 40,\n",
       " 'mary': 41,\n",
       " 'tiene': 42,\n",
       " 'son': 43,\n",
       " 'con': 44,\n",
       " 'como': 45,\n",
       " 'quien': 46,\n",
       " 'estaba': 47,\n",
       " 'su': 48,\n",
       " 'este': 49,\n",
       " 'favor': 50,\n",
       " 'estas': 51,\n",
       " 'eres': 52,\n",
       " 'quiero': 53,\n",
       " 'ellos': 54,\n",
       " 'fue': 55,\n",
       " 'bien': 56,\n",
       " 'casa': 57,\n",
       " 'ahora': 58,\n",
       " 'tomas': 59,\n",
       " 'donde': 60,\n",
       " 'mas': 61,\n",
       " 'estan': 62,\n",
       " 'nos': 63,\n",
       " 'he': 64,\n",
       " 'solo': 65,\n",
       " 'puede': 66,\n",
       " 'ha': 67,\n",
       " 'era': 68,\n",
       " 'todos': 69,\n",
       " 'al': 70,\n",
       " 'para': 71,\n",
       " 'ir': 72,\n",
       " 'tan': 73,\n",
       " 'todo': 74,\n",
       " 'estamos': 75,\n",
       " 'necesito': 76,\n",
       " 'ya': 77,\n",
       " 'nadie': 78,\n",
       " 'puedes': 79,\n",
       " 'trabajo': 80,\n",
       " 'voy': 81,\n",
       " 'usted': 82,\n",
       " 'tienes': 83,\n",
       " 'demasiado': 84,\n",
       " 'ese': 85,\n",
       " 'nada': 86,\n",
       " 'y': 87,\n",
       " 'hay': 88,\n",
       " 'mucho': 89,\n",
       " 'nunca': 90,\n",
       " 'hizo': 91,\n",
       " 'perro': 92,\n",
       " 'esa': 93,\n",
       " 'algo': 94,\n",
       " 'libro': 95,\n",
       " 'hoy': 96,\n",
       " 'poco': 97,\n",
       " 'dos': 98,\n",
       " 'parece': 99,\n",
       " 'todavia': 100,\n",
       " 'dinero': 101,\n",
       " 'tiempo': 102,\n",
       " 'nuevo': 103,\n",
       " 'sabe': 104,\n",
       " 'somos': 105,\n",
       " 'quiere': 106,\n",
       " 'mis': 107,\n",
       " 'gustan': 108,\n",
       " 'ser': 109,\n",
       " 'nosotros': 110,\n",
       " 'vez': 111,\n",
       " 'coche': 112,\n",
       " 'estar': 113,\n",
       " 'sos': 114,\n",
       " 'feliz': 115,\n",
       " 'va': 116,\n",
       " 'buen': 117,\n",
       " 'tarde': 118,\n",
       " 'ti': 119,\n",
       " 'ahi': 120,\n",
       " 'frances': 121,\n",
       " 'hablar': 122,\n",
       " 'hacer': 123,\n",
       " 'verdad': 124,\n",
       " 'hace': 125,\n",
       " 'creo': 126,\n",
       " 'tenemos': 127,\n",
       " 'ayuda': 128,\n",
       " 'alli': 129,\n",
       " 'boston': 130,\n",
       " 'hombre': 131,\n",
       " 'has': 132,\n",
       " 'deja': 133,\n",
       " 'vi': 134,\n",
       " 've': 135,\n",
       " 'mal': 136,\n",
       " 'alguien': 137,\n",
       " 'auto': 138,\n",
       " 'vamos': 139,\n",
       " 'si': 140,\n",
       " 'mejor': 141,\n",
       " 'siento': 142,\n",
       " 'podria': 143,\n",
       " 'podemos': 144,\n",
       " 'cuando': 145,\n",
       " 'hice': 146,\n",
       " 'vida': 147,\n",
       " 'odio': 148,\n",
       " 'dia': 149,\n",
       " 'conmigo': 150,\n",
       " 'siempre': 151,\n",
       " 'les': 152,\n",
       " 'encanta': 153,\n",
       " 'otra': 154,\n",
       " 'dejame': 155,\n",
       " 'rapido': 156,\n",
       " 'cual': 157,\n",
       " 'ustedes': 158,\n",
       " 'vino': 159,\n",
       " 'tenia': 160,\n",
       " 'puerta': 161,\n",
       " 'bueno': 162,\n",
       " 'ver': 163,\n",
       " 'hacerlo': 164,\n",
       " 'ven': 165,\n",
       " 'tambien': 166,\n",
       " 'os': 167,\n",
       " 'comer': 168,\n",
       " 'buena': 169,\n",
       " 'sus': 170,\n",
       " 'deberia': 171,\n",
       " 'dijo': 172,\n",
       " 'listo': 173,\n",
       " 'padre': 174,\n",
       " 'habitacion': 175,\n",
       " 'habla': 176,\n",
       " 'nuestro': 177,\n",
       " 'realmente': 178,\n",
       " 'ayudar': 179,\n",
       " 'queria': 180,\n",
       " 'hecho': 181,\n",
       " 'mismo': 182,\n",
       " 'nadar': 183,\n",
       " 'cansado': 184,\n",
       " 'ocupado': 185,\n",
       " 'del': 186,\n",
       " 'acabo': 187,\n",
       " 'razon': 188,\n",
       " 'grande': 189,\n",
       " 'noche': 190,\n",
       " 'gracias': 191,\n",
       " 'mira': 192,\n",
       " 'gato': 193,\n",
       " 'miedo': 194,\n",
       " 'manana': 195,\n",
       " 'acuerdo': 196,\n",
       " 'debo': 197,\n",
       " 'cama': 198,\n",
       " 'dije': 199,\n",
       " 'tus': 200,\n",
       " 'espera': 201,\n",
       " 'visto': 202,\n",
       " 'mio': 203,\n",
       " 'tal': 204,\n",
       " 'bastante': 205,\n",
       " 'alto': 206,\n",
       " 'veo': 207,\n",
       " 'ellas': 208,\n",
       " 'necesita': 209,\n",
       " 'dame': 210,\n",
       " 'idea': 211,\n",
       " 'amigos': 212,\n",
       " 'hemos': 213,\n",
       " 'quieres': 214,\n",
       " 'pareces': 215,\n",
       " 'casi': 216,\n",
       " 'estado': 217,\n",
       " 'fui': 218,\n",
       " 'hambre': 219,\n",
       " 'dio': 220,\n",
       " 'agua': 221,\n",
       " 'sabes': 222,\n",
       " 'sabia': 223,\n",
       " 'uno': 224,\n",
       " 'comida': 225,\n",
       " 'problema': 226,\n",
       " 'facil': 227,\n",
       " 'frio': 228,\n",
       " 'fuera': 229,\n",
       " 'lunes': 230,\n",
       " 'amigo': 231,\n",
       " 'duele': 232,\n",
       " 'dejo': 233,\n",
       " 'conozco': 234,\n",
       " 'estos': 235,\n",
       " 'vio': 236,\n",
       " 'madre': 237,\n",
       " 'pronto': 238,\n",
       " 'anos': 239,\n",
       " 'nino': 240,\n",
       " 'loco': 241,\n",
       " 'haz': 242,\n",
       " 'dormir': 243,\n",
       " 'libros': 244,\n",
       " 'puso': 245,\n",
       " 'mano': 246,\n",
       " 'sin': 247,\n",
       " 'television': 248,\n",
       " 'vive': 249,\n",
       " 'ojos': 250,\n",
       " 'menos': 251,\n",
       " 'cantar': 252,\n",
       " 'estuvo': 253,\n",
       " 'hora': 254,\n",
       " 'enfermo': 255,\n",
       " 'amo': 256,\n",
       " 'seguro': 257,\n",
       " 'mundo': 258,\n",
       " 'tienen': 259,\n",
       " 'pelo': 260,\n",
       " 'murio': 261,\n",
       " 'perros': 262,\n",
       " 'perdido': 263,\n",
       " 'joven': 264,\n",
       " 'compre': 265,\n",
       " 'mujer': 266,\n",
       " 'maria': 267,\n",
       " 'nombre': 268,\n",
       " 'contigo': 269,\n",
       " 'viejo': 270,\n",
       " 'hablo': 271,\n",
       " 'triste': 272,\n",
       " 'entrar': 273,\n",
       " 'espero': 274,\n",
       " 'sueno': 275,\n",
       " 'suerte': 276,\n",
       " 'necesitamos': 277,\n",
       " 'estais': 278,\n",
       " 'haciendo': 279,\n",
       " 'reloj': 280,\n",
       " 'perdi': 281,\n",
       " 'hasta': 282,\n",
       " 'momento': 283,\n",
       " 'toma': 284,\n",
       " 'tres': 285,\n",
       " 'queremos': 286,\n",
       " 'sigue': 287,\n",
       " 'viene': 288,\n",
       " 'escuela': 289,\n",
       " 'llave': 290,\n",
       " 'culpa': 291,\n",
       " 'historia': 292,\n",
       " 'vete': 293,\n",
       " 'fuerte': 294,\n",
       " 'calor': 295,\n",
       " 'vas': 296,\n",
       " 'cafe': 297,\n",
       " 'gran': 298,\n",
       " 'temprano': 299,\n",
       " 'cerca': 300,\n",
       " 'cerveza': 301,\n",
       " 'llorar': 302,\n",
       " 'irme': 303,\n",
       " 'jugar': 304,\n",
       " 'perdio': 305,\n",
       " 'ido': 306,\n",
       " 'sola': 307,\n",
       " 'venir': 308,\n",
       " 'vivo': 309,\n",
       " 'di': 310,\n",
       " 'necesitas': 311,\n",
       " 'seas': 312,\n",
       " 'hijo': 313,\n",
       " 'media': 314,\n",
       " 'cuanto': 315,\n",
       " 'leer': 316,\n",
       " 'ingles': 317,\n",
       " 'semana': 318,\n",
       " 'mia': 319,\n",
       " 'trabaja': 320,\n",
       " 'cosas': 321,\n",
       " 'gusto': 322,\n",
       " 'pagar': 323,\n",
       " 'pueden': 324,\n",
       " 'tuve': 325,\n",
       " 'han': 326,\n",
       " 'gente': 327,\n",
       " 'manos': 328,\n",
       " 'libre': 329,\n",
       " 'salir': 330,\n",
       " 'esperar': 331,\n",
       " 'estupido': 332,\n",
       " 'leche': 333,\n",
       " 'cierto': 334,\n",
       " 'lista': 335,\n",
       " 'dificil': 336,\n",
       " 'muerto': 337,\n",
       " 'llama': 338,\n",
       " 'borracho': 339,\n",
       " 'vale': 340,\n",
       " 'bebe': 341,\n",
       " 'camino': 342,\n",
       " 'duro': 343,\n",
       " 'vos': 344,\n",
       " 'estaban': 345,\n",
       " 'zapatos': 346,\n",
       " 'sea': 347,\n",
       " 'llego': 348,\n",
       " 'primero': 349,\n",
       " 'hazlo': 350,\n",
       " 'trabajar': 351,\n",
       " 'quedate': 352,\n",
       " 'comiendo': 353,\n",
       " 'decir': 354,\n",
       " 'esos': 355,\n",
       " 'minuto': 356,\n",
       " 'bicicleta': 357,\n",
       " 'pasa': 358,\n",
       " 'lado': 359,\n",
       " 'quedo': 360,\n",
       " 'asi': 361,\n",
       " 'gatos': 362,\n",
       " 'o': 363,\n",
       " 'hermana': 364,\n",
       " 'familia': 365,\n",
       " 'respuesta': 366,\n",
       " 'ayer': 367,\n",
       " 'rico': 368,\n",
       " 'divertido': 369,\n",
       " 'extrano': 370,\n",
       " 'vuelve': 371,\n",
       " 'hacia': 372,\n",
       " 'persona': 373,\n",
       " 'llamo': 374,\n",
       " 'mala': 375,\n",
       " 'ninos': 376,\n",
       " 'sombrero': 377,\n",
       " 'saben': 378,\n",
       " 'hablando': 379,\n",
       " 'quieren': 380,\n",
       " 'ama': 381,\n",
       " 'ves': 382,\n",
       " 'cabeza': 383,\n",
       " 'debe': 384,\n",
       " 'volvio': 385,\n",
       " 'malo': 386,\n",
       " 'funciona': 387,\n",
       " 'aca': 388,\n",
       " 'da': 389,\n",
       " 'chico': 390,\n",
       " 'caja': 391,\n",
       " 'queda': 392,\n",
       " 'boca': 393,\n",
       " 'telefono': 394,\n",
       " 'vuelta': 395,\n",
       " 'paso': 396,\n",
       " 'cuenta': 397,\n",
       " 'felices': 398,\n",
       " 'empezo': 399,\n",
       " 'plan': 400,\n",
       " 'juego': 401,\n",
       " 'estabas': 402,\n",
       " 'comio': 403,\n",
       " 'esperando': 404,\n",
       " 'bajo': 405,\n",
       " 'estabamos': 406,\n",
       " 'vosotros': 407,\n",
       " 'abogado': 408,\n",
       " 'cara': 409,\n",
       " 'otro': 410,\n",
       " 'lleva': 411,\n",
       " 'mintiendo': 412,\n",
       " 'inteligente': 413,\n",
       " 'hiciste': 414,\n",
       " 'edad': 415,\n",
       " 'parar': 416,\n",
       " 'deberiamos': 417,\n",
       " 'verte': 418,\n",
       " 'tenis': 419,\n",
       " 'estuve': 420,\n",
       " 'importante': 421,\n",
       " 'esposa': 422,\n",
       " 'debes': 423,\n",
       " 'sal': 424,\n",
       " 'entiendo': 425,\n",
       " 'tome': 426,\n",
       " 'ocupada': 427,\n",
       " 'encontre': 428,\n",
       " 'amor': 429,\n",
       " 'encantan': 430,\n",
       " 'vuestro': 431,\n",
       " 'secreto': 432,\n",
       " 'suficiente': 433,\n",
       " 'palabra': 434,\n",
       " 'bailar': 435,\n",
       " 'sido': 436,\n",
       " 'manzana': 437,\n",
       " 'ni': 438,\n",
       " 'nuestra': 439,\n",
       " 'cierra': 440,\n",
       " 'venga': 441,\n",
       " 'cuidado': 442,\n",
       " 'come': 443,\n",
       " 'estare': 444,\n",
       " 'ciudad': 445,\n",
       " 'podes': 446,\n",
       " 'conoces': 447,\n",
       " 'lugar': 448,\n",
       " 'profesor': 449,\n",
       " 'habia': 450,\n",
       " 'ojala': 451,\n",
       " 'queres': 452,\n",
       " 'guerra': 453,\n",
       " 'aun': 454,\n",
       " 'camisa': 455,\n",
       " 'escucha': 456,\n",
       " 'gano': 457,\n",
       " 'acaso': 458,\n",
       " 'asiento': 459,\n",
       " 'dormido': 460,\n",
       " 'equivocado': 461,\n",
       " 'leyendo': 462,\n",
       " 'odia': 463,\n",
       " 'hermano': 464,\n",
       " 'afuera': 465,\n",
       " 'hijos': 466,\n",
       " 'toda': 467,\n",
       " 'sento': 468,\n",
       " 'despierto': 469,\n",
       " 'suyo': 470,\n",
       " 'salio': 471,\n",
       " 'carne': 472,\n",
       " 'ayudo': 473,\n",
       " 'levanto': 474,\n",
       " 'dice': 475,\n",
       " 'carta': 476,\n",
       " 'carro': 477,\n",
       " 'ambos': 478,\n",
       " 'pequeno': 479,\n",
       " 'bano': 480,\n",
       " 'cuarto': 481,\n",
       " 'llaves': 482,\n",
       " 'juntos': 483,\n",
       " 'estudiar': 484,\n",
       " 'parecia': 485,\n",
       " 'mesa': 486,\n",
       " 'parte': 487,\n",
       " 'correr': 488,\n",
       " 'deje': 489,\n",
       " 'hagas': 490,\n",
       " 'lejos': 491,\n",
       " 'termino': 492,\n",
       " 'tren': 493,\n",
       " 'importa': 494,\n",
       " 'dios': 495,\n",
       " 'hare': 496,\n",
       " 'grito': 497,\n",
       " 'ganar': 498,\n",
       " 'musica': 499,\n",
       " 'broma': 500,\n",
       " 'cancion': 501,\n",
       " 'digas': 502,\n",
       " 'tipo': 503,\n",
       " 'dolor': 504,\n",
       " 'sois': 505,\n",
       " 'conocen': 506,\n",
       " 'cuantos': 507,\n",
       " 'conoce': 508,\n",
       " 'tenes': 509,\n",
       " 'policia': 510,\n",
       " 'acaba': 511,\n",
       " 'seis': 512,\n",
       " 'suena': 513,\n",
       " 'sobre': 514,\n",
       " 'antes': 515,\n",
       " 'trabajando': 516,\n",
       " 'justo': 517,\n",
       " 'ello': 518,\n",
       " 'llame': 519,\n",
       " 'herido': 520,\n",
       " 'despues': 521,\n",
       " 'enojado': 522,\n",
       " 'tuyo': 523,\n",
       " 'dentro': 524,\n",
       " 'voz': 525,\n",
       " 'trampa': 526,\n",
       " 'tuvo': 527,\n",
       " 'equipo': 528,\n",
       " 'cocinar': 529,\n",
       " 'bolsa': 530,\n",
       " 'peligro': 531,\n",
       " 'chica': 532,\n",
       " 'canadiense': 533,\n",
       " 'amiga': 534,\n",
       " 'ropa': 535,\n",
       " 'serio': 536,\n",
       " 'gordo': 537,\n",
       " 'senti': 538,\n",
       " 'ten': 539,\n",
       " 'van': 540,\n",
       " 'caminar': 541,\n",
       " 'podeis': 542,\n",
       " 'teneis': 543,\n",
       " 'venido': 544,\n",
       " 'oido': 545,\n",
       " 'listos': 546,\n",
       " 'dias': 547,\n",
       " 'boligrafo': 548,\n",
       " 'reglas': 549,\n",
       " 'doctor': 550,\n",
       " 'llorando': 551,\n",
       " 'bromeando': 552,\n",
       " 'morir': 553,\n",
       " 'idiota': 554,\n",
       " 'error': 555,\n",
       " 'usar': 556,\n",
       " 'nina': 557,\n",
       " 'volver': 558,\n",
       " 'mucha': 559,\n",
       " 'padres': 560,\n",
       " 'largo': 561,\n",
       " 'despacio': 562,\n",
       " 'segundo': 563,\n",
       " 'olvide': 564,\n",
       " 'lloro': 565,\n",
       " 'echo': 566,\n",
       " 'paciente': 567,\n",
       " 'hombres': 568,\n",
       " 'amable': 569,\n",
       " 'sentia': 570,\n",
       " 'fin': 571,\n",
       " 'encontrar': 572,\n",
       " 'escucho': 573,\n",
       " 'pescado': 574,\n",
       " 'ingenuo': 575,\n",
       " 'casado': 576,\n",
       " 'aburrido': 577,\n",
       " 'saber': 578,\n",
       " 'alla': 579,\n",
       " 'quienes': 580,\n",
       " 'mujeres': 581,\n",
       " 'taxi': 582,\n",
       " 'tanto': 583,\n",
       " 'estudiando': 584,\n",
       " 'fumar': 585,\n",
       " 'dime': 586,\n",
       " 'autobus': 587,\n",
       " 'estudiante': 588,\n",
       " 'siguio': 589,\n",
       " 'deberias': 590,\n",
       " 'esas': 591,\n",
       " 'rompio': 592,\n",
       " 'pedi': 593,\n",
       " 'banco': 594,\n",
       " 'vaya': 595,\n",
       " 'perfecto': 596,\n",
       " 'toca': 597,\n",
       " 'pie': 598,\n",
       " 'genial': 599,\n",
       " 'cambio': 600,\n",
       " 'falta': 601,\n",
       " 'anda': 602,\n",
       " 'corriendo': 603,\n",
       " 'recuerdo': 604,\n",
       " 'dicho': 605,\n",
       " 'enfadado': 606,\n",
       " 'quieras': 607,\n",
       " 'roto': 608,\n",
       " 'manzanas': 609,\n",
       " 'sera': 610,\n",
       " 'ruido': 611,\n",
       " 'conducir': 612,\n",
       " 'japones': 613,\n",
       " 'verlo': 614,\n",
       " 'gustaria': 615,\n",
       " 'llamar': 616,\n",
       " 'compro': 617,\n",
       " 'sol': 618,\n",
       " 'seria': 619,\n",
       " 'tokio': 620,\n",
       " 'cosa': 621,\n",
       " 'camara': 622,\n",
       " 'manera': 623,\n",
       " 'mantente': 624,\n",
       " 'vista': 625,\n",
       " 'vayas': 626,\n",
       " 'entra': 627,\n",
       " 'prisa': 628,\n",
       " 'comi': 629,\n",
       " 'pago': 630,\n",
       " 'cansada': 631,\n",
       " 'caliente': 632,\n",
       " 'atras': 633,\n",
       " 'fueron': 634,\n",
       " 'contento': 635,\n",
       " 'llega': 636,\n",
       " 'dejar': 637,\n",
       " 'irte': 638,\n",
       " 'cocina': 639,\n",
       " 'ayudarte': 640,\n",
       " 'pregunto': 641,\n",
       " 'medico': 642,\n",
       " 'mirando': 643,\n",
       " 'brazo': 644,\n",
       " 'matar': 645,\n",
       " 'viste': 646,\n",
       " 'tener': 647,\n",
       " 'hermanos': 648,\n",
       " 'puesto': 649,\n",
       " 'adonde': 650,\n",
       " 'empezar': 651,\n",
       " 'ningun': 652,\n",
       " 'timido': 653,\n",
       " 'abre': 654,\n",
       " 'dejalo': 655,\n",
       " 'pan': 656,\n",
       " 'escribe': 657,\n",
       " 'normal': 658,\n",
       " 'correcto': 659,\n",
       " 'mama': 660,\n",
       " 'problemas': 661,\n",
       " 'sed': 662,\n",
       " 'posible': 663,\n",
       " 'pasado': 664,\n",
       " 'habeis': 665,\n",
       " 'perder': 666,\n",
       " 'volvere': 667,\n",
       " 'turno': 668,\n",
       " 'vayamos': 669,\n",
       " 'vere': 670,\n",
       " 'sintio': 671,\n",
       " 'paga': 672,\n",
       " 'vacia': 673,\n",
       " 'cena': 674,\n",
       " 'ganas': 675,\n",
       " 'lloviendo': 676,\n",
       " 'todas': 677,\n",
       " 'dano': 678,\n",
       " 'quisiera': 679,\n",
       " 'esperanza': 680,\n",
       " 'paris': 681,\n",
       " 'aquel': 682,\n",
       " 'cualquier': 683,\n",
       " 'dar': 684,\n",
       " 'solia': 685,\n",
       " 'papel': 686,\n",
       " 'adentro': 687,\n",
       " 'luego': 688,\n",
       " 'ire': 689,\n",
       " 'sabemos': 690,\n",
       " 'quede': 691,\n",
       " 'salvo': 692,\n",
       " 'enferma': 693,\n",
       " 'rojo': 694,\n",
       " 'baja': 695,\n",
       " 'coge': 696,\n",
       " 'tuya': 697,\n",
       " 'llegado': 698,\n",
       " 'empieza': 699,\n",
       " 'pregunta': 700,\n",
       " 'mentiroso': 701,\n",
       " 'blanco': 702,\n",
       " 'irse': 703,\n",
       " 'guapa': 704,\n",
       " 'huele': 705,\n",
       " 'bus': 706,\n",
       " 'nosotras': 707,\n",
       " 'noticias': 708,\n",
       " 'nieve': 709,\n",
       " 'pego': 710,\n",
       " 'rio': 711,\n",
       " 'valiente': 712,\n",
       " 'disparo': 713,\n",
       " 'tonto': 714,\n",
       " 'limitate': 715,\n",
       " 'corazon': 716,\n",
       " 'haces': 717,\n",
       " 'viven': 718,\n",
       " 'apenas': 719,\n",
       " 'fiesta': 720,\n",
       " 'intento': 721,\n",
       " 'agradable': 722,\n",
       " 'abajo': 723,\n",
       " 'adelante': 724,\n",
       " 'espere': 725,\n",
       " 'vuelto': 726,\n",
       " 'sonrio': 727,\n",
       " 'raro': 728,\n",
       " 'caso': 729,\n",
       " 'quedar': 730,\n",
       " 'detesto': 731,\n",
       " 'hielo': 732,\n",
       " 'hago': 733,\n",
       " 'canta': 734,\n",
       " 'hicimos': 735,\n",
       " 'haga': 736,\n",
       " 'conocemos': 737,\n",
       " 'comprar': 738,\n",
       " 'unos': 739,\n",
       " 'dulce': 740,\n",
       " 'engano': 741,\n",
       " 'jefe': 742,\n",
       " 'pude': 743,\n",
       " 'tomo': 744,\n",
       " 'llevo': 745,\n",
       " 'empleo': 746,\n",
       " 'toco': 747,\n",
       " 'luz': 748,\n",
       " 'arma': 749,\n",
       " 'caballo': 750,\n",
       " 'dejes': 751,\n",
       " 'pescar': 752,\n",
       " 'tio': 753,\n",
       " 'menudo': 754,\n",
       " 'muchos': 755,\n",
       " 'verano': 756,\n",
       " 'llegar': 757,\n",
       " 'haber': 758,\n",
       " 'dientes': 759,\n",
       " 'consejo': 760,\n",
       " 'llover': 761,\n",
       " 'pidio': 762,\n",
       " 'foto': 763,\n",
       " 'torta': 764,\n",
       " 'nuestros': 765,\n",
       " 'hola': 766,\n",
       " 'vemos': 767,\n",
       " 'ponte': 768,\n",
       " 'intenta': 769,\n",
       " 'buenas': 770,\n",
       " 'terminado': 771,\n",
       " 'pasar': 772,\n",
       " 'sientate': 773,\n",
       " 'venid': 774,\n",
       " 'lee': 775,\n",
       " 'mando': 776,\n",
       " 'gratis': 777,\n",
       " 'hicieron': 778,\n",
       " 'miro': 779,\n",
       " 'debemos': 780,\n",
       " 'pero': 781,\n",
       " 'preparado': 782,\n",
       " 'nervioso': 783,\n",
       " 'vivir': 784,\n",
       " 'piensa': 785,\n",
       " 'colegio': 786,\n",
       " 'corto': 787,\n",
       " 'entro': 788,\n",
       " 'papa': 789,\n",
       " 'pajaro': 790,\n",
       " 'beso': 791,\n",
       " 'cuerda': 792,\n",
       " 'futbol': 793,\n",
       " 'caballos': 794,\n",
       " 'prefiero': 795,\n",
       " 'viendo': 796,\n",
       " 'escuchando': 797,\n",
       " 'japon': 798,\n",
       " 'eran': 799,\n",
       " 'adora': 800,\n",
       " 'escribio': 801,\n",
       " 'muerte': 802,\n",
       " 'enemigo': 803,\n",
       " 'confiar': 804,\n",
       " 'ventana': 805,\n",
       " 'fuego': 806,\n",
       " 'levanta': 807,\n",
       " 'preguntale': 808,\n",
       " 'hable': 809,\n",
       " 'lleno': 810,\n",
       " 'pajaros': 811,\n",
       " 'irnos': 812,\n",
       " 'adoro': 813,\n",
       " 'muriendo': 814,\n",
       " 'diez': 815,\n",
       " 'mire': 816,\n",
       " 'cualquiera': 817,\n",
       " 'agrada': 818,\n",
       " 'nueva': 819,\n",
       " 'contacto': 820,\n",
       " 'aire': 821,\n",
       " 'sopa': 822,\n",
       " 'atrapado': 823,\n",
       " 'cerro': 824,\n",
       " 'mayor': 825,\n",
       " 'pena': 826,\n",
       " 'paciencia': 827,\n",
       " 'ayudarme': 828,\n",
       " 'gustaba': 829,\n",
       " 'negro': 830,\n",
       " 'alta': 831,\n",
       " 'algunos': 832,\n",
       " 'peso': 833,\n",
       " 'dale': 834,\n",
       " 'nariz': 835,\n",
       " 'pienso': 836,\n",
       " 'respeto': 837,\n",
       " 'dan': 838,\n",
       " 'toques': 839,\n",
       " 'taza': 840,\n",
       " 'camion': 841,\n",
       " 'punto': 842,\n",
       " 'almuerzo': 843,\n",
       " 'dedo': 844,\n",
       " 'lapiz': 845,\n",
       " 'totalmente': 846,\n",
       " 'acerca': 847,\n",
       " 'corre': 848,\n",
       " 'quieto': 849,\n",
       " 'calvo': 850,\n",
       " 'debil': 851,\n",
       " 'cayo': 852,\n",
       " 'veia': 853,\n",
       " 'silencio': 854,\n",
       " 'arriba': 855,\n",
       " 'entonces': 856,\n",
       " 'cantando': 857,\n",
       " 'heroe': 858,\n",
       " 'soltero': 859,\n",
       " 'real': 860,\n",
       " 'ojo': 861,\n",
       " 'intentarlo': 862,\n",
       " 'verme': 863,\n",
       " 'vengo': 864,\n",
       " 'chicos': 865,\n",
       " 'azul': 866,\n",
       " 'gustas': 867,\n",
       " 'alguna': 868,\n",
       " 'necesitaba': 869,\n",
       " 'veces': 870,\n",
       " 'durmiendo': 871,\n",
       " 'monton': 872,\n",
       " 'grandes': 873,\n",
       " 'mios': 874,\n",
       " 'tarta': 875,\n",
       " 'descansar': 876,\n",
       " 'atencion': 877,\n",
       " 'claro': 878,\n",
       " 'robo': 879,\n",
       " 'encontrado': 880,\n",
       " 'escribir': 881,\n",
       " 'ley': 882,\n",
       " 'vendra': 883,\n",
       " 'treinta': 884,\n",
       " 'simplemente': 885,\n",
       " 'cinco': 886,\n",
       " 'gafas': 887,\n",
       " 'tele': 888,\n",
       " 'abierta': 889,\n",
       " 'peligroso': 890,\n",
       " 'mensaje': 891,\n",
       " 'cartas': 892,\n",
       " 'orgulloso': 893,\n",
       " 'abrir': 894,\n",
       " 'misma': 895,\n",
       " 'alegro': 896,\n",
       " 'continua': 897,\n",
       " 'ganado': 898,\n",
       " 'corrio': 899,\n",
       " 'ninguna': 900,\n",
       " 'calma': 901,\n",
       " 'ayudame': 902,\n",
       " 'mintio': 903,\n",
       " 'volar': 904,\n",
       " 'oscuro': 905,\n",
       " 'vimos': 906,\n",
       " 'hables': 907,\n",
       " 'asustado': 908,\n",
       " 'loca': 909,\n",
       " 'termina': 910,\n",
       " 'golf': 911,\n",
       " 'dudas': 912,\n",
       " 'confundido': 913,\n",
       " 'pierna': 914,\n",
       " 'abra': 915,\n",
       " 'necesitan': 916,\n",
       " 'gracioso': 917,\n",
       " 'vine': 918,\n",
       " 'primavera': 919,\n",
       " 'detras': 920,\n",
       " 'peor': 921,\n",
       " 'regalo': 922,\n",
       " 'espalda': 923,\n",
       " 'simple': 924,\n",
       " 'izquierda': 925,\n",
       " 'fuimos': 926,\n",
       " 'cuchillo': 927,\n",
       " 'traeme': 928,\n",
       " 'cambiado': 929,\n",
       " 'decision': 930,\n",
       " 'corbata': 931,\n",
       " 'crees': 932,\n",
       " 'abrio': 933,\n",
       " 'piano': 934,\n",
       " 'pasando': 935,\n",
       " 'paraguas': 936,\n",
       " 'piernas': 937,\n",
       " 'viajar': 938,\n",
       " 'completamente': 939,\n",
       " 'pelicula': 940,\n",
       " 'larga': 941,\n",
       " 'podrias': 942,\n",
       " 'creer': 943,\n",
       " 'caro': 944,\n",
       " 'abrigo': 945,\n",
       " 'hablas': 946,\n",
       " 'breve': 947,\n",
       " 'uso': 948,\n",
       " 'pagare': 949,\n",
       " 'estupendo': 950,\n",
       " 'toalla': 951,\n",
       " 'golpeo': 952,\n",
       " 'camina': 953,\n",
       " 'cruel': 954,\n",
       " 'mordio': 955,\n",
       " 'ocupados': 956,\n",
       " 'muneca': 957,\n",
       " 'deseo': 958,\n",
       " 'hermosa': 959,\n",
       " 'celoso': 960,\n",
       " 'saberlo': 961,\n",
       " 'canto': 962,\n",
       " 'bienvenido': 963,\n",
       " 'estudio': 964,\n",
       " 'culpable': 965,\n",
       " 'tenido': 966,\n",
       " 'bolso': 967,\n",
       " 'avion': 968,\n",
       " 'hambriento': 969,\n",
       " 'siente': 970,\n",
       " 'paz': 971,\n",
       " 'numero': 972,\n",
       " 'unas': 973,\n",
       " 'pelota': 974,\n",
       " 'riendo': 975,\n",
       " 'estara': 976,\n",
       " 'muertos': 977,\n",
       " 'trata': 978,\n",
       " 'viaje': 979,\n",
       " 'preocupado': 980,\n",
       " 'radio': 981,\n",
       " 'oficina': 982,\n",
       " 'lago': 983,\n",
       " 'entiende': 984,\n",
       " 'vives': 985,\n",
       " 'nacio': 986,\n",
       " 'siendo': 987,\n",
       " 'oportunidad': 988,\n",
       " 'escuche': 989,\n",
       " 'llamame': 990,\n",
       " 'entre': 991,\n",
       " 'gorda': 992,\n",
       " 'prueba': 993,\n",
       " 'profundo': 994,\n",
       " 'perfectamente': 995,\n",
       " 'llena': 996,\n",
       " 'pobre': 997,\n",
       " 'gane': 998,\n",
       " 'confia': 999,\n",
       " 'vosotras': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_lang.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz # 64\n",
    "    self.enc_units = enc_units # 24000 // 64 = 375\n",
    "    # As I explained in last article, you propagate input 9414 dimensional vectors to 256 embedding vectors. \n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # (9414, 256)\n",
    "    \n",
    "    # We use a RNN model named GRU for this seq2seq translation model. \n",
    "    # All you have to keep in mind is, in this implentation, at time step t, a GRU cell takes embedding_dim(=256) dimensional \n",
    "    # vector as an input, and gives out a 16 dimensional (the maximum size of input sentences) output vector and \n",
    "    # succeeds a \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, # 1024: the dimension of the hidden vector/state\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, # 9414\n",
    "                  embedding_dim, # 256\n",
    "                  units, # 1024\n",
    "                  BATCH_SIZE # 24000\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_input_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d33e94e4f2c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# サンプル入力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_input_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Encoder output shape: (batch size, sequence length, units) {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Encoder Hidden state shape: (batch size, units) {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'example_input_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# サンプル入力\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16, 1024]), TensorShape([64, 1024]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output.shape, sample_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0].numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.reduce_sum(attention_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4935"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9414"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  # You input a (batch size, max input length) (=(64, 16)) matrix as an input\n",
    "  # and a (batch size, max output length) (=(64, 11)) as an output, and get a loss. \n",
    "  with tf.GradientTape() as tape:\n",
    "        \n",
    "    # You put input sentences as an input (64, 16) tensor. \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, \n",
    "                                           dec_hidden, \n",
    "                                           enc_output) # You need encoder outputs to calculate attentions. \n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  # You initialize the 'unit' dimensional hidden layer (1024 dimensional) as a \n",
    "  # 'unit' dimensional zero vector. \n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    # You input a (batch size, max input length) (=(64, 16)) matrix as an input\n",
    "    # and a (batch size, max output length) (=(64, 11)) as an output, and get a loss. \n",
    "    # 'enc_hidden' is the last 'units' dimensional hidden state vector (1024 dimensional) of the encoder. \n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
